{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               FloatType,\n",
    "                               LongType,\n",
    "                               TimestampType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/25 15:52:22 WARN Utils: Your hostname, OneForAll-NickdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.20.10.6 instead (on interface en6)\n",
      "22/05/25 15:52:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/05/25 15:52:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"data_spark_on_emr\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_spark_on_emr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8d31c0f2e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.parquet.binaryAsString', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.startTime', '1653465143664'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/spark-warehouse'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.port', '56841'),\n",
       " ('spark.driver.host', '172.20.10.6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'data_spark_on_emr'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1653465145229')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ALL DATA AND SAVE TO simulation data file, and then analysis those data in spark.\n",
    "# ****** Read data from local file system(be a s3) to spark DataFrame ******\n",
    "# All data format will be saved as a parquet file.\n",
    "\n",
    "# file path: data >> immigration_data\n",
    "# There are three tables: immigration_table, immigration_personal_table and immigration_label_table\n",
    "# ***immigration_table, immigration_personal_table and immigration_label_table schema***\n",
    "\"\"\"\"Table: immigration_main_information schema\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Read immigration data\n",
    "\n",
    "\n",
    "\"\"\"Table: immigration_personal schema\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"Table: immigration_label schema\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path: data >> news_data\n",
    "\"\"\"Table: news_article schema\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table: us_cities_demographics schema\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file path: data >> usCitiesDemographics_data\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/pandas/io/sas/sas7bdat.py:763\u001b[0m, in \u001b[0;36mSAS7BDATReader._read_next_page\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/pandas/io/sas/sas7bdat.py?line=758'>759</a>\u001b[0m         rslt \u001b[39m=\u001b[39m rslt\u001b[39m.\u001b[39mset_index(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/pandas/io/sas/sas7bdat.py?line=760'>761</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m rslt\n\u001b[0;32m--> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/pandas/io/sas/sas7bdat.py?line=762'>763</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_next_page\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/pandas/io/sas/sas7bdat.py?line=763'>764</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_page_data_subheader_pointers \u001b[39m=\u001b[39m []\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/pandas/io/sas/sas7bdat.py?line=764'>765</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_page \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path_or_buf\u001b[39m.\u001b[39mread(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_page_length)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas.io.sas._sas.Parser.read_next_page'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/pandas/io/sas/sas7bdat.py\", line 763, in _read_next_page\n",
      "    def _read_next_page(self):\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "\n",
    "from sas7bdat import SAS7BDAT\n",
    "df_imm_apr_16 = pd.read_sas(path=data, format=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48f95af79faaae749105c8389524df19196568350894b01545e0b6f755f2644b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
