{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "# from signal import signal, SIGPIPE, SIG_DFL\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, udf, to_date\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               DateType,\n",
    "                               FloatType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create local spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oneforall_nick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oneforall_nick/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/Users/oneforall_nick/spark-2.4.8-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a4d813b9-1549-4505-b79a-151a74526da0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      ":: resolution report :: resolve 299ms :: artifacts dl 7ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a4d813b9-1549-4505-b79a-151a74526da0\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/7ms)\n",
      "22/06/23 19:15:51 WARN Utils: Your hostname, OneForAll-NickdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1, but we couldn't find any external IP address!\n",
      "22/06/23 19:15:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/06/23 19:15:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark_emr_udactity\") \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check spark session information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://localhost:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_emr_udactity</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f996f767d50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.app.name', 'spark_emr_udactity'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'localhost'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.driver.port', '49243'),\n",
       " ('spark.app.id', 'local-1655982953690'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access AWS S3 to get my source data After I upload data from local to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "# config.read_file(open('dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "# Access data from AWS S3\n",
    "# SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "SOURCE_S3_BUCKET = 's3://mydatapool'\n",
    "# Write data to AWS S3\n",
    "# DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n",
    "DEST_S3_BUCKET = 's3://destetlbucket'\n",
    "# *********************************************\n",
    "\n",
    "# ***** Local Testing configure ************\n",
    "# SOURCE_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/'\n",
    "# DEST_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/dest_data'\n",
    "\n",
    "# ***** Local Testing configure *****************\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")\n",
    "\n",
    "s3_access = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Label Data\n",
    "*Data format: <br>*\n",
    "    TXT\n",
    "*This step will seperate multiple tables:*\n",
    "- imm_cit_res\n",
    "- imm_port\n",
    "- imm_mod\n",
    "- imm_addr\n",
    "- imm_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------------------+\n",
      "|col_of_imm_cntyl|  value_of_imm_cntyl|value_of_imm_cntyl_organizations|\n",
      "+----------------+--------------------+--------------------------------+\n",
      "|             582|and Not Reported ...|            and Not Reported ...|\n",
      "|             582|   no land arrivals)|               no land arrivals)|\n",
      "|             717|        ST EUSTATIUS|                    ST EUSTATIUS|\n",
      "|             717|                SABA|                            SABA|\n",
      "|             245|                 PRC|                             PRC|\n",
      "|             473|      FED. STATES OF|                  FED. STATES OF|\n",
      "|             471|            NORTHERN|                        NORTHERN|\n",
      "+----------------+--------------------+--------------------------------+\n",
      "\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "|code_of_imm_destination_city|value_of_imm_destination_city|value_of_alias_imm_destination_city|\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "|                         ALC|                        ALCAN|                                 AK|\n",
      "|                         ANC|                    ANCHORAGE|                                 AK|\n",
      "|                         BAR|         BAKER AAF - BAKER...|                                 AK|\n",
      "|                         DAC|                DALTONS CACHE|                                 AK|\n",
      "|                         PIZ|         DEW STATION PT LA...|                                 AK|\n",
      "|                         DTH|                 DUTCH HARBOR|                                 AK|\n",
      "|                         EGL|                        EAGLE|                                 AK|\n",
      "|                         FRB|                    FAIRBANKS|                                 AK|\n",
      "|                         HOM|                        HOMER|                                 AK|\n",
      "|                         HYD|                        HYDER|                                 AK|\n",
      "|                         JUN|                       JUNEAU|                                 AK|\n",
      "|                         5KE|                    KETCHIKAN|                                 AK|\n",
      "|                         KET|                    KETCHIKAN|                                 AK|\n",
      "|                         MOS|         MOSES POINT INTER...|                                 AK|\n",
      "|                         NIK|                      NIKISKI|                                 AK|\n",
      "|                         NOM|                          NOM|                                 AK|\n",
      "|                         PKC|                  POKER CREEK|                                 AK|\n",
      "|                         ORI|               PORT LIONS SPB|                                 AK|\n",
      "|                         SKA|                      SKAGWAY|                                 AK|\n",
      "|                         SNP|              ST. PAUL ISLAND|                                 AK|\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------------+------------------------+\n",
      "|code_of_imm_travel_code|value_of_imm_travel_code|\n",
      "+-----------------------+------------------------+\n",
      "|                   null|                     Air|\n",
      "|                   null|                     Sea|\n",
      "|                   null|                    Land|\n",
      "|                   null|            Not reported|\n",
      "+-----------------------+------------------------+\n",
      "\n",
      "+-------------------+--------------------+\n",
      "|code_of_imm_address|value_of_imm_address|\n",
      "+-------------------+--------------------+\n",
      "|                 AL|             ALABAMA|\n",
      "|                 AK|              ALASKA|\n",
      "|                 AZ|             ARIZONA|\n",
      "|                 AR|            ARKANSAS|\n",
      "|                 CA|          CALIFORNIA|\n",
      "|                 CO|            COLORADO|\n",
      "|                 CT|         CONNECTICUT|\n",
      "|                 DE|            DELAWARE|\n",
      "|                 DC|   DIST. OF COLUMBIA|\n",
      "|                 FL|             FLORIDA|\n",
      "|                 GA|             GEORGIA|\n",
      "|                 GU|                GUAM|\n",
      "|                 HI|              HAWAII|\n",
      "|                 ID|               IDAHO|\n",
      "|                 IL|            ILLINOIS|\n",
      "|                 IN|             INDIANA|\n",
      "|                 IA|                IOWA|\n",
      "|                 KS|              KANSAS|\n",
      "|                 KY|            KENTUCKY|\n",
      "|                 LA|           LOUISIANA|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+-----------------+\n",
      "|code_of_imm_visa|value_of_imm_visa|\n",
      "+----------------+-----------------+\n",
      "|               1|         Business|\n",
      "|               2|         Pleasure|\n",
      "|               3|          Student|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** immigration_labels_descriptions ******\n",
    "\n",
    "# Get AWS S3 data Object: immigration_labels_descriptions.SAS\n",
    "s3_object = s3_access.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "text = s3_object['Body'].read()\n",
    "context = text.decode(encoding ='utf-8')\n",
    "# for obj in s3_object.objects.all():\n",
    "#     print(obj.key)\n",
    "\n",
    "context = context.replace('\\t', '')\n",
    "\n",
    "\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(\n",
    "        ';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\")\n",
    "                         for line in content_line_split]\n",
    "    content_two_dims = [i.strip().split('=') for i in content_line_list[1:]]\n",
    "    content_three_dims = [[i[0].strip(), i[1].strip().split(', ')[:][0], e]\n",
    "                          for i in content_two_dims if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_two_dims, content_three_dims\n",
    "\n",
    "# TODO:OK: three columns\n",
    "imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")\n",
    "df_imm_city_res_label = spark.sparkContext.parallelize(imm_cit_res_three).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\", \"value_of_imm_cntyl_organizations\"]) \\\n",
    "    .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl_organizations\").cast(\"String\")) \\\n",
    "\n",
    "df_imm_city_res_label.show()\n",
    "\n",
    "# TODO:OK: three columns\n",
    "imm_port_two, imm_port_three = code_mapping(context, \"i94prtl\")\n",
    "df_imm_destination_city = spark.sparkContext.parallelize(imm_port_three).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"]) \\\n",
    "                                                .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "                                                .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "                                                .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\"))\n",
    "\n",
    "df_imm_destination_city.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_mode_two, imm_mode_three = code_mapping(context, \"i94model\")\n",
    "df_imm_travel_code = spark.sparkContext.parallelize(imm_mode_two).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"]) \\\n",
    "                                           .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Integer\")) \\\n",
    "                                           .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))\n",
    "df_imm_travel_code.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_addr_two, imm_addr_three = code_mapping(context, \"i94addrl\")\n",
    "df_imm_address = spark.sparkContext.parallelize(imm_addr_two).toDF([\"code_of_imm_address\", \"value_of_imm_address\"]) \\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\"))\n",
    "df_imm_address.show()\n",
    "\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_visa = {'1': 'Business',\n",
    "            '2': 'Pleasure',\n",
    "            '3': 'Student'}\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(imm_visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"]) \\\n",
    "                                    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\"))\n",
    "df_imm_visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: News\n",
    "- Data format: <br>\n",
    "    CSV\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [news_publish_time#1508]\n",
      "   +- InMemoryRelation [news_publish_time#1508], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(2) HashAggregate(keys=[news_publish_time#1508], functions=[])\n",
      "            +- Exchange hashpartitioning(news_publish_time#1508, 5)\n",
      "               +- *(1) HashAggregate(keys=[news_publish_time#1508], functions=[])\n",
      "                  +- *(1) Project [cast(cast(unix_timestamp(publish_time#1378, yyyy-MM-dd, Some(Asia/Taipei)) as timestamp) as date) AS news_publish_time#1508]\n",
      "                     +- *(1) FileScan csv [publish_time#1378] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<publish_time:string>\n"
     ]
    }
   ],
   "source": [
    "# file path: data >> news_article\n",
    "\"\"\"Table: news_article schema\n",
    "pk: cord_uid -> news_cord_uid\n",
    "1. source_x -> news_source\n",
    "    schema: StringType()\n",
    "2. title -> news_title\n",
    "    schema: StringType()\n",
    "3. license -> news_licence\n",
    "    schema: StringType()\n",
    "4. abstract -> news_abstract\n",
    "    schema: StringType()\n",
    "5. publish_time -> news_publish_time (fk)\n",
    "    schema: TimestampType()\n",
    "6. authors -> news_authors\n",
    "    schema: StringType()\n",
    "7. url -> news_url\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "data_news = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "# news_schema = StructType([\n",
    "#     StructField(name=\"news_cord_uid\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_source\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_title\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_licence\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_abstract\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_publish_time\", dataType=DateType(), nullable=True),\n",
    "#     StructField(name=\"news_authors\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_url\", dataType=StringType(), nullable=True)\n",
    "# ])\n",
    "\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=data_news)\n",
    "\n",
    "df_news = df_news.withColumn(\"news_cord_uid\", col(\"cord_uid\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_source\", col(\"source_x\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_title\", col(\"title\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_licence\", col(\"license\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_abstract\", col(\"abstract\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_publish_time\", to_date(col(\"publish_time\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"news_authors\", col(\"authors\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_url\", col(\"url\").cast(\"String\")) \\\n",
    "    .select(col(\"news_cord_uid\"),\n",
    "            col(\"news_source\"),\n",
    "            col(\"news_title\"),\n",
    "            col(\"news_licence\"),\n",
    "            col(\"news_abstract\"),\n",
    "            col(\"news_publish_time\"),\n",
    "            col(\"news_authors\"),\n",
    "            col(\"news_url\"))\n",
    "\n",
    "df_news_tmp = df_news.createOrReplaceTempView(\"news_article_data\")\n",
    "\n",
    "df_news_tmp = spark.sql(\n",
    "    \"SELECT DISTINCT news_publish_time FROM news_article_data\")\n",
    "\n",
    "df_news_tmp.persist()\n",
    "\n",
    "df_news_tmp.explain()\n",
    "\n",
    "# df_news_tmp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "|news_cord_uid|news_source|news_title|news_licence|news_abstract|news_publish_time|news_authors|  news_url|\n",
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "|     vho70jcx|    biorxiv|SIANN: ...|     biorxiv|   Next-ge...|       2014-01-10|  Samuel ...|https:/...|\n",
      "|     i9tbix2v|    biorxiv|Spatial...|     biorxiv|   An emer...|       2014-06-04|  Lin WAN...|https:/...|\n",
      "|     62gfisc6|    biorxiv|Sequenc...|     biorxiv|   Germlin...|       2014-07-03|  Corey T...|https:/...|\n",
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_news.show(n=3, truncate=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Us Cities Demographics data\n",
    "- Data format: <br>\n",
    "    CSV\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [cidemo_city#2180, cidemo_state#2194, cidemo_median_age#2209, cidemo_total_population#2260, cidemo_state_code#2342, cidemo_count#2389, cidemo_id#2420L]\n",
      "   +- InMemoryRelation [cidemo_city#2180, cidemo_state#2194, cidemo_median_age#2209, cidemo_total_population#2260, cidemo_state_code#2342, cidemo_count#2389, cidemo_id#2420L], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(1) Project [City#2156 AS cidemo_city#2180, State#2157 AS cidemo_state#2194, cast(Median Age#2158 as float) AS cidemo_median_age#2209, cast(Total Population#2161 as int) AS cidemo_total_population#2260, State Code#2165 AS cidemo_state_code#2342, cast(Count#2167 as int) AS cidemo_count#2389, monotonically_increasing_id() AS cidemo_id#2420L]\n",
      "            +- *(1) FileScan csv [City#2156,State#2157,Median Age#2158,Male Population#2159,Female Population#2160,Total Population#2161,Number of Veterans#2162,Foreign-born#2163,Average Household Size#2164,State Code#2165,Race#2166,Count#2167] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n"
     ]
    }
   ],
   "source": [
    "# Create a us-cities data dimension table\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "pk: generated -> cidemo_id\n",
    "    schema: IntegerType()\n",
    "1. City -> cidemo_city\n",
    "    schema: StringType()\n",
    "2. State -> cidemo_state\n",
    "    schema: StringType()\n",
    "3. Median Age -> cidemo_median_age\n",
    "    schema: FloatType()\n",
    "4. Total Population -> cidemo_total_population\n",
    "    schema: IntegerType()\n",
    "5. State Code -> cidemo_state_code (fk)\n",
    "    schema: StringType()\n",
    "6. Count -> cidemo_count\n",
    "    schema: IntegerType()\n",
    "\"\"\"\n",
    "\n",
    "data_us_cities_demographics = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\"\n",
    "\n",
    "# TODO -> Must be defined a function that generated each table schema:\n",
    "us_cities_demographics_data_schema = StructType([\n",
    "    StructField(name=\"cidemo_city\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_median_age\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cidemo_total_population\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state_code\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_count\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using pyspark to read csv file\n",
    "df_us_cities_demographics = spark.read.options(header=True, delimiter=';').csv(data_us_cities_demographics)\n",
    "\"\"\"\n",
    "root\n",
    " |-- cidemo_city: string (nullable = true)\n",
    " |-- cidemo_state: string (nullable = true)\n",
    " |-- cidemo_median_age: float (nullable = true)\n",
    " |-- cidemo_total_population: integer (nullable = true)\n",
    " |-- cidemo_state_code: string (nullable = true)\n",
    " |-- cidemo_count: integer (nullable = true)\n",
    "\"\"\"\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_city\", col(\"City\").cast(\"String\")) \\\n",
    "                    .withColumn(\"cidemo_state\", col(\"State\").cast(\"String\")) \\\n",
    "                        .withColumn(\"cidemo_median_age\", col(\"Median Age\").cast(\"Float\")) \\\n",
    "                            .withColumn(\"cidemo_male_population\", col(\"Male Population\").cast(\"Integer\")) \\\n",
    "                                .withColumn(\"cidemo_female_population\", col(\"Female Population\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"cidemo_total_population\", col(\"Total Population\").cast(\"Integer\")) \\\n",
    "                                            .withColumn(\"cidemo_number_of_veterans\", col(\"Number of Veterans\").cast(\"Integer\")) \\\n",
    "                                                .withColumn(\"cidemo_foreign_born\", col(\"Foreign-born\").cast(\"Integer\")) \\\n",
    "                                                    .withColumn(\"cidemo_average_household_size\", col(\"Average Household Size\").cast(\"Float\")) \\\n",
    "                                                        .withColumn(\"cidemo_state_code\", col(\"State Code\").cast(\"String\")) \\\n",
    "                                                            .withColumn(\"cidemo_race\", col(\"Race\").cast(\"String\")) \\\n",
    "    .withColumn(\"cidemo_count\", col(\"Count\").cast(\"Integer\")) \\\n",
    "                    .select(col(\"cidemo_city\"),\n",
    "                            col(\"cidemo_state\"),\n",
    "                            col(\"cidemo_median_age\"),\n",
    "                            col(\"cidemo_total_population\"),\n",
    "                            col(\"cidemo_state_code\"),\n",
    "                            col(\"cidemo_count\"))\n",
    "\n",
    "# Auto-generated series of id\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_id\", monotonically_increasing_id())\n",
    "\n",
    "df_us_cities_demographics_temp = df_us_cities_demographics.createOrReplaceTempView(\"us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp = spark.sql(\"SELECT * FROM us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp.persist()\n",
    "\n",
    "df_us_cities_demographics_temp.explain()\n",
    "\n",
    "# df_us_cities_demographics_temp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "|     cidemo_city|  cidemo_state|cidemo_median_age|cidemo_total_population|cidemo_state_code|cidemo_count|cidemo_id|\n",
      "+----------------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "|   Silver Spring|      Maryland|             33.8|                  82463|               MD|       25924|        0|\n",
      "|          Quincy| Massachusetts|             41.0|                  93629|               MA|       58723|        1|\n",
      "|          Hoover|       Alabama|             38.5|                  84839|               AL|        4759|        2|\n",
      "|Rancho Cucamonga|    California|             34.5|                 175232|               CA|       24437|        3|\n",
      "|          Newark|    New Jersey|             34.6|                 281913|               NJ|       76402|        4|\n",
      "|          Peoria|      Illinois|             33.1|                 118661|               IL|        1343|        5|\n",
      "|        Avondale|       Arizona|             29.1|                  80683|               AZ|       11592|        6|\n",
      "|     West Covina|    California|             39.8|                 108489|               CA|       32716|        7|\n",
      "|        O'Fallon|      Missouri|             36.0|                  85032|               MO|        2583|        8|\n",
      "|      High Point|North Carolina|             35.5|                 109828|               NC|       11060|        9|\n",
      "|          Folsom|    California|             40.9|                  76368|               CA|        5822|       10|\n",
      "|          Folsom|    California|             40.9|                  76368|               CA|         998|       11|\n",
      "|    Philadelphia|  Pennsylvania|             34.1|                1567442|               PA|      122721|       12|\n",
      "|         Wichita|        Kansas|             34.6|                 389955|               KS|       65162|       13|\n",
      "|         Wichita|        Kansas|             34.6|                 389955|               KS|        8791|       14|\n",
      "|      Fort Myers|       Florida|             37.3|                  74015|               FL|       50169|       15|\n",
      "|      Pittsburgh|  Pennsylvania|             32.9|                 304385|               PA|      208863|       16|\n",
      "|          Laredo|         Texas|             28.8|                 255789|               TX|        1253|       17|\n",
      "|        Berkeley|    California|             32.5|                 120971|               CA|       27089|       18|\n",
      "|     Santa Clara|    California|             35.2|                 126216|               CA|       55847|       19|\n",
      "+----------------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_cities_demographics_temp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** imm_data ******\n",
    "imm_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "df_imm_data = spark.read.format('com.github.saurfang.sas.spark').load(imm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "|  6.0|20...|   4.0| 692.0| 692.0|    XXX|  20...|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null|  19...|  10...|  null|  null|   null| 1....| null|      B2|\n",
      "|  7.0|20...|   4.0| 254.0| 276.0|    ATL|  20...|    1.0|     AL|   null|  25.0|    3.0|  1.0|   20...|     SEO| null|      G|   null|      Y|   null|  19...|    D/S|     M|  null|   null| 3....|00296|      F1|\n",
      "| 15.0|20...|   4.0| 101.0| 101.0|    WAS|  20...|    1.0|     MI|  20...|  55.0|    2.0|  1.0|   20...|    null| null|      T|      O|   null|      M|  19...|  09...|     M|  null|     OS| 6....|   93|      B2|\n",
      "| 16.0|20...|   4.0| 101.0| 101.0|    NYC|  20...|    1.0|     MA|  20...|  28.0|    2.0|  1.0|   20...|    null| null|      O|      O|   null|      M|  19...|  09...|  null|  null|     AA| 9....|00199|      B2|\n",
      "| 17.0|20...|   4.0| 101.0| 101.0|    NYC|  20...|    1.0|     MA|  20...|   4.0|    2.0|  1.0|   20...|    null| null|      O|      O|   null|      M|  20...|  09...|  null|  null|     AA| 9....|00199|      B2|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm_data.show(n=5, truncate=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration personal data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [imm_per_cic_id#611, imm_person_birth_year#641, imm_person_gender#672, imm_visatype#704]\n",
      "   +- InMemoryRelation [imm_per_cic_id#611, imm_person_birth_year#641, imm_person_gender#672, imm_visatype#704], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(1) Project [cast(cicid#0 as string) AS imm_per_cic_id#611, cast(biryear#20 as int) AS imm_person_birth_year#641, gender#22 AS imm_person_gender#672, visatype#27 AS imm_visatype#704]\n",
      "            +- *(1) Scan SasRelation(/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0) [cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] PushedFilters: [], ReadSchema: struct<cicid:double,i94yr:double,i94mon:double,i94cit:double,i94res:double,i94port:string,arrdate...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "# show(n=5, truncate=5)\n",
    "df_immigration_personal = df_imm_data.withColumn(\"imm_per_cic_id\", col(\"cicid\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_person_birth_year\", col(\"biryear\").cast(\"Integer\"))\\\n",
    "           .withColumn(\"imm_person_gender\", col(\"gender\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_visatype\", col(\"visatype\").cast(\"String\")).select(col(\"imm_per_cic_id\"), \\\n",
    "                                                                              col(\"imm_person_birth_year\"), \\\n",
    "                                                                              col(\"imm_person_gender\"), \\\n",
    "                                                                              col(\"imm_visatype\"))\n",
    "\n",
    "df_immigration_personal_tmp = df_immigration_personal.createOrReplaceTempView(\"imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp = spark.sql(\"SELECT * FROM imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp.persist()\n",
    "\n",
    "df_immigration_personal_tmp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+-----------------+------------+\n",
      "|imm_per_cic_id|imm_person_birth_year|imm_person_gender|imm_visatype|\n",
      "+--------------+---------------------+-----------------+------------+\n",
      "|           6.0|                 1979|             null|          B2|\n",
      "|           7.0|                 1991|                M|          F1|\n",
      "|          15.0|                 1961|                M|          B2|\n",
      "|          16.0|                 1988|             null|          B2|\n",
      "|          17.0|                 2012|             null|          B2|\n",
      "+--------------+---------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_personal_tmp.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration main data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [imm_main_cic_id#785, imm_year#815, imm_month#846, imm_cntyl#878, imm_visa#911, imm_port#945, imm_arrival_date#981, imm_departure_date#1018, imm_model#1055, imm_address#1093, imm_airline#1132, imm_flight_no#1172]\n",
      "   +- InMemoryRelation [imm_main_cic_id#785, imm_year#815, imm_month#846, imm_cntyl#878, imm_visa#911, imm_port#945, imm_arrival_date#981, imm_departure_date#1018, imm_model#1055, imm_address#1093, imm_airline#1132, imm_flight_no#1172], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(2) Project [cast(cicid#0 as int) AS imm_main_cic_id#785, cast(i94yr#1 as int) AS imm_year#815, cast(i94mon#2 as int) AS imm_month#846, cast(i94cit#3 as int) AS imm_cntyl#878, cast(i94visa#11 as int) AS imm_visa#911, i94port#5 AS imm_port#945, pythonUDF0#1237 AS imm_arrival_date#981, pythonUDF1#1238 AS imm_departure_date#1018, cast(i94mode#7 as int) AS imm_model#1055, i94addr#8 AS imm_address#1093, airline#24 AS imm_airline#1132, fltno#26 AS imm_flight_no#1172]\n",
      "            +- BatchEvalPython [<lambda>(arrdate#6), <lambda>(depdate#9)], [airline#24, arrdate#6, cicid#0, depdate#9, fltno#26, i94addr#8, i94cit#3, i94mode#7, i94mon#2, i94port#5, i94visa#11, i94yr#1, pythonUDF0#1237, pythonUDF1#1238]\n",
      "               +- *(1) Project [airline#24, arrdate#6, cicid#0, depdate#9, fltno#26, i94addr#8, i94cit#3, i94mode#7, i94mon#2, i94port#5, i94visa#11, i94yr#1]\n",
      "                  +- *(1) Scan SasRelation(/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0) [cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] PushedFilters: [], ReadSchema: struct<cicid:double,i94yr:double,i94mon:double,i94cit:double,i94res:double,i94port:string,arrdate...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr: 4 digit year of the arrival  -> imm_year\n",
    "2. i94mon: numeric month of the arrival -> imm_month\n",
    "3. i94citi&i94res: 3 digit code of origin city -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa: reason for immigration -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port: 3 character code of destination city --> Foreign key (used to map to USDemographics and City Temperature data) -> imm_port\n",
    "6. arrdate: arrival date of the departure -> imm_arrival_date:\n",
    "7. deptdate: departure date\n",
    "date_add\n",
    "7. i94mode: 1 digit travel code -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_datetime(days: DoubleType) -> datetime:\n",
    "    \"\"\"convert_to_datetime converts days to datetime format\n",
    "\n",
    "    Args:\n",
    "        days (DoubleType): from sas arrive or departure date\n",
    "\n",
    "    Returns:\n",
    "        datetime: added days to datetime format result.\n",
    "    \"\"\"\n",
    "    if days is not None:\n",
    "        date = datetime.strptime('1960-01-01', '%Y-%m-%d')\n",
    "\n",
    "        return date + timedelta(days=days)\n",
    "\n",
    "udf_convert_to_datetime = udf(lambda x: convert_to_datetime(x), DateType())\n",
    "\n",
    "immigration_main_information = df_imm_data.withColumn(\"imm_main_cic_id\", col(\"cicid\").cast(\"Integer\"))\\\n",
    "            .withColumn(\"imm_year\", col(\"i94yr\").cast(\"Integer\"))\\\n",
    "                .withColumn(\"imm_month\", col(\"i94mon\").cast(\"Integer\"))\\\n",
    "                    .withColumn(\"imm_cntyl\", col(\"i94cit\").cast(\"Integer\"))\\\n",
    "                        .withColumn(\"imm_visa\", col(\"i94visa\").cast(\"Integer\"))\\\n",
    "                            .withColumn(\"imm_port\", col(\"i94port\").cast(\"String\"))\\\n",
    "                                .withColumn(\"imm_arrival_date\", udf_convert_to_datetime(col(\"arrdate\")))\\\n",
    "                                    .withColumn(\"imm_departure_date\", udf_convert_to_datetime(col(\"depdate\")))\\\n",
    "                                        .withColumn(\"imm_model\", col(\"i94mode\").cast(\"Integer\"))\\\n",
    "                                            .withColumn(\"imm_address\", col(\"i94addr\").cast(\"String\"))\\\n",
    "                                                .withColumn(\"imm_airline\", col(\"airline\").cast(\"String\"))\\\n",
    "                                                    .withColumn(\"imm_flight_no\", col(\"fltno\").cast(\"String\"))\\\n",
    "        .select(col('imm_main_cic_id'), \\\n",
    "                    col('imm_year'),\\\n",
    "                        col('imm_month'),\\\n",
    "                            col('imm_cntyl'),\\\n",
    "                                col('imm_visa'),\\\n",
    "                                    col('imm_port'),\\\n",
    "                                        col('imm_arrival_date'),\\\n",
    "                                            col('imm_departure_date'),\\\n",
    "                                                col('imm_model'),\\\n",
    "                                                    col('imm_address'),\\\n",
    "                                                        col('imm_airline'),\\\n",
    "                                                            col('imm_flight_no'))\n",
    "\n",
    "df_immigration_main_information = immigration_main_information.createOrReplaceTempView(\n",
    "    \"immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information = spark.sql(\"SELECT * FROM immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information.persist()\n",
    "\n",
    "df_immigration_main_information.explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imm_main_cic_id: integer (nullable = true)\n",
      " |-- imm_year: integer (nullable = true)\n",
      " |-- imm_month: integer (nullable = true)\n",
      " |-- imm_cntyl: integer (nullable = true)\n",
      " |-- imm_visa: integer (nullable = true)\n",
      " |-- imm_port: string (nullable = true)\n",
      " |-- imm_arrival_date: date (nullable = true)\n",
      " |-- imm_departure_date: date (nullable = true)\n",
      " |-- imm_model: integer (nullable = true)\n",
      " |-- imm_address: string (nullable = true)\n",
      " |-- imm_airline: string (nullable = true)\n",
      " |-- imm_flight_no: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_main_information.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact: Nofification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=======================>                                  (2 + 3) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "|imm_main_cic_id|imm_year|imm_month|imm_cntyl|imm_visa|imm_port|imm_arrival_date|imm_departure_date|imm_model|imm_address|imm_airline|imm_flight_no|imm_per_cic_id|imm_person_birth_year|imm_person_gender|imm_visatype|news_cord_uid|news_source|news_title|news_licence|news_abstract|news_publish_time|news_authors|news_url|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          vhs|        PMC|       The|         cc-|          RNA|              201|         Jr|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          1am|        PMC|       Pre|         cc-|          We |              201|         Guo|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          4ov|        PMC|       Doe|         cc-|          The|              201|         Poo|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          rkd|        PMC|       Mea|         cc-|          Bil|              201|         Ken|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          jns|        PMC|       DNA|         cc-|          To |              201|         Liu|     htt|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/23 23:02:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2361589 ms exceeds timeout 120000 ms\n",
      "22/06/23 23:02:40 ERROR TaskSchedulerImpl: Lost executor driver on localhost: Executor heartbeat timed out after 2361589 ms\n",
      "22/06/23 23:02:40 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_10_piece0 !\n",
      "22/06/23 23:02:40 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_11_piece0 !\n",
      "22/06/23 23:02:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Notification Table\n",
    "\"\"\"\n",
    "t2.imm_main_cic_id\n",
    "t2.imm_per_cic_id\n",
    "t2.news_cord_uid\n",
    "src.cidemo_id\n",
    "src.value_of_imm_destination_city\n",
    "t2.news_title\n",
    "t2.news_abstract\n",
    "t2.news_publish_time\n",
    "t2.news_authors\n",
    "\"\"\"\n",
    "\n",
    "#  ** t1: join imm two tables\n",
    "#  ** t2: join news table with t1\n",
    "#  ** t3: join us cities table with t2\n",
    "\n",
    "df_notification = spark.sql(\n",
    "        \"WITH t1 AS \\\n",
    "            (SELECT * \\\n",
    "               FROM immigration_main_information_data imid \\\n",
    "             INNER JOIN imm_personal ip \\\n",
    "                    ON imid.imm_main_cic_id = ip.imm_per_cic_id \\\n",
    "                 WHERE imid.imm_year = 2016 \\\n",
    "            ), t2 AS \\\n",
    "                (SELECT * \\\n",
    "                   FROM t1 \\\n",
    "                 INNER JOIN news_article_data nad \\\n",
    "                        ON t1.imm_arrival_date = nad.news_publish_time \\\n",
    "            ) \\\n",
    "            SELECT  * \\\n",
    "              FROM t2 \\\n",
    "            LIMIT 5 \\\n",
    "        \"\n",
    "    )\n",
    "\n",
    "df_notification.show(n=5, truncate=3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f40c152a15a669d798eb1ad4e6f345bdd77350f6745bfc8751a72382d50440f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nick_udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
