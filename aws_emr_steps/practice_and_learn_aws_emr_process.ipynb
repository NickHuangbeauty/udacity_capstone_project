{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from  datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, monotonically_increasing_id, udf, to_date\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               DateType,\n",
    "                               FloatType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 17:15:01 WARN Utils: Your hostname, OneForAll-NickdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.101 instead (on interface en0)\n",
      "22/05/27 17:15:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oneforall_nick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oneforall_nick/.ivy2/jars\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-7b149d6f-f5c6-43b0-a912-d7498a47fab5;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;3.0.0-s_2.12 in spark-packages\n",
      "\tfound com.epam#parso;2.0.11 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.12;12.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.13.2 in central\n",
      ":: resolution report :: resolve 333ms :: artifacts dl 12ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.11 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.13.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.12;12.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;3.0.0-s_2.12 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-7b149d6f-f5c6-43b0-a912-d7498a47fab5\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/7ms)\n",
      "22/05/27 17:15:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    appName(\"data_spark_on_emr\").\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_spark_on_emr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f971240d220>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.parquet.binaryAsString', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.driver.host', '10.0.0.101'),\n",
       " ('spark.app.startTime', '1653642903338'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.driver.port', '49462'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/spark-warehouse'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://10.0.0.101:49462/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar,spark://10.0.0.101:49462/jars/org.scala-lang_scala-reflect-2.12.10.jar,spark://10.0.0.101:49462/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,spark://10.0.0.101:49462/jars/com.epam_parso-2.0.11.jar,spark://10.0.0.101:49462/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,spark://10.0.0.101:49462/jars/org.slf4j_slf4j-api-1.7.5.jar'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'data_spark_on_emr'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.app.id', 'local-1653642904732'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:3.0.0-s_2.12')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ALL DATA AND SAVE TO simulation data file, and then analysis those data in spark.\n",
    "# ****** Read data from local file system(be a s3) to spark DataFrame ******\n",
    "# All data format will be saved as a parquet file.\n",
    "\n",
    "# file path: data >> immigration_data\n",
    "# There are three tables: immigration_table, immigration_personal_table and immigration_label_table\n",
    "# ***immigration_table, immigration_personal_table and immigration_label_table schema***\n",
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr  -> imm_year\n",
    "2. i94mon -> imm_month\n",
    "3. i94citi&i94res -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port -> imm_port\n",
    "6. arrdate -> imm_arrival_date:\n",
    "7. i94mode -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: immigration_label\n",
    "\n",
    "- df_imm_city_res_label\n",
    "- df_imm_destination_city\n",
    "- df_imm_travel_code\n",
    "- df_imm_address\n",
    "- df_imm_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_label_data_read = SAS7BDAT(label_data)\n",
    "\n",
    "# t = list(imm_label_data_read)\n",
    "# TODO: make sure that is a correct data\n",
    "\n",
    "# def read_data_to_df(call_func):\n",
    "#     # def wrapper(label_data):\n",
    "#     def wrapper(*args, **kwargs):\n",
    "#         # return call_func(*args)\n",
    "#         print(\"Reading data...\")\n",
    "#         with open(label_data) as f:\n",
    "#             context = f.read().replace('\\t', '')\n",
    "#         call_func(context, *args, **kwargs)\n",
    "#         print(\"Data reading finished.\")\n",
    "#     return wrapper\n",
    "\n",
    "\n",
    "with open(label_data) as f:\n",
    "    context = f.read().replace('\\t', '')\n",
    "    # content_mapping = context[context.index('i94prtl'):]\n",
    "    # content_line_split = content_mapping[:content_mapping.index(\n",
    "    #     ';')].split('\\n')\n",
    "    # content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    # content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    # # content_dict = [[i[0].strip(), i[1].strip(), [e for e in i[1].strip().split(', ')[1:]]] for i in content_dict if len(i) == 2]\n",
    "    # content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "\n",
    "\n",
    "# @read_data_to_df\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_dict\n",
    "\n",
    "# i94cit_res = code_mapping(context, \"i94cntyl\")\n",
    "i94port = code_mapping(context, \"i94prtl\")\n",
    "# i94mode = code_mapping(context, \"i94model\")\n",
    "# i94addr = code_mapping(context, \"i94addrl\")\n",
    "# i94visa = {'1': 'Business',\n",
    "#            '2': 'Pleasure',\n",
    "#            '3': 'Student'}\n",
    "\n",
    "# for i in i94port:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"code1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Writing into aws s3 and format parquet file.\n",
    "df_imm_city_res_label = spark.sparkContext.parallelize(i94cit_res.items()).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\"])\\\n",
    "    .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\"))\n",
    "\n",
    "df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"])\\\n",
    "    .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\"))\n",
    "\n",
    "df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city_data\")\n",
    "df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city_data\")\n",
    "\n",
    "# df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city\")\n",
    "# df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city WHERE code_of_imm_destination_city IS NOT NULL\")\n",
    "\n",
    "df_imm_travel_code = spark.sparkContext.parallelize(i94mode.items()).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"])\\\n",
    "    .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))\n",
    "\n",
    "df_imm_address = spark.sparkContext.parallelize(i94addr.items()).toDF([\"code_of_imm_address\", \"value_of_imm_address\"])\\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\"))\\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\"))\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(i94visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"])\\\n",
    "    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\"))\n",
    "\n",
    "\n",
    "\n",
    "# df_imm_city_res_label.createOrReplaceTempView(\"imm_city_res_label\")\n",
    "\n",
    "# df_imm_city_res_label = spark.sql(\"SELECT * FROM imm_city_res_label\")\n",
    "# df_imm_city_res.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "|code_of_imm_destination_city|value_of_imm_destination_city|value_of_alias_imm_destination_city|\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "|                         ALC|                        ALCAN|                                 AK|\n",
      "|                         ANC|                    ANCHORAGE|                                 AK|\n",
      "|                         BAR|                   BAKER A...|                                 AK|\n",
      "|                         DAC|                   DALTONS...|                                 AK|\n",
      "|                         PIZ|                   DEW STA...|                                 AK|\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm_destination_city_tmp.show(n=5, truncate=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "287\n"
     ]
    }
   ],
   "source": [
    "# TODO: Check data source\n",
    "for column in df_imm_city_res_label.columns:\n",
    "    n = df_imm_city_res_label.select(column).distinct().count()\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[col_of_imm_cntyl: int, value_of_imm_cntyl: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm_city_res.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: news_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [publish_time#6445]\n",
      "   +- InMemoryRelation [publish_time#6445], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(2) HashAggregate(keys=[publish_time#6445], functions=[])\n",
      "            +- Exchange hashpartitioning(publish_time#6445, 5), ENSURE_REQUIREMENTS, [id=#1786]\n",
      "               +- *(1) HashAggregate(keys=[publish_time#6445], functions=[])\n",
      "                  +- FileScan csv [publish_time#6445] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<publish_time:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# file path: data >> news_article\n",
    "\"\"\"Table: news_article schema\n",
    "pk: cord_uid -> news_cord_uid\n",
    "1. source_x -> news_source\n",
    "    schema: StringType()\n",
    "2. title -> news_title\n",
    "    schema: StringType()\n",
    "3. license -> news_licence\n",
    "    schema: StringType()\n",
    "4. abstract -> news_abstract\n",
    "    schema: StringType()\n",
    "5. publish_time -> news_publish_time (fk)\n",
    "    schema: TimestampType()\n",
    "6. authors -> news_authors\n",
    "    schema: StringType()\n",
    "7. url -> news_url\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "data_news = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "# news_schema = StructType([\n",
    "#     StructField(name=\"news_cord_uid\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_source\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_title\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_licence\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_abstract\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_publish_time\", dataType=DateType(), nullable=True),\n",
    "#     StructField(name=\"news_authors\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_url\", dataType=StringType(), nullable=True)\n",
    "# ])\n",
    "\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=data_news)\n",
    "\n",
    "df_news = df_news.withColumn(\"news_cord_uid\", col(\"cord_uid\").cast(\"String\")) \\\n",
    "                    .withColumn(\"news_source\", col(\"source_x\").cast(\"String\")) \\\n",
    "                        .withColumn(\"news_title\", col(\"title\").cast(\"String\")) \\\n",
    "                            .withColumn(\"news_licence\", col(\"license\").cast(\"String\")) \\\n",
    "                                .withColumn(\"news_abstract\", col(\"abstract\").cast(\"String\")) \\\n",
    "                                    .withColumn(\"news_publish_time\", to_date(df_news.publish_time, \"yyyy-MM-dd\")) \\\n",
    "                                        .withColumn(\"news_authors\", col(\"authors\").cast(\"String\")) \\\n",
    "                                            .withColumn(\"news_url\", col(\"url\").cast(\"String\")) \\\n",
    "                 .select(col(\"news_cord_uid\"),\n",
    "                         col(\"news_source\"),\n",
    "                         col(\"news_title\"),\n",
    "                         col(\"news_licence\"),\n",
    "                         col(\"news_abstract\"),\n",
    "                         col(\"news_publish_time\"),\n",
    "                         col(\"news_authors\"),\n",
    "                         col(\"news_url\")\n",
    "                         )\n",
    "\n",
    "df_news_tmp = df_news.createOrReplaceTempView(\"news_article_data\")\n",
    "\n",
    "df_news_tmp = spark.sql(\"SELECT DISTINCT publish_time FROM news_article_data\")\n",
    "\n",
    "df_news_tmp.persist()\n",
    "\n",
    "df_news_tmp.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'unpersist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000036?line=0'>1</a>\u001b[0m df_news_tmp\u001b[39m.\u001b[39;49munpersist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'unpersist'"
     ]
    }
   ],
   "source": [
    "df_news_tmp.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Us Cities Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [cidemo_city#11347, cidemo_state#11361, cidemo_median_age#11376, cidemo_total_population#11427, cidemo_state_code#11509, cidemo_count#11556]\n",
      "   +- InMemoryRelation [cidemo_city#11347, cidemo_state#11361, cidemo_median_age#11376, cidemo_total_population#11427, cidemo_state_code#11509, cidemo_count#11556], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [City#11323 AS cidemo_city#11347, State#11324 AS cidemo_state#11361, cast(Median Age#11325 as float) AS cidemo_median_age#11376, cast(Total Population#11328 as int) AS cidemo_total_population#11427, State Code#11332 AS cidemo_state_code#11509, cast(Count#11334 as int) AS cidemo_count#11556]\n",
      "            +- FileScan csv [City#11323,State#11324,Median Age#11325,Total Population#11328,State Code#11332,Count#11334] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Median Age:string,Total Population:string,State Code:string,Count...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a us-cities data dimension table\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "pk: generated -> cidemo_id\n",
    "    schema: IntegerType()\n",
    "1. City -> cidemo_city\n",
    "    schema: StringType()\n",
    "2. State -> cidemo_state\n",
    "    schema: StringType()\n",
    "3. Median Age -> cidemo_median_age\n",
    "    schema: FloatType()\n",
    "4. Total Population -> cidemo_total_population\n",
    "    schema: IntegerType()\n",
    "5. State Code -> cidemo_state_code (fk)\n",
    "    schema: StringType()\n",
    "6. Count -> cidemo_count\n",
    "    schema: IntegerType()\n",
    "\"\"\"\n",
    "\n",
    "data_us_cities_demographics = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\"\n",
    "\n",
    "# TODO -> Must be defined a function that generated each table schema:\n",
    "us_cities_demographics_data_schema = StructType([\n",
    "    StructField(name=\"cidemo_city\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_median_age\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cidemo_total_population\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state_code\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_count\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using pyspark to read csv file\n",
    "df_news = spark.read.options(header=True, delimiter=';').csv(data_us_cities_demographics)\n",
    "\"\"\"\n",
    "root\n",
    " |-- cidemo_city: string (nullable = true)\n",
    " |-- cidemo_state: string (nullable = true)\n",
    " |-- cidemo_median_age: float (nullable = true)\n",
    " |-- cidemo_total_population: integer (nullable = true)\n",
    " |-- cidemo_state_code: string (nullable = true)\n",
    " |-- cidemo_count: integer (nullable = true)\n",
    "\"\"\"\n",
    "df_news = df_news.withColumn(\"cidemo_city\", col(\"City\").cast(\"String\")) \\\n",
    "                    .withColumn(\"cidemo_state\", col(\"State\").cast(\"String\")) \\\n",
    "                        .withColumn(\"cidemo_median_age\", col(\"Median Age\").cast(\"Float\")) \\\n",
    "                            .withColumn(\"cidemo_male_population\", col(\"Male Population\").cast(\"Integer\")) \\\n",
    "                                .withColumn(\"cidemo_female_population\", col(\"Female Population\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"cidemo_total_population\", col(\"Total Population\").cast(\"Integer\")) \\\n",
    "                                            .withColumn(\"cidemo_number_of_veterans\", col(\"Number of Veterans\").cast(\"Integer\")) \\\n",
    "                                                .withColumn(\"cidemo_foreign_born\", col(\"Foreign-born\").cast(\"Integer\")) \\\n",
    "                                                    .withColumn(\"cidemo_average_household_size\", col(\"Average Household Size\").cast(\"Float\")) \\\n",
    "                                                        .withColumn(\"cidemo_state_code\", col(\"State Code\").cast(\"String\")) \\\n",
    "                                                            .withColumn(\"cidemo_race\", col(\"Race\").cast(\"String\")) \\\n",
    "    .withColumn(\"cidemo_count\", col(\"Count\").cast(\"Integer\")) \\\n",
    "                    .select(col(\"cidemo_city\"),\n",
    "                            col(\"cidemo_state\"),\n",
    "                            col(\"cidemo_median_age\"),\n",
    "                            col(\"cidemo_total_population\"),\n",
    "                            col(\"cidemo_state_code\"),\n",
    "                            col(\"cidemo_count\"))\n",
    "\n",
    "# Auto-generated series of id\n",
    "df_us_cities_demographics = df_news.withColumn(\"cidemo_id\", monotonically_increasing_id())\n",
    "\n",
    "df_us_cities_demographics_temp = df_news.createOrReplaceTempView(\"us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp = spark.sql(\"SELECT * FROM us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp.persist()\n",
    "\n",
    "df_us_cities_demographics_temp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+-----------------+-----------------------+-----------------+------------+\n",
      "|cidemo_city|cidemo_state|cidemo_median_age|cidemo_total_population|cidemo_state_code|cidemo_count|\n",
      "+-----------+------------+-----------------+-----------------------+-----------------+------------+\n",
      "| Silver ...|    Maryland|             33.8|                  82463|               MD|       25924|\n",
      "|     Quincy|  Massach...|             41.0|                  93629|               MA|       58723|\n",
      "|     Hoover|     Alabama|             38.5|                  84839|               AL|        4759|\n",
      "| Rancho ...|  California|             34.5|                 175232|               CA|       24437|\n",
      "|     Newark|  New Jersey|             34.6|                 281913|               NJ|       76402|\n",
      "+-----------+------------+-----------------+-----------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_cities_demographics_temp.show(n=5, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cidemo_city: string, cidemo_state: string, cidemo_median_age: float, cidemo_total_population: int, cidemo_state_code: string, cidemo_count: int]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_cities_demographics_temp.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This code block is be placed in analysis jupyter notebook files\n",
    "imm_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "\n",
    "\n",
    "df_imm_data = spark.read.format(\"com.github.saurfang.sas.spark\").load(imm_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_imm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [imm_per_cic_id#1564, imm_person_birth_year#1594, imm_person_gender#1625, imm_visatype#1657]\n",
      "   +- InMemoryRelation [imm_per_cic_id#1564, imm_person_birth_year#1594, imm_person_gender#1625, imm_visatype#1657], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "         +- *(1) Project [cast(cicid#1333 as string) AS imm_per_cic_id#1564, cast(biryear#1353 as int) AS imm_person_birth_year#1594, gender#1355 AS imm_person_gender#1625, visatype#1360 AS imm_visatype#1657]\n",
      "            +- *(1) Scan SasRelation(/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat,null,false,false,false,None,false,false,false,false,60,None,None) [cicid#1333,i94yr#1334,i94mon#1335,i94cit#1336,i94res#1337,i94port#1338,arrdate#1339,i94mode#1340,i94addr#1341,depdate#1342,i94bir#1343,i94visa#1344,count#1345,dtadfile#1346,visapost#1347,occup#1348,entdepa#1349,entdepd#1350,entdepu#1351,matflag#1352,biryear#1353,dtaddto#1354,gender#1355,insnum#1356,... 4 more fields] PushedAggregates: [], PushedFilters: [], PushedGroupby: [], ReadSchema: struct<cicid:double,i94yr:double,i94mon:double,i94cit:double,i94res:double,i94port:string,arrdate...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "# show(n=5, truncate=5)\n",
    "df_immigration_personal = df_imm_data.withColumn(\"imm_per_cic_id\", col(\"cicid\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_person_birth_year\", col(\"biryear\").cast(\"Integer\"))\\\n",
    "           .withColumn(\"imm_person_gender\", col(\"gender\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_visatype\", col(\"visatype\").cast(\"String\")).select(col(\"imm_per_cic_id\"), \\\n",
    "                                                                              col(\"imm_person_birth_year\"), \\\n",
    "                                                                              col(\"imm_person_gender\"), \\\n",
    "                                                                              col(\"imm_visatype\"))\n",
    "\n",
    "df_immigration_personal_tmp = df_immigration_personal.createOrReplaceTempView(\"imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp = spark.sql(\"SELECT * FROM imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp.persist()\n",
    "\n",
    "df_immigration_personal_tmp.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+\n",
      "|imm_main_cic_id|imm_year|imm_month|imm_cntyl|imm_visa|imm_port|imm_arrival_date|imm_departure_date|imm_model|imm_address|imm_airline|imm_flight_no|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+\n",
      "|              6|    2016|        4|      692|       2|     XXX|      2016-04-29|              null|     null|       null|       null|         null|\n",
      "|              7|    2016|        4|      254|       3|     ATL|      2016-04-07|              null|        1|         AL|       null|        00296|\n",
      "|             15|    2016|        4|      101|       2|     WAS|      2016-04-01|        2016-08-25|        1|         MI|         OS|           93|\n",
      "|             16|    2016|        4|      101|       2|     NYC|      2016-04-01|        2016-04-23|        1|         MA|         AA|        00199|\n",
      "|             17|    2016|        4|      101|       2|     NYC|      2016-04-01|        2016-04-23|        1|         MA|         AA|        00199|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr: 4 digit year of the arrival  -> imm_year\n",
    "2. i94mon: numeric month of the arrival -> imm_month\n",
    "3. i94citi&i94res: 3 digit code of origin city -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa: reason for immigration -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port: 3 character code of destination city --> Foreign key (used to map to USDemographics and City Temperature data) -> imm_port\n",
    "6. arrdate: arrival date of the departure -> imm_arrival_date:\n",
    "7. deptdate: departure date\n",
    "date_add\n",
    "7. i94mode: 1 digit travel code -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_datetime(days: DoubleType) -> datetime:\n",
    "    \"\"\"convert_to_datetime converts days to datetime format\n",
    "\n",
    "    Args:\n",
    "        days (DoubleType): from sas arrive or departure date\n",
    "\n",
    "    Returns:\n",
    "        datetime: added days to datetime format result.\n",
    "    \"\"\"\n",
    "    if days is not None:\n",
    "        date = datetime.strptime('1960-01-01', '%Y-%m-%d')\n",
    "\n",
    "        return date + timedelta(days=days)\n",
    "\n",
    "udf_convert_to_datetime = udf(lambda x: convert_to_datetime(x), DateType())\n",
    "\n",
    "# df_imm_data.show(n=5, truncate=5)\n",
    "# t1 = df_imm_data.withColumn(\"imm_depdate\", col(\"depdate\")).select(col(\"imm_depdate\")).printSchema()\n",
    "\n",
    "immigration_main_information = df_imm_data.withColumn(\"imm_main_cic_id\", col(\"cicid\").cast(\"Integer\"))\\\n",
    "            .withColumn(\"imm_year\", col(\"i94yr\").cast(\"Integer\"))\\\n",
    "                .withColumn(\"imm_month\", col(\"i94mon\").cast(\"Integer\"))\\\n",
    "                    .withColumn(\"imm_cntyl\", col(\"i94cit\").cast(\"Integer\"))\\\n",
    "                        .withColumn(\"imm_visa\", col(\"i94visa\").cast(\"Integer\"))\\\n",
    "                            .withColumn(\"imm_port\", col(\"i94port\").cast(\"String\"))\\\n",
    "                                .withColumn(\"imm_arrival_date\", udf_convert_to_datetime(col(\"arrdate\")))\\\n",
    "                                    .withColumn(\"imm_departure_date\", udf_convert_to_datetime(col(\"depdate\")))\\\n",
    "                                        .withColumn(\"imm_model\", col(\"i94mode\").cast(\"Integer\"))\\\n",
    "                                            .withColumn(\"imm_address\", col(\"i94addr\").cast(\"String\"))\\\n",
    "                                                .withColumn(\"imm_airline\", col(\"airline\").cast(\"String\"))\\\n",
    "                                                    .withColumn(\"imm_flight_no\", col(\"fltno\").cast(\"String\"))\\\n",
    "        .select(col('imm_main_cic_id'), \\\n",
    "                    col('imm_year'),\\\n",
    "                        col('imm_month'),\\\n",
    "                            col('imm_cntyl'),\\\n",
    "                                col('imm_visa'),\\\n",
    "                                    col('imm_port'),\\\n",
    "                                        col('imm_arrival_date'),\\\n",
    "                                            col('imm_departure_date'),\\\n",
    "                                                col('imm_model'),\\\n",
    "                                                    col('imm_address'),\\\n",
    "                                                        col('imm_airline'),\\\n",
    "                                                            col('imm_flight_no'))\n",
    "\n",
    "df_immigration_main_information = immigration_main_information.createOrReplaceTempView(\n",
    "    \"immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information = spark.sql(\"SELECT * FROM immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information.show(n=5, truncate=10)\n",
    "# df_immigration_main_information.persist()\n",
    "\n",
    "# df_immigration_main_information.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_data_read = SAS7BDAT(imm_data)\n",
    "\n",
    "# imm_data_read.header\n",
    "# for i in imm_data_read.columns:\n",
    "#     print(\"col_id \", i.col_id)\n",
    "#     print(\"  name\",  i.name.decode(encoding ='utf-8'))\n",
    "#     print(\"  label\", i.label.decode(encoding ='utf-8'))\n",
    "#     print(\"  format\", i.format)\n",
    "#     print(\"  type\", i.type)\n",
    "#     print(\"  length\", i.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Dealing with multiple data files\n",
    "# TODO: Make a def for doing this.\n",
    "# Method1: Using pandas to read file.\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat', iterator=True, chunksize=5000000)\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat')\n",
    "\n",
    "# imm_chunks_1 = list(pdf_immigration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.io.sas.sas7bdat import SAS7BDATReader\n",
    "\n",
    "# rdr = SAS7BDATReader(imm_data, convert_header_text=False)\n",
    "# df3 = rdr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 268:>                                                        (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# TODO: Notification Table\n",
    "\"\"\"\n",
    "t2.imm_main_cic_id\n",
    "t2.imm_per_cic_id\n",
    "t2.news_cord_uid\n",
    "src.cidemo_id\n",
    "src.value_of_imm_destination_city\n",
    "t2.news_title\n",
    "t2.news_abstract\n",
    "t2.news_publish_time\n",
    "t2.news_authors\n",
    "\"\"\"\n",
    "\n",
    "#  ** t1: join imm two tables\n",
    "#  ** t2: join news table with t1\n",
    "#  ** t3: join us cities table with t2\n",
    "\n",
    "df_notification = spark.sql(\n",
    "    \"WITH t1 AS \\\n",
    "    (SELECT * \\\n",
    "       FROM immigration_main_information_data imid \\\n",
    "      LEFT JOIN imm_personal ip \\\n",
    "             ON imid.imm_main_cic_id = ip.imm_per_cic_id \\\n",
    "        WHERE imid.imm_year = 2016 \\\n",
    "     ), t2 AS \\\n",
    "        (SELECT * \\\n",
    "           FROM t1 \\\n",
    "         LEFT JOIN news_article_data nad \\\n",
    "                ON t1.imm_arrival_date = nad.news_publish_time \\\n",
    "     )\\\n",
    "    SELECT * \\\n",
    "       FROM t2 \\\n",
    "     LEFT JOIN (SELECT * FROM us_cities_demographics_data ucdd INNER JOIN imm_destination_city_data idcd ON ucdd.cidemo_state_code = idcd.value_of_alias_imm_destination_city) src \\\n",
    "            ON t2.imm_port = src.code_of_imm_destination_city \\\n",
    "    \"\n",
    ").dropDuplicates()\n",
    "\n",
    "df_notification.show(n=5, truncate=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48f95af79faaae749105c8389524df19196568350894b01545e0b6f755f2644b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
