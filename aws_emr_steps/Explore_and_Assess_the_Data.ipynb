{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "# from signal import signal, SIGPIPE, SIG_DFL\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, udf, to_date, count\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               DateType,\n",
    "                               FloatType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create local spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oneforall_nick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oneforall_nick/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/Users/oneforall_nick/spark-2.4.8-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-730c2d17-b0c9-45a8-98d9-85d870922a64;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      ":: resolution report :: resolve 319ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-730c2d17-b0c9-45a8-98d9-85d870922a64\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/7ms)\n",
      "22/06/26 09:20:28 WARN Utils: Your hostname, OneForAll-NickdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.104 instead (on interface en0)\n",
      "22/06/26 09:20:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/06/26 09:20:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark_emr_udactity\") \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check spark session information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.104:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_emr_udactity</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f84560c2c10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.app.name', 'spark_emr_udactity'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '192.168.1.104'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.app.id', 'local-1656206430658'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '52686'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access AWS S3 to get my source data After I upload data from local to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "# config.read_file(open('dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "# Access data from AWS S3\n",
    "# SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "SOURCE_S3_BUCKET = 's3://mydatapool'\n",
    "# Write data to AWS S3\n",
    "# DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n",
    "DEST_S3_BUCKET = 's3://destetlbucket'\n",
    "# *********************************************\n",
    "\n",
    "# ***** Local Testing configure ************\n",
    "# SOURCE_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/'\n",
    "# DEST_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/dest_data'\n",
    "\n",
    "# ***** Local Testing configure *****************\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")\n",
    "\n",
    "s3_access = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Label Data\n",
    "*Data format: <br>*\n",
    "    TXT\n",
    "*This step will seperate multiple tables:*\n",
    "- imm_cit_res\n",
    "- imm_port\n",
    "- imm_mod\n",
    "- imm_addr\n",
    "- imm_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** immigration_labels_descriptions ******\n",
    "\n",
    "# Get AWS S3 data Object: immigration_labels_descriptions.SAS\n",
    "s3_object = s3_access.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "text = s3_object['Body'].read()\n",
    "context = text.decode(encoding ='utf-8')\n",
    "# for obj in s3_object.objects.all():\n",
    "#     print(obj.key)\n",
    "\n",
    "context = context.replace('\\t', '')\n",
    "\n",
    "\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(\n",
    "        ';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\")\n",
    "                         for line in content_line_split]\n",
    "    content_two_dims = [i.strip().split('=') for i in content_line_list[1:]]\n",
    "    content_three_dims = [[i[0].strip(), i[1].strip().split(', ')[:][0], e]\n",
    "                          for i in content_two_dims if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_two_dims, content_three_dims\n",
    "\n",
    "# TODO:OK: three columns\n",
    "imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")\n",
    "df_imm_city_res_label = spark.sparkContext.parallelize(imm_cit_res_three).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\", \"value_of_imm_cntyl_organizations\"]) \\\n",
    "    .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl_organizations\").cast(\"String\")) \\\n",
    "    .select('col_of_imm_cntyl', 'value_of_imm_cntyl').distinct().dropDuplicates(['col_of_imm_cntyl'])\n",
    "\n",
    "df_imm_city_res_label.show()\n",
    "\n",
    "# TODO:OK: three columns\n",
    "imm_port_two, imm_port_three = code_mapping(context, \"i94prtl\")\n",
    "df_imm_destination_city = spark.sparkContext.parallelize(imm_port_three).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"]) \\\n",
    "                                                .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "                                                .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "                                                .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_destination_city'])\n",
    "\n",
    "df_imm_destination_city.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_mode_two, imm_mode_three = code_mapping(context, \"i94model\")\n",
    "df_imm_travel_code = spark.sparkContext.parallelize(imm_mode_two).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"]) \\\n",
    "                                           .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Float\")) \\\n",
    "                                           .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_travel_code'])\n",
    "df_imm_travel_code.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_addr_two, imm_addr_three = code_mapping(context, \"i94addrl\")\n",
    "df_imm_address = spark.sparkContext.parallelize(imm_addr_two).toDF([\"code_of_imm_address\", \"value_of_imm_address\"]) \\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_address'])\n",
    "\n",
    "df_imm_address.show()\n",
    "\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_visa = {'1': 'Business',\n",
    "            '2': 'Pleasure',\n",
    "            '3': 'Student'}\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(imm_visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"]) \\\n",
    "                                    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_visa'])\n",
    "df_imm_visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_mode_two, imm_mode_three = code_mapping(context, \"i94model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1 ', ' Air'], ['2 ', ' Sea'], ['3 ', ' Land'], ['9 ', ' Not reported']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_mode_two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm_travel_code = spark.sparkContext.parallelize(imm_mode_two).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"]) \\\n",
    "    .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Float\")) \\\n",
    "    .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: News\n",
    "- Data format: <br>\n",
    "    CSV\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/26 14:35:06 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [news_cord_uid#8553, news_source#8572, news_title#8592, news_licence#8613, news_abstract#8635, news_publish_time#8658, news_authors#8682, news_url#8707]\n",
      "   +- InMemoryRelation [news_cord_uid#8553, news_source#8572, news_title#8592, news_licence#8613, news_abstract#8635, news_publish_time#8658, news_authors#8682, news_url#8707], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(2) HashAggregate(keys=[news_source#4149, news_authors#4259, news_url#4284, news_publish_time#4235, news_cord_uid#4130, news_licence#4190, news_abstract#4212, news_title#4169], functions=[])\n",
      "            +- Exchange hashpartitioning(news_source#4149, news_authors#4259, news_url#4284, news_publish_time#4235, news_cord_uid#4130, news_licence#4190, news_abstract#4212, news_title#4169, 5)\n",
      "               +- *(1) HashAggregate(keys=[news_source#4149, news_authors#4259, news_url#4284, news_publish_time#4235, news_cord_uid#4130, news_licence#4190, news_abstract#4212, news_title#4169], functions=[])\n",
      "                  +- *(1) Project [cord_uid#4096 AS news_cord_uid#4130, source_x#4098 AS news_source#4149, title#4099 AS news_title#4169, license#4103 AS news_licence#4190, abstract#4104 AS news_abstract#4212, cast(cast(unix_timestamp(publish_time#4105, yyyy-MM-dd, Some(Asia/Taipei)) as timestamp) as date) AS news_publish_time#4235, authors#4106 AS news_authors#4259, url#4112 AS news_url#4284]\n",
      "                     +- *(1) FileScan csv [cord_uid#4096,source_x#4098,title#4099,license#4103,abstract#4104,publish_time#4105,authors#4106,url#4112] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cord_uid:string,source_x:string,title:string,license:string,abstract:string,publish_time:s...\n"
     ]
    }
   ],
   "source": [
    "# file path: data >> news_article\n",
    "\"\"\"Table: news_article schema\n",
    "pk: cord_uid -> news_cord_uid\n",
    "1. source_x -> news_source\n",
    "    schema: StringType()\n",
    "2. title -> news_title\n",
    "    schema: StringType()\n",
    "3. license -> news_licence\n",
    "    schema: StringType()\n",
    "4. abstract -> news_abstract\n",
    "    schema: StringType()\n",
    "5. publish_time -> news_publish_time (fk)\n",
    "    schema: TimestampType()\n",
    "6. authors -> news_authors\n",
    "    schema: StringType()\n",
    "7. url -> news_url\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "data_news = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=data_news)\n",
    "\n",
    "df_news = df_news.withColumn(\"news_cord_uid\", col(\"cord_uid\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_source\", col(\"source_x\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_title\", col(\"title\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_licence\", col(\"license\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_abstract\", col(\"abstract\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_publish_time\", to_date(col(\"publish_time\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"news_authors\", col(\"authors\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_url\", col(\"url\").cast(\"String\")) \\\n",
    "    .select(col(\"news_cord_uid\"),\n",
    "            col(\"news_source\"),\n",
    "            col(\"news_title\"),\n",
    "            col(\"news_licence\"),\n",
    "            col(\"news_abstract\"),\n",
    "            col(\"news_publish_time\"),\n",
    "            col(\"news_authors\"),\n",
    "            col(\"news_url\")).distinct()\n",
    "# .dropDuplicates(['news_cord_uid', 'news_source'])\n",
    "df_news_tmp = df_news.createOrReplaceTempView(\"news_article_data\")\n",
    "\n",
    "df_news_tmp = spark.sql(\"SELECT * FROM news_article_data\")\n",
    "\n",
    "df_news_tmp.persist()\n",
    "\n",
    "df_news_tmp.explain()\n",
    "\n",
    "# df_news_tmp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 341:>                                                        (0 + 8) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "|news_cord_uid|news_source|news_title|news_licence|news_abstract|news_publish_time|news_authors|  news_url|\n",
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "|     i9tbix2v|    biorxiv|Spatial...|     biorxiv|   An emer...|       2014-06-04|  Lin WAN...|https:/...|\n",
      "|     nljskxut|    biorxiv|What's ...|     biorxiv|   Whole g...|       2015-11-06|  Sissel ...|https:/...|\n",
      "|     lfm6erzy|    biorxiv|How dom...|     biorxiv|   Abstrac...|       2016-02-26|  Robert ...|https:/...|\n",
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_news.show(n=3, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- news_cord_uid: string (nullable = true)\n",
      " |-- news_source: string (nullable = true)\n",
      " |-- news_title: string (nullable = true)\n",
      " |-- news_licence: string (nullable = true)\n",
      " |-- news_abstract: string (nullable = true)\n",
      " |-- news_publish_time: date (nullable = true)\n",
      " |-- news_authors: string (nullable = true)\n",
      " |-- news_url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_news.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45807"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news.count() # drop duplicates column: uid not yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_news[df_news['news_cord_uid'].isNull()].show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|news_cord_uid     |count(news_cord_uid)|\n",
      "+------------------+--------------------+\n",
      "|c4u0gxp5          |2                   |\n",
      "|                  |2                   |\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select news_cord_uid, count(news_cord_uid) from news_article_data group by news_cord_uid having count(news_cord_uid) > 1\").show(\n",
    "    truncate=False)\n",
    "\n",
    "uuid = 'news_cord_uid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 418:=======>                                                 (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+\n",
      "|     news_cord_uid|check_duplicated_IDs|\n",
      "+------------------+--------------------+\n",
      "|          c4u0gxp5|                   2|\n",
      "|                  |                   2|\n",
      "+------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Check Duplicate data record\n",
    "df_news.groupBy(col(uuid)).agg(count(uuid).alias('check_duplicated_IDs')).filter(col('check_duplicated_IDs') > 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "The table has duplicated IDs please check it: Column<b'news_cord_uid'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lh/ngst1lt51m9_gyz7qrg2f3nh0000gn/T/ipykernel_15360/1372220196.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     raise Exception(\n\u001b[0;32m----> 5\u001b[0;31m         f\"The table has duplicated IDs please check it: {df_news[uuid]}\")\n\u001b[0m",
      "\u001b[0;31mException\u001b[0m: The table has duplicated IDs please check it: Column<b'news_cord_uid'>"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/26 19:17:12 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 4594226 ms exceeds timeout 120000 ms\n",
      "22/06/26 19:17:12 ERROR TaskSchedulerImpl: Lost executor driver on localhost: Executor heartbeat timed out after 4594226 ms\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_762_2 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_762_3 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_762_0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_384_piece0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_69_piece0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_973_0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_762_4 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_308_piece0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_762_1 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_15_piece0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_78_piece0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for rdd_100_0 !\n",
      "22/06/26 19:17:12 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_54_piece0 !\n",
      "22/06/26 19:17:12 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if df_news.groupBy(col(uuid)).agg(count(uuid).alias('check_duplicated_IDs')).filter(col('check_duplicated_IDs') > 1).count() == 0:\n",
    "    print(\"The table: df_news has no duplicated IDs\")\n",
    "else:\n",
    "    raise Exception(\n",
    "        f\"The table has duplicated IDs please check it: {df_news[uuid]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClientError",
     "evalue": "An error occurred (ExpiredToken) when calling the ListObjectsV2 operation: The provided token has expired.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lh/ngst1lt51m9_gyz7qrg2f3nh0000gn/T/ipykernel_15360/2241238455.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdim_bucket_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nick_udacity_capstone_project/lib/python3.7/site-packages/s3path.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \"\"\"\n\u001b[1;32m    704\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_absolute_path_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nick_udacity_capstone_project/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1108\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nick_udacity_capstone_project/lib/python3.7/site-packages/s3path.py\u001b[0m in \u001b[0;36mlistdir\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscandir_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscandir_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nick_udacity_capstone_project/lib/python3.7/site-packages/s3path.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscandir_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscandir_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nick_udacity_capstone_project/lib/python3.7/site-packages/s3path.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontinuation_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ContinuationToken'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontinuation_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_objects_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CommonPrefixes'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mfull_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prefix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prefix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Prefix'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nick_udacity_capstone_project/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m                 )\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/nick_udacity_capstone_project/lib/python3.7/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (ExpiredToken) when calling the ListObjectsV2 operation: The provided token has expired."
     ]
    }
   ],
   "source": [
    "# TODO: ***** Using in Data Quality *****\n",
    "from s3path import S3Path\n",
    "\n",
    "# Expection duplicated IDs Count\n",
    "EXPECTION_DUPLICATED_IDS_COUNT = 0\n",
    "\n",
    "# Source Data Count After filter processed\n",
    "df_immigration_personal_source_count = 123\n",
    "df_immigration_main_information_source_count = 123\n",
    "df_news_source_count = 123\n",
    "df_us_cities_demographics_source_count = 123\n",
    "df_imm_destination_city_source_count = 123\n",
    "df_imm_city_res_label_source_count = 123\n",
    "df_imm_travel_code_source_count = 123\n",
    "df_imm_address_source_count = 123\n",
    "df_imm_visa_source_count = 123\n",
    "\n",
    "# Dest AWS S3 Bucket\n",
    "dest_aws_s3_bucket = 'destetlbucket'\n",
    "dim_bucket_path = S3Path(f\"/{dest_aws_s3_bucket}/dimension_table\")\n",
    "\n",
    "\n",
    "# Each table's table name, unique ID key name, EXPECTION_DUPLICATED_IDS_COUNT and source data count\n",
    "dict_dimension_table_IDs = {\n",
    "    'df_immigration_personal': [\n",
    "        'imm_per_cic_id',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_immigration_personal_source_count\n",
    "    ],\n",
    "    'df_immigration_main_information': [\n",
    "        'imm_main_cic_id',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_immigration_main_information_source_count\n",
    "    ],\n",
    "    'df_news': [\n",
    "        'news_cord_uid',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_news_source_count\n",
    "    ],\n",
    "    'df_us_cities_demographics': [\n",
    "        'cidemo_id',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_us_cities_demographics_source_count\n",
    "    ],\n",
    "    # ***** imm_cit_res *****\n",
    "    'df_imm_city_res_label': [\n",
    "        'col_of_imm_cntyl',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_imm_city_res_label_source_count\n",
    "    ],\n",
    "    # ***** imm_port *****\n",
    "    'df_imm_destination_city': [\n",
    "        'code_of_imm_destination_city',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_imm_destination_city_source_count\n",
    "    ],\n",
    "    # ***** imm_mod *****\n",
    "    'df_imm_travel_code': [\n",
    "        'code_of_imm_travel_code',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_imm_travel_code_source_count\n",
    "    ],\n",
    "    # ***** imm_addr *****\n",
    "    'df_imm_address': [\n",
    "        'code_of_imm_address',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_imm_address_source_count\n",
    "    ],\n",
    "    # ***** imm_visa *****\n",
    "    'df_imm_visa': [\n",
    "        'code_of_imm_visa',\n",
    "        EXPECTION_DUPLICATED_IDS_COUNT,\n",
    "        df_imm_visa_source_count\n",
    "    ]\n",
    "}\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "for path in dim_bucket_path.iterdir():\n",
    "    if path.is_dir():\n",
    "        path = str(path)\n",
    "        # For loop this dict_dimension_table_IDs dictionary data to check my duplicated IDs, etc.\n",
    "        for dim_table, key_duplicated_source_etl in dict_dimension_table_IDs.items():\n",
    "            dimension_table_name = path.split('/')[-1]\n",
    "            if dimension_table_name in dim_table:\n",
    "                df = spark.read.parquet(f\"s3:/{path}\")\n",
    "\n",
    "                # For checking each dimension table's IDs is duplicated or not.\n",
    "                check_dataframe_duplicated_IDs_count = df.groupBy(key_duplicated_source_etl[0]).agg(\n",
    "                    count(key_duplicated_source_etl[0]).alias('check_duplicated_IDs')).filter(col('check_duplicated_IDs') > 1).count()\n",
    "\n",
    "                if check_dataframe_duplicated_IDs_count != key_duplicated_source_etl[1]:\n",
    "                    raise Exception(f\"Check table {dim_table} has duplicated IDs, not expected {key_duplicated_source_etl[1]}!!\")\n",
    "                else:\n",
    "                    print(f\"The table {dim_table} is expectations of value {key_duplicated_source_etl[1]}.\")\n",
    "\n",
    "\n",
    "\n",
    "# ***** Checking Source Data Count but figure out some filter condition to make sure it's correctly. *****\n",
    "for path in dim_bucket_path.iterdir():\n",
    "    if path.is_dir():\n",
    "        path = str(path)\n",
    "        df = spark.read.parquet(f\"s3:/{path}\")\n",
    "        if df.count() <= 0:\n",
    "            print(\"Table is empty\")\n",
    "        else:\n",
    "            print(f\"Table Name: {str(path).split('/')[-1]}, Total Rows: {df.count():,}, Total Columns: {len(df.dtypes)}\")\n",
    "            df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Us Cities Demographics data\n",
    "- Data format: <br>\n",
    "    CSV\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [cidemo_city#6937, cidemo_state#6952, cidemo_median_age#6968, cidemo_total_population#7022, cidemo_state_code#7108, cidemo_count#7157, cidemo_id#7189L]\n",
      "   +- InMemoryRelation [cidemo_city#6937, cidemo_state#6952, cidemo_median_age#6968, cidemo_total_population#7022, cidemo_state_code#7108, cidemo_count#7157, cidemo_id#7189L], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(1) Project [City#6899 AS cidemo_city#6937, State#6900 AS cidemo_state#6952, cast(Median Age#6901 as float) AS cidemo_median_age#6968, cast(Total Population#6904 as int) AS cidemo_total_population#7022, State Code#6908 AS cidemo_state_code#7108, cast(Count#6910 as int) AS cidemo_count#7157, monotonically_increasing_id() AS cidemo_id#7189L]\n",
      "            +- *(1) FileScan csv [City#6899,State#6900,Median Age#6901,Male Population#6902,Female Population#6903,Total Population#6904,Number of Veterans#6905,Foreign-born#6906,Average Household Size#6907,State Code#6908,Race#6909,Count#6910] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Median Age:string,Male Population:string,Female Population:string...\n"
     ]
    }
   ],
   "source": [
    "# Create a us-cities data dimension table\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "pk: generated -> cidemo_id\n",
    "    schema: IntegerType()\n",
    "1. City -> cidemo_city\n",
    "    schema: StringType()\n",
    "2. State -> cidemo_state\n",
    "    schema: StringType()\n",
    "3. Median Age -> cidemo_median_age\n",
    "    schema: FloatType()\n",
    "4. Total Population -> cidemo_total_population\n",
    "    schema: IntegerType()\n",
    "5. State Code -> cidemo_state_code (fk)\n",
    "    schema: StringType()\n",
    "6. Count -> cidemo_count\n",
    "    schema: IntegerType()\n",
    "\"\"\"\n",
    "\n",
    "data_us_cities_demographics = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\"\n",
    "\n",
    "# -> Must be defined a function that generated each table schema:\n",
    "us_cities_demographics_data_schema = StructType([\n",
    "    StructField(name=\"cidemo_city\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_median_age\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cidemo_total_population\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state_code\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_count\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using pyspark to read csv file\n",
    "df_us_cities_demographics = spark.read.options(header=True, delimiter=';').csv(data_us_cities_demographics)\n",
    "\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_id\", monotonically_increasing_id()) \\\n",
    "                .withColumn(\"cidemo_city\", col(\"City\").cast(\"String\")) \\\n",
    "                    .withColumn(\"cidemo_state\", col(\"State\").cast(\"String\")) \\\n",
    "                        .withColumn(\"cidemo_median_age\", col(\"Median Age\").cast(\"Float\")) \\\n",
    "                            .withColumn(\"cidemo_male_population\", col(\"Male Population\").cast(\"Integer\")) \\\n",
    "                                .withColumn(\"cidemo_female_population\", col(\"Female Population\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"cidemo_total_population\", col(\"Total Population\").cast(\"Integer\")) \\\n",
    "                                            .withColumn(\"cidemo_number_of_veterans\", col(\"Number of Veterans\").cast(\"Integer\")) \\\n",
    "                                                .withColumn(\"cidemo_foreign_born\", col(\"Foreign-born\").cast(\"Integer\")) \\\n",
    "                                                    .withColumn(\"cidemo_average_household_size\", col(\"Average Household Size\").cast(\"Float\")) \\\n",
    "                                                        .withColumn(\"cidemo_state_code\", col(\"State Code\").cast(\"String\")) \\\n",
    "                                                            .withColumn(\"cidemo_race\", col(\"Race\").cast(\"String\")) \\\n",
    "    .withColumn(\"cidemo_count\", col(\"Count\").cast(\"Integer\")) \\\n",
    "                    .select(col(\"cidemo_city\"),\n",
    "                            col(\"cidemo_state\"),\n",
    "                            col(\"cidemo_median_age\"),\n",
    "                            col(\"cidemo_total_population\"),\n",
    "                            col(\"cidemo_state_code\"),\n",
    "                            col(\"cidemo_count\"))\n",
    "\n",
    "# Auto-generated series of id\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_id\", monotonically_increasing_id())\n",
    "\n",
    "df_us_cities_demographics_temp = df_us_cities_demographics.createOrReplaceTempView(\"us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp = spark.sql(\"SELECT * FROM us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp.persist()\n",
    "\n",
    "df_us_cities_demographics_temp.explain()\n",
    "\n",
    "# df_us_cities_demographics_temp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "|     cidemo_city|  cidemo_state|cidemo_median_age|cidemo_total_population|cidemo_state_code|cidemo_count|cidemo_id|\n",
      "+----------------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "|   Silver Spring|      Maryland|             33.8|                  82463|               MD|       25924|        0|\n",
      "|          Quincy| Massachusetts|             41.0|                  93629|               MA|       58723|        1|\n",
      "|          Hoover|       Alabama|             38.5|                  84839|               AL|        4759|        2|\n",
      "|Rancho Cucamonga|    California|             34.5|                 175232|               CA|       24437|        3|\n",
      "|          Newark|    New Jersey|             34.6|                 281913|               NJ|       76402|        4|\n",
      "|          Peoria|      Illinois|             33.1|                 118661|               IL|        1343|        5|\n",
      "|        Avondale|       Arizona|             29.1|                  80683|               AZ|       11592|        6|\n",
      "|     West Covina|    California|             39.8|                 108489|               CA|       32716|        7|\n",
      "|        O'Fallon|      Missouri|             36.0|                  85032|               MO|        2583|        8|\n",
      "|      High Point|North Carolina|             35.5|                 109828|               NC|       11060|        9|\n",
      "|          Folsom|    California|             40.9|                  76368|               CA|        5822|       10|\n",
      "|          Folsom|    California|             40.9|                  76368|               CA|         998|       11|\n",
      "|    Philadelphia|  Pennsylvania|             34.1|                1567442|               PA|      122721|       12|\n",
      "|         Wichita|        Kansas|             34.6|                 389955|               KS|       65162|       13|\n",
      "|         Wichita|        Kansas|             34.6|                 389955|               KS|        8791|       14|\n",
      "|      Fort Myers|       Florida|             37.3|                  74015|               FL|       50169|       15|\n",
      "|      Pittsburgh|  Pennsylvania|             32.9|                 304385|               PA|      208863|       16|\n",
      "|          Laredo|         Texas|             28.8|                 255789|               TX|        1253|       17|\n",
      "|        Berkeley|    California|             32.5|                 120971|               CA|       27089|       18|\n",
      "|     Santa Clara|    California|             35.2|                 126216|               CA|       55847|       19|\n",
      "+----------------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_cities_demographics_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cidemo_city: string (nullable = true)\n",
      " |-- cidemo_state: string (nullable = true)\n",
      " |-- cidemo_median_age: float (nullable = true)\n",
      " |-- cidemo_total_population: integer (nullable = true)\n",
      " |-- cidemo_state_code: string (nullable = true)\n",
      " |-- cidemo_count: integer (nullable = true)\n",
      " |-- cidemo_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_cities_demographics_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,891'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{df_us_cities_demographics_temp.count():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|cidemo_id|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cidemo_id from us_cities_demographics_data group by cidemo_id having count(cidemo_id) > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** imm_data ******\n",
    "imm_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "df_imm_data = spark.read.format('com.github.saurfang.sas.spark').load(imm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3,096,313'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{df_imm_data.count():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [cicid#1237, i94yr#1238, i94mon#1239, i94cit#1240, i94res#1241, i94port#1242, arrdate#1243, i94mode#1244, i94addr#1245, depdate#1246, i94bir#1247, i94visa#1248, count#1249, dtadfile#1250, visapost#1251, occup#1252, entdepa#1253, entdepd#1254, entdepu#1255, matflag#1256, biryear#1257, dtaddto#1258, gender#1259, insnum#1260, ... 4 more fields]\n",
      "   +- InMemoryRelation [cicid#1237, i94yr#1238, i94mon#1239, i94cit#1240, i94res#1241, i94port#1242, arrdate#1243, i94mode#1244, i94addr#1245, depdate#1246, i94bir#1247, i94visa#1248, count#1249, dtadfile#1250, visapost#1251, occup#1252, entdepa#1253, entdepd#1254, entdepu#1255, matflag#1256, biryear#1257, dtaddto#1258, gender#1259, insnum#1260, ... 4 more fields], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(1) Scan SasRelation(/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0) [cicid#1237,i94yr#1238,i94mon#1239,i94cit#1240,i94res#1241,i94port#1242,arrdate#1243,i94mode#1244,i94addr#1245,depdate#1246,i94bir#1247,i94visa#1248,count#1249,dtadfile#1250,visapost#1251,occup#1252,entdepa#1253,entdepd#1254,entdepu#1255,matflag#1256,biryear#1257,dtaddto#1258,gender#1259,insnum#1260,... 4 more fields] PushedFilters: [], ReadSchema: struct<cicid:double,i94yr:double,i94mon:double,i94cit:double,i94res:double,i94port:string,arrdate...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/26 11:52:08 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "df_imm_data_tmp = df_imm_data.createOrReplaceTempView(\"imm_data\")\n",
    "\n",
    "df_imm_data_tmp = spark.sql(\"select * from imm_data\")\n",
    "\n",
    "df_imm_data_tmp.persist()\n",
    "\n",
    "df_imm_data_tmp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if spark.sql(\"select cicid from imm_data group by cicid having count(cicid) > 1\").count() == 0:\n",
    "    print(\"This source table df_imm_data has no duplicated record\")\n",
    "    # TODO: write out a parquet file and restored in AWS S3\n",
    "else:\n",
    "    spark.sql(\"select cicid from imm_data group by cicid having count(cicid) > 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration personal data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [imm_per_cic_id#611, imm_person_birth_year#641, imm_person_gender#672, imm_visatype#704]\n",
      "   +- InMemoryRelation [imm_per_cic_id#611, imm_person_birth_year#641, imm_person_gender#672, imm_visatype#704], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(1) Project [cast(cicid#0 as string) AS imm_per_cic_id#611, cast(biryear#20 as int) AS imm_person_birth_year#641, gender#22 AS imm_person_gender#672, visatype#27 AS imm_visatype#704]\n",
      "            +- *(1) Scan SasRelation(/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0) [cicid#0,i94yr#1,i94mon#2,i94cit#3,i94res#4,i94port#5,arrdate#6,i94mode#7,i94addr#8,depdate#9,i94bir#10,i94visa#11,count#12,dtadfile#13,visapost#14,occup#15,entdepa#16,entdepd#17,entdepu#18,matflag#19,biryear#20,dtaddto#21,gender#22,insnum#23,... 4 more fields] PushedFilters: [], ReadSchema: struct<cicid:double,i94yr:double,i94mon:double,i94cit:double,i94res:double,i94port:string,arrdate...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "# show(n=5, truncate=5)\n",
    "df_immigration_personal = df_imm_data.withColumn(\"imm_per_cic_id\", col(\"cicid\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_person_birth_year\", col(\"biryear\").cast(\"Integer\"))\\\n",
    "           .withColumn(\"imm_person_gender\", col(\"gender\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_visatype\", col(\"visatype\").cast(\"String\")).select(col(\"imm_per_cic_id\"), \\\n",
    "                                                                              col(\"imm_person_birth_year\"), \\\n",
    "                                                                              col(\"imm_person_gender\"), \\\n",
    "                                                                              col(\"imm_visatype\"))\n",
    "\n",
    "df_immigration_personal_tmp = df_immigration_personal.createOrReplaceTempView(\"imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp = spark.sql(\"SELECT * FROM imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp.persist()\n",
    "\n",
    "df_immigration_personal_tmp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------------+-----------------+------------+\n",
      "|imm_per_cic_id|imm_person_birth_year|imm_person_gender|imm_visatype|\n",
      "+--------------+---------------------+-----------------+------------+\n",
      "|           6.0|                 1979|             null|          B2|\n",
      "|           7.0|                 1991|                M|          F1|\n",
      "|          15.0|                 1961|                M|          B2|\n",
      "|          16.0|                 1988|             null|          B2|\n",
      "|          17.0|                 2012|             null|          B2|\n",
      "+--------------+---------------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_personal_tmp.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration main data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [imm_main_cic_id#1799, imm_year#1829, imm_month#1860, imm_cntyl#1892, imm_visa#1925, imm_port#1959, imm_arrival_date#1995, imm_departure_date#2032, imm_model#2069, imm_address#2107, imm_airline#2146, imm_flight_no#2186]\n",
      "   +- InMemoryRelation [imm_main_cic_id#1799, imm_year#1829, imm_month#1860, imm_cntyl#1892, imm_visa#1925, imm_port#1959, imm_arrival_date#1995, imm_departure_date#2032, imm_model#2069, imm_address#2107, imm_airline#2146, imm_flight_no#2186], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(2) Project [cast(cicid#1237 as int) AS imm_main_cic_id#1799, cast(i94yr#1238 as int) AS imm_year#1829, cast(i94mon#1239 as int) AS imm_month#1860, cast(i94cit#1240 as int) AS imm_cntyl#1892, cast(i94visa#1248 as int) AS imm_visa#1925, i94port#1242 AS imm_port#1959, pythonUDF0#2251 AS imm_arrival_date#1995, pythonUDF1#2252 AS imm_departure_date#2032, cast(i94mode#1244 as int) AS imm_model#2069, i94addr#1245 AS imm_address#2107, airline#1261 AS imm_airline#2146, fltno#1263 AS imm_flight_no#2186]\n",
      "            +- BatchEvalPython [<lambda>(arrdate#1243), <lambda>(depdate#1246)], [airline#1261, arrdate#1243, cicid#1237, depdate#1246, fltno#1263, i94addr#1245, i94cit#1240, i94mode#1244, i94mon#1239, i94port#1242, i94visa#1248, i94yr#1238, pythonUDF0#2251, pythonUDF1#2252]\n",
      "               +- *(1) Project [airline#1261, arrdate#1243, cicid#1237, depdate#1246, fltno#1263, i94addr#1245, i94cit#1240, i94mode#1244, i94mon#1239, i94port#1242, i94visa#1248, i94yr#1238]\n",
      "                  +- *(1) Scan SasRelation(/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0) [cicid#1237,i94yr#1238,i94mon#1239,i94cit#1240,i94res#1241,i94port#1242,arrdate#1243,i94mode#1244,i94addr#1245,depdate#1246,i94bir#1247,i94visa#1248,count#1249,dtadfile#1250,visapost#1251,occup#1252,entdepa#1253,entdepd#1254,entdepu#1255,matflag#1256,biryear#1257,dtaddto#1258,gender#1259,insnum#1260,... 4 more fields] PushedFilters: [], ReadSchema: struct<cicid:double,i94yr:double,i94mon:double,i94cit:double,i94res:double,i94port:string,arrdate...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr: 4 digit year of the arrival  -> imm_year\n",
    "2. i94mon: numeric month of the arrival -> imm_month\n",
    "3. i94citi&i94res: 3 digit code of origin city -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa: reason for immigration -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port: 3 character code of destination city --> Foreign key (used to map to USDemographics and City Temperature data) -> imm_port\n",
    "6. arrdate: arrival date of the departure -> imm_arrival_date:\n",
    "7. deptdate: departure date\n",
    "date_add\n",
    "7. i94mode: 1 digit travel code -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_datetime(days: DoubleType) -> datetime:\n",
    "    \"\"\"convert_to_datetime converts days to datetime format\n",
    "\n",
    "    Args:\n",
    "        days (DoubleType): from sas arrive or departure date\n",
    "\n",
    "    Returns:\n",
    "        datetime: added days to datetime format result.\n",
    "    \"\"\"\n",
    "    if days is not None:\n",
    "        date = datetime.strptime('1960-01-01', '%Y-%m-%d')\n",
    "\n",
    "        return date + timedelta(days=days)\n",
    "\n",
    "udf_convert_to_datetime = udf(lambda x: convert_to_datetime(x), DateType())\n",
    "\n",
    "immigration_main_information = df_imm_data.withColumn(\"imm_main_cic_id\", col(\"cicid\").cast(\"Integer\"))\\\n",
    "            .withColumn(\"imm_year\", col(\"i94yr\").cast(\"Integer\"))\\\n",
    "                .withColumn(\"imm_month\", col(\"i94mon\").cast(\"Integer\"))\\\n",
    "                    .withColumn(\"imm_cntyl\", col(\"i94cit\").cast(\"Integer\"))\\\n",
    "                        .withColumn(\"imm_visa\", col(\"i94visa\").cast(\"Integer\"))\\\n",
    "                            .withColumn(\"imm_port\", col(\"i94port\").cast(\"String\"))\\\n",
    "                                .withColumn(\"imm_arrival_date\", udf_convert_to_datetime(col(\"arrdate\")))\\\n",
    "                                    .withColumn(\"imm_departure_date\", udf_convert_to_datetime(col(\"depdate\")))\\\n",
    "                                        .withColumn(\"imm_model\", col(\"i94mode\").cast(\"Integer\"))\\\n",
    "                                            .withColumn(\"imm_address\", col(\"i94addr\").cast(\"String\"))\\\n",
    "                                                .withColumn(\"imm_airline\", col(\"airline\").cast(\"String\"))\\\n",
    "                                                    .withColumn(\"imm_flight_no\", col(\"fltno\").cast(\"String\"))\\\n",
    "        .select(col('imm_main_cic_id'), \\\n",
    "                    col('imm_year'),\\\n",
    "                        col('imm_month'),\\\n",
    "                            col('imm_cntyl'),\\\n",
    "                                col('imm_visa'),\\\n",
    "                                    col('imm_port'),\\\n",
    "                                        col('imm_arrival_date'),\\\n",
    "                                            col('imm_departure_date'),\\\n",
    "                                                col('imm_model'),\\\n",
    "                                                    col('imm_address'),\\\n",
    "                                                        col('imm_airline'),\\\n",
    "                                                            col('imm_flight_no'))\n",
    "\n",
    "df_immigration_main_information = immigration_main_information.createOrReplaceTempView(\n",
    "    \"immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information = spark.sql(\"SELECT * FROM immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information.persist()\n",
    "\n",
    "df_immigration_main_information.explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imm_main_cic_id: integer (nullable = true)\n",
      " |-- imm_year: integer (nullable = true)\n",
      " |-- imm_month: integer (nullable = true)\n",
      " |-- imm_cntyl: integer (nullable = true)\n",
      " |-- imm_visa: integer (nullable = true)\n",
      " |-- imm_port: string (nullable = true)\n",
      " |-- imm_arrival_date: date (nullable = true)\n",
      " |-- imm_departure_date: date (nullable = true)\n",
      " |-- imm_model: integer (nullable = true)\n",
      " |-- imm_address: string (nullable = true)\n",
      " |-- imm_airline: string (nullable = true)\n",
      " |-- imm_flight_no: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_main_information.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact: Nofification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=======================>                                  (2 + 3) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "|imm_main_cic_id|imm_year|imm_month|imm_cntyl|imm_visa|imm_port|imm_arrival_date|imm_departure_date|imm_model|imm_address|imm_airline|imm_flight_no|imm_per_cic_id|imm_person_birth_year|imm_person_gender|imm_visatype|news_cord_uid|news_source|news_title|news_licence|news_abstract|news_publish_time|news_authors|news_url|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          vhs|        PMC|       The|         cc-|          RNA|              201|         Jär|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          1am|        PMC|       Pre|         cc-|          We |              201|         Guo|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          4ov|        PMC|       Doe|         cc-|          The|              201|         Poo|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          rkd|        PMC|       Mea|         cc-|          Bil|              201|         Ken|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          jns|        PMC|       DNA|         cc-|          To |              201|         Liu|     htt|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/23 23:02:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2361589 ms exceeds timeout 120000 ms\n",
      "22/06/23 23:02:40 ERROR TaskSchedulerImpl: Lost executor driver on localhost: Executor heartbeat timed out after 2361589 ms\n",
      "22/06/23 23:02:40 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_10_piece0 !\n",
      "22/06/23 23:02:40 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_11_piece0 !\n",
      "22/06/23 23:02:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Notification Table\n",
    "\"\"\"\n",
    "t2.imm_main_cic_id\n",
    "t2.imm_per_cic_id\n",
    "t2.news_cord_uid\n",
    "src.cidemo_id\n",
    "src.value_of_imm_destination_city\n",
    "t2.news_title\n",
    "t2.news_abstract\n",
    "t2.news_publish_time\n",
    "t2.news_authors\n",
    "\"\"\"\n",
    "\n",
    "#  ** t1: join imm two tables\n",
    "#  ** t2: join news table with t1\n",
    "#  ** t3: join us cities table with t2\n",
    "\n",
    "df_notification = spark.sql(\n",
    "        \"WITH t1 AS \\\n",
    "            (SELECT * \\\n",
    "               FROM immigration_main_information_data imid \\\n",
    "             INNER JOIN imm_personal ip \\\n",
    "                    ON imid.imm_main_cic_id = ip.imm_per_cic_id \\\n",
    "                 WHERE imid.imm_year = 2016 \\\n",
    "            ), t2 AS \\\n",
    "                (SELECT * \\\n",
    "                   FROM t1 \\\n",
    "                 INNER JOIN news_article_data nad \\\n",
    "                        ON t1.imm_arrival_date = nad.news_publish_time \\\n",
    "            ) \\\n",
    "            SELECT  * \\\n",
    "              FROM t2 \\\n",
    "            LIMIT 5 \\\n",
    "        \"\n",
    "    )\n",
    "\n",
    "df_notification.show(n=5, truncate=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f40c152a15a669d798eb1ad4e6f345bdd77350f6745bfc8751a72382d50440f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nick_udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
