{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import logging\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from signal import signal, SIGPIPE, SIG_DFL\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, udf, to_date\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               DateType,\n",
    "                               FloatType)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/17 12:17:30 WARN Utils: Your hostname, OneForAll-NickdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.20.10.3 instead (on interface en0)\n",
      "22/06/17 12:17:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oneforall_nick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oneforall_nick/.ivy2/jars\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1b3b8564-94b5-4eb8-b740-2e59b82fb841;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      ":: resolution report :: resolve 269ms :: artifacts dl 10ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-1b3b8564-94b5-4eb8-b740-2e59b82fb841\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/8ms)\n",
      "22/06/17 12:17:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark_emr_udactity\") \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_emr_udactity</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fac03eda220>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.parquet.binaryAsString', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://172.20.10.3:49266/jars/com.epam_parso-2.0.8.jar,spark://172.20.10.3:49266/jars/org.scala-lang_scala-reflect-2.11.8.jar,spark://172.20.10.3:49266/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,spark://172.20.10.3:49266/jars/org.slf4j_slf4j-api-1.7.5.jar,spark://172.20.10.3:49266/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar'),\n",
       " ('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.app.name', 'spark_emr_udactity'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.driver.host', '172.20.10.3'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/spark-warehouse'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.app.startTime', '1655439452045'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.app.id', 'local-1655439453242'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '49266')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** Without Catching SIGPIPE *********\n",
    "signal(SIGPIPE, SIG_DFL)\n",
    "# ****************************************\n",
    "\n",
    "\n",
    "# ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(\n",
    "    open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "# config.read_file(open('dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "# Access data from AWS S3\n",
    "# SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "SOURCE_S3_BUCKET = 's3://mydatapool'\n",
    "# Write data to AWS S3\n",
    "# DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n",
    "DEST_S3_BUCKET = 's3://destetlbucket'\n",
    "# *********************************************\n",
    "\n",
    "# TODO ***** Local Testing configure ************\n",
    "# SOURCE_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/'\n",
    "# DEST_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/dest_data'\n",
    "\n",
    "# ***** Local Testing configure *****************\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")\n",
    "\n",
    "s3_access = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "# config.read_file(open('s3://mydatapool/config/dl.cfg'))\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "\n",
    "# s3://mydatapool/config/dl.cfg\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "# Access data from AWS S3\n",
    "SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "# SOURCE_S3_BUCKET = 's3://mydatapool'\n",
    "# Write data to AWS S3\n",
    "DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "# import boto3\n",
    "\n",
    "# session = boto3.Session(\n",
    "#     aws_access_key_id=aws_access_key,\n",
    "#     aws_secret_access_key=aws_secret_access_key\n",
    "# )\n",
    "\n",
    "# s3 = session.resource('s3')\n",
    "\n",
    "# s3_object = s3.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "# text = s3_object['Body'].read()\n",
    "# context = text.decode(encoding ='utf-8')\n",
    "# # for obj in s3_object.objects.all():\n",
    "# #     print(obj.key)\n",
    "\n",
    "# context = context.replace('\\t', '')\n",
    "# context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ****** Read data from local file system(be a s3) to spark DataFrame ******\n",
    "# >>> s3_session = boto3.Session(aws_access_key_id='ASIAUW7BQYXKBE3COG4H',\n",
    "# ...                                    aws_secret_access_key='Bsqf1Tg/ajFm0bzsXiYs1zPxAeF65m5ZYNVI7E2b',\n",
    "# ...                                    aws_session_token='FwoGZXIvYXdzEIv//////////wEaDOPwODxNOGtesK0FRiLVAcHA4fdotLDXjivG9ANe54T0wdGYPdIKv8YAF1oeYdR1ThW9fo4J8Alx0DoSkOElGsCIZy7BHmJtl0LlpLlJ3wryLbZNJZ6AKODGit5OL8/E+4HgOtaJkPBUdtuwdVyg1IexI3qEE6v3f14axyNe32tt1MnsBOPHWniw3NpDSRJz/PksFf4VWkyCT6u5e9ebEWabfsAIZb3PBxhGGv0tZnA+4TZT2NQ+qYyOPFPUgCUiQalzDI7fN23XYGbItVMvP5l0hqAYaygqgLiL0gF57tGw51AL7ij515+VBjItwr531DHqobzWTNBl0EL1ldlXTYCaK8xx9wBWBYblPAdP0v7G3MNJrskxTz2l',\n",
    "# ...                                    region_name = 'us-west-2')\n",
    "# >>> s3 = s3_session.resource('s3')\n",
    "# >>> s3_object = s3.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "# >>> text = s3_object['Body'].read()\n",
    "# >>> context = text.decode(encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=31'>32</a>\u001b[0m \u001b[39m# TODO:OK: three columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=32'>33</a>\u001b[0m \u001b[39m# imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=33'>34</a>\u001b[0m \u001b[39m# df_imm_city_res_label = spark.sparkContext.parallelize(imm_cit_res_three).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\", \"value_of_imm_cntyl_organizations\"]) \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=54'>55</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=55'>56</a>\u001b[0m \u001b[39m# TODO:OK: two columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=56'>57</a>\u001b[0m imm_addr_two, imm_addr_three \u001b[39m=\u001b[39m code_mapping(context, \u001b[39m\"\u001b[39m\u001b[39mi94addrl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=57'>58</a>\u001b[0m df_imm_address \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39mparallelize(imm_addr_two)\u001b[39m.\u001b[39mtoDF([\u001b[39m\"\u001b[39m\u001b[39mcode_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalue_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m]) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=58'>59</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mcode_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mcode_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mString\u001b[39m\u001b[39m\"\u001b[39m)) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=59'>60</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mvalue_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mvalue_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mString\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=60'>61</a>\u001b[0m df_imm_address\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=63'>64</a>\u001b[0m \u001b[39m# TODO:OK: two columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=64'>65</a>\u001b[0m \u001b[39m# imm_visa = {'1': 'Business',\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=65'>66</a>\u001b[0m \u001b[39m#             '2': 'Pleasure',\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=66'>67</a>\u001b[0m \u001b[39m#             '3': 'Student'}\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "s3_object = s3.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "text = s3_object['Body'].read()\n",
    "context = text.decode(encoding ='utf-8')\n",
    "# for obj in s3_object.objects.all():\n",
    "#     print(obj.key)\n",
    "\n",
    "context = context.replace('\\t', '')\n",
    "\n",
    "\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(\n",
    "        ';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\")\n",
    "                         for line in content_line_split]\n",
    "    content_two_dims = [i.strip().split('=') for i in content_line_list[1:]]\n",
    "    content_three_dims = [[i[0].strip(), i[1].strip().split(', ')[:][0], e]\n",
    "                          for i in content_two_dims if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_two_dims, content_three_dims\n",
    "\n",
    "# TODO:OK: three columns\n",
    "# imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")\n",
    "# df_imm_city_res_label = spark.sparkContext.parallelize(imm_cit_res_three).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\", \"value_of_imm_cntyl_organizations\"]) \\\n",
    "#     .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\")) \\\n",
    "#     .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\")) \\\n",
    "#     .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl_organizations\").cast(\"String\")) \\\n",
    "\n",
    "# df_imm_city_res_label.show()\n",
    "\n",
    "# TODO:OK: three columns\n",
    "# imm_port_two, imm_port_three = code_mapping(context, \"i94prtl\")\n",
    "# df_imm_destination_city = spark.sparkContext.parallelize(imm_port_three).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"]) \\\n",
    "#                                                 .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "#                                                 .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "#                                                 .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\"))\n",
    "# df_imm_destination_city.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "# imm_mode_two, imm_mode_three = code_mapping(context, \"i94model\")\n",
    "# df_imm_travel_code = spark.sparkContext.parallelize(imm_mode_two).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"]) \\\n",
    "#                                            .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Integer\")) \\\n",
    "#                                            .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))\n",
    "# df_imm_travel_code.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_addr_two, imm_addr_three = code_mapping(context, \"i94addrl\")\n",
    "df_imm_address = spark.sparkContext.parallelize(imm_addr_two).toDF([\"code_of_imm_address\", \"value_of_imm_address\"]) \\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\"))\n",
    "df_imm_address.show()\n",
    "\n",
    "\n",
    "# TODO:OK: two columns\n",
    "# imm_visa = {'1': 'Business',\n",
    "#             '2': 'Pleasure',\n",
    "#             '3': 'Student'}\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(imm_visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"]) \\\n",
    "                                    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\"))\n",
    "# df_imm_visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: immigration_label\n",
    "\n",
    "- df_imm_city_res_label\n",
    "- df_imm_destination_city\n",
    "- df_imm_travel_code\n",
    "- df_imm_address\n",
    "- df_imm_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_label_data_read = SAS7BDAT(label_data)\n",
    "\n",
    "# t = list(imm_label_data_read)\n",
    "# TODO: make sure that is a correct data\n",
    "\n",
    "# def read_data_to_df(call_func):\n",
    "#     # def wrapper(label_data):\n",
    "#     def wrapper(*args, **kwargs):\n",
    "#         # return call_func(*args)\n",
    "#         print(\"Reading data...\")\n",
    "#         with open(label_data) as f:\n",
    "#             context = f.read().replace('\\t', '')\n",
    "#         call_func(context, *args, **kwargs)\n",
    "#         print(\"Data reading finished.\")\n",
    "#     return wrapper\n",
    "\n",
    "\n",
    "with open(label_data) as f:\n",
    "    context = f.read().replace('\\t', '')\n",
    "    # content_mapping = context[context.index('i94prtl'):]\n",
    "    # content_line_split = content_mapping[:content_mapping.index(\n",
    "    #     ';')].split('\\n')\n",
    "    # content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    # content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    # # content_dict = [[i[0].strip(), i[1].strip(), [e for e in i[1].strip().split(', ')[1:]]] for i in content_dict if len(i) == 2]\n",
    "    # content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "\n",
    "\n",
    "# @read_data_to_df\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_dict\n",
    "\n",
    "i94cit_res = code_mapping(context, \"i94cntyl\")\n",
    "i94port = code_mapping(context, \"i94prtl\")\n",
    "i94mode = code_mapping(context, \"i94model\")\n",
    "i94addr = code_mapping(context, \"i94addrl\")\n",
    "i94visa = {'1': 'Business',\n",
    "           '2': 'Pleasure',\n",
    "           '3': 'Student'}\n",
    "\n",
    "# for i in i94port:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"code1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/17 16:51:10 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2672867 ms exceeds timeout 120000 ms\n",
      "22/06/17 16:51:11 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Writing into aws s3 and format parquet file.\n",
    "df_imm_city_res_label = spark.sparkContext.parallelize(i94cit_res).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\"])\\\n",
    "    .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\"))\n",
    "\n",
    "df_imm_city_res_label.repartition(10).write.partitionBy('value_of_imm_cntyl') \\\n",
    "    .parquet(mode=\"overwrite\", path='/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/dest_data/imm_city_res_label')\n",
    "\n",
    "# df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"])\\\n",
    "#     .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "#     .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "#     .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\"))\n",
    "\n",
    "# df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city_data\")\n",
    "# df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city_data\")\n",
    "\n",
    "# df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city\")\n",
    "# df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city WHERE code_of_imm_destination_city IS NOT NULL\")\n",
    "\n",
    "# df_imm_travel_code = spark.sparkContext.parallelize(i94mode.items()).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"])\\\n",
    "#     .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Integer\"))\\\n",
    "#     .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))\n",
    "\n",
    "# df_imm_address = spark.sparkContext.parallelize(i94addr.items()).toDF([\"code_of_imm_address\", \"value_of_imm_address\"])\\\n",
    "#     .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\"))\\\n",
    "#     .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\"))\n",
    "\n",
    "# df_imm_visa = spark.sparkContext.parallelize(i94visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"])\\\n",
    "#     .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\"))\\\n",
    "#     .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\"))\n",
    "\n",
    "\n",
    "\n",
    "# df_imm_city_res_label.createOrReplaceTempView(\"imm_city_res_label\")\n",
    "\n",
    "# df_imm_city_res_label = spark.sql(\"SELECT * FROM imm_city_res_label\")\n",
    "# df_imm_city_res.persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm_destination_city_tmp.show(n=5, truncate=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check data source\n",
    "for column in df_imm_city_res_label.columns:\n",
    "    n = df_imm_city_res_label.select(column).distinct().count()\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm_city_res.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: news_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path: data >> news_article\n",
    "\"\"\"Table: news_article schema\n",
    "pk: cord_uid -> news_cord_uid\n",
    "1. source_x -> news_source\n",
    "    schema: StringType()\n",
    "2. title -> news_title\n",
    "    schema: StringType()\n",
    "3. license -> news_licence\n",
    "    schema: StringType()\n",
    "4. abstract -> news_abstract\n",
    "    schema: StringType()\n",
    "5. publish_time -> news_publish_time (fk)\n",
    "    schema: TimestampType()\n",
    "6. authors -> news_authors\n",
    "    schema: StringType()\n",
    "7. url -> news_url\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "data_news = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "# news_schema = StructType([\n",
    "#     StructField(name=\"news_cord_uid\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_source\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_title\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_licence\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_abstract\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_publish_time\", dataType=DateType(), nullable=True),\n",
    "#     StructField(name=\"news_authors\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_url\", dataType=StringType(), nullable=True)\n",
    "# ])\n",
    "\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=data_news)\n",
    "\n",
    "df_news = df_news.withColumn(\"news_cord_uid\", col(\"cord_uid\").cast(\"String\")) \\\n",
    "                    .withColumn(\"news_source\", col(\"source_x\").cast(\"String\")) \\\n",
    "                        .withColumn(\"news_title\", col(\"title\").cast(\"String\")) \\\n",
    "                            .withColumn(\"news_licence\", col(\"license\").cast(\"String\")) \\\n",
    "                                .withColumn(\"news_abstract\", col(\"abstract\").cast(\"String\")) \\\n",
    "                                    .withColumn(\"news_publish_time\", to_date(df_news.publish_time, \"yyyy-MM-dd\")) \\\n",
    "                                        .withColumn(\"news_authors\", col(\"authors\").cast(\"String\")) \\\n",
    "                                            .withColumn(\"news_url\", col(\"url\").cast(\"String\")) \\\n",
    "                 .select(col(\"news_cord_uid\"),\n",
    "                         col(\"news_source\"),\n",
    "                         col(\"news_title\"),\n",
    "                         col(\"news_licence\"),\n",
    "                         col(\"news_abstract\"),\n",
    "                         col(\"news_publish_time\"),\n",
    "                         col(\"news_authors\"),\n",
    "                         col(\"news_url\")\n",
    "                         )\n",
    "\n",
    "df_news_tmp = df_news.createOrReplaceTempView(\"news_article_data\")\n",
    "\n",
    "df_news_tmp = spark.sql(\"SELECT DISTINCT publish_time FROM news_article_data\")\n",
    "\n",
    "df_news_tmp.persist()\n",
    "\n",
    "df_news_tmp.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_tmp.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Us Cities Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a us-cities data dimension table\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "pk: generated -> cidemo_id\n",
    "    schema: IntegerType()\n",
    "1. City -> cidemo_city\n",
    "    schema: StringType()\n",
    "2. State -> cidemo_state\n",
    "    schema: StringType()\n",
    "3. Median Age -> cidemo_median_age\n",
    "    schema: FloatType()\n",
    "4. Total Population -> cidemo_total_population\n",
    "    schema: IntegerType()\n",
    "5. State Code -> cidemo_state_code (fk)\n",
    "    schema: StringType()\n",
    "6. Count -> cidemo_count\n",
    "    schema: IntegerType()\n",
    "\"\"\"\n",
    "\n",
    "data_us_cities_demographics = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\"\n",
    "\n",
    "# TODO -> Must be defined a function that generated each table schema:\n",
    "us_cities_demographics_data_schema = StructType([\n",
    "    StructField(name=\"cidemo_city\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_median_age\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cidemo_total_population\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state_code\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_count\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using pyspark to read csv file\n",
    "df_us_cities_demographics = spark.read.options(header=True, delimiter=';').csv(data_us_cities_demographics)\n",
    "\"\"\"\n",
    "root\n",
    " |-- cidemo_city: string (nullable = true)\n",
    " |-- cidemo_state: string (nullable = true)\n",
    " |-- cidemo_median_age: float (nullable = true)\n",
    " |-- cidemo_total_population: integer (nullable = true)\n",
    " |-- cidemo_state_code: string (nullable = true)\n",
    " |-- cidemo_count: integer (nullable = true)\n",
    "\"\"\"\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_city\", col(\"City\").cast(\"String\")) \\\n",
    "                    .withColumn(\"cidemo_state\", col(\"State\").cast(\"String\")) \\\n",
    "                        .withColumn(\"cidemo_median_age\", col(\"Median Age\").cast(\"Float\")) \\\n",
    "                            .withColumn(\"cidemo_male_population\", col(\"Male Population\").cast(\"Integer\")) \\\n",
    "                                .withColumn(\"cidemo_female_population\", col(\"Female Population\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"cidemo_total_population\", col(\"Total Population\").cast(\"Integer\")) \\\n",
    "                                            .withColumn(\"cidemo_number_of_veterans\", col(\"Number of Veterans\").cast(\"Integer\")) \\\n",
    "                                                .withColumn(\"cidemo_foreign_born\", col(\"Foreign-born\").cast(\"Integer\")) \\\n",
    "                                                    .withColumn(\"cidemo_average_household_size\", col(\"Average Household Size\").cast(\"Float\")) \\\n",
    "                                                        .withColumn(\"cidemo_state_code\", col(\"State Code\").cast(\"String\")) \\\n",
    "                                                            .withColumn(\"cidemo_race\", col(\"Race\").cast(\"String\")) \\\n",
    "    .withColumn(\"cidemo_count\", col(\"Count\").cast(\"Integer\")) \\\n",
    "                    .select(col(\"cidemo_city\"),\n",
    "                            col(\"cidemo_state\"),\n",
    "                            col(\"cidemo_median_age\"),\n",
    "                            col(\"cidemo_total_population\"),\n",
    "                            col(\"cidemo_state_code\"),\n",
    "                            col(\"cidemo_count\"))\n",
    "\n",
    "# Auto-generated series of id\n",
    "df_us_cities_demographics = df_news.withColumn(\"cidemo_id\", monotonically_increasing_id())\n",
    "\n",
    "df_us_cities_demographics_temp = df_news.createOrReplaceTempView(\"us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp = spark.sql(\"SELECT * FROM us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp.persist()\n",
    "\n",
    "df_us_cities_demographics_temp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_cities_demographics_temp.show(n=5, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_cities_demographics_temp.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o124.load.\n: java.lang.NoClassDefFoundError: com/epam/parso/impl/SasFileReaderImpl\n\tat com.github.saurfang.sas.spark.SasRelation.inferSchema(SasRelation.scala:186)\n\tat com.github.saurfang.sas.spark.SasRelation.<init>(SasRelation.scala:73)\n\tat com.github.saurfang.sas.spark.SasRelation$.apply(SasRelation.scala:45)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:209)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:42)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:27)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39m# TODO: This code block is be placed in analysis jupyter notebook files\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000021?line=1'>2</a>\u001b[0m imm_data \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000021?line=4'>5</a>\u001b[0m df_imm_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mcom.github.saurfang.sas.spark\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mload(imm_data)\n",
      "File \u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=156'>157</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=158'>159</a>\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=159'>160</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1297'>1298</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1298'>1299</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1299'>1300</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1300'>1301</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1306'>1307</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o124.load.\n: java.lang.NoClassDefFoundError: com/epam/parso/impl/SasFileReaderImpl\n\tat com.github.saurfang.sas.spark.SasRelation.inferSchema(SasRelation.scala:186)\n\tat com.github.saurfang.sas.spark.SasRelation.<init>(SasRelation.scala:73)\n\tat com.github.saurfang.sas.spark.SasRelation$.apply(SasRelation.scala:45)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:209)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:42)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:27)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# TODO: This code block is be placed in analysis jupyter notebook files\n",
    "imm_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "\n",
    "\n",
    "df_imm_data = spark.read.format(\"com.github.saurfang.sas.spark\").load(imm_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_imm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "# show(n=5, truncate=5)\n",
    "df_immigration_personal = df_imm_data.withColumn(\"imm_per_cic_id\", col(\"cicid\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_person_birth_year\", col(\"biryear\").cast(\"Integer\"))\\\n",
    "           .withColumn(\"imm_person_gender\", col(\"gender\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_visatype\", col(\"visatype\").cast(\"String\")).select(col(\"imm_per_cic_id\"), \\\n",
    "                                                                              col(\"imm_person_birth_year\"), \\\n",
    "                                                                              col(\"imm_person_gender\"), \\\n",
    "                                                                              col(\"imm_visatype\"))\n",
    "\n",
    "df_immigration_personal_tmp = df_immigration_personal.createOrReplaceTempView(\"imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp = spark.sql(\"SELECT * FROM imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp.persist()\n",
    "\n",
    "df_immigration_personal_tmp.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr: 4 digit year of the arrival  -> imm_year\n",
    "2. i94mon: numeric month of the arrival -> imm_month\n",
    "3. i94citi&i94res: 3 digit code of origin city -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa: reason for immigration -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port: 3 character code of destination city --> Foreign key (used to map to USDemographics and City Temperature data) -> imm_port\n",
    "6. arrdate: arrival date of the departure -> imm_arrival_date:\n",
    "7. deptdate: departure date\n",
    "date_add\n",
    "7. i94mode: 1 digit travel code -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_datetime(days: DoubleType) -> datetime:\n",
    "    \"\"\"convert_to_datetime converts days to datetime format\n",
    "\n",
    "    Args:\n",
    "        days (DoubleType): from sas arrive or departure date\n",
    "\n",
    "    Returns:\n",
    "        datetime: added days to datetime format result.\n",
    "    \"\"\"\n",
    "    if days is not None:\n",
    "        date = datetime.strptime('1960-01-01', '%Y-%m-%d')\n",
    "\n",
    "        return date + timedelta(days=days)\n",
    "\n",
    "udf_convert_to_datetime = udf(lambda x: convert_to_datetime(x), DateType())\n",
    "\n",
    "immigration_main_information = df_imm_data.withColumn(\"imm_main_cic_id\", col(\"cicid\").cast(\"Integer\"))\\\n",
    "            .withColumn(\"imm_year\", col(\"i94yr\").cast(\"Integer\"))\\\n",
    "                .withColumn(\"imm_month\", col(\"i94mon\").cast(\"Integer\"))\\\n",
    "                    .withColumn(\"imm_cntyl\", col(\"i94cit\").cast(\"Integer\"))\\\n",
    "                        .withColumn(\"imm_visa\", col(\"i94visa\").cast(\"Integer\"))\\\n",
    "                            .withColumn(\"imm_port\", col(\"i94port\").cast(\"String\"))\\\n",
    "                                .withColumn(\"imm_arrival_date\", udf_convert_to_datetime(col(\"arrdate\")))\\\n",
    "                                    .withColumn(\"imm_departure_date\", udf_convert_to_datetime(col(\"depdate\")))\\\n",
    "                                        .withColumn(\"imm_model\", col(\"i94mode\").cast(\"Integer\"))\\\n",
    "                                            .withColumn(\"imm_address\", col(\"i94addr\").cast(\"String\"))\\\n",
    "                                                .withColumn(\"imm_airline\", col(\"airline\").cast(\"String\"))\\\n",
    "                                                    .withColumn(\"imm_flight_no\", col(\"fltno\").cast(\"String\"))\\\n",
    "        .select(col('imm_main_cic_id'), \\\n",
    "                    col('imm_year'),\\\n",
    "                        col('imm_month'),\\\n",
    "                            col('imm_cntyl'),\\\n",
    "                                col('imm_visa'),\\\n",
    "                                    col('imm_port'),\\\n",
    "                                        col('imm_arrival_date'),\\\n",
    "                                            col('imm_departure_date'),\\\n",
    "                                                col('imm_model'),\\\n",
    "                                                    col('imm_address'),\\\n",
    "                                                        col('imm_airline'),\\\n",
    "                                                            col('imm_flight_no'))\n",
    "\n",
    "df_immigration_main_information = immigration_main_information.createOrReplaceTempView(\n",
    "    \"immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information = spark.sql(\"SELECT * FROM immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information.persist()\n",
    "\n",
    "df_immigration_main_information.explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_data_read = SAS7BDAT(imm_data)\n",
    "\n",
    "# imm_data_read.header\n",
    "# for i in imm_data_read.columns:\n",
    "#     print(\"col_id \", i.col_id)\n",
    "#     print(\"  name\",  i.name.decode(encoding ='utf-8'))\n",
    "#     print(\"  label\", i.label.decode(encoding ='utf-8'))\n",
    "#     print(\"  format\", i.format)\n",
    "#     print(\"  type\", i.type)\n",
    "#     print(\"  length\", i.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with multiple data files\n",
    "# TODO: Make a def for doing this.\n",
    "# Method1: Using pandas to read file.\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat', iterator=True, chunksize=5000000)\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat')\n",
    "\n",
    "# imm_chunks_1 = list(pdf_immigration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.io.sas.sas7bdat import SAS7BDATReader\n",
    "\n",
    "# rdr = SAS7BDATReader(imm_data, convert_header_text=False)\n",
    "# df3 = rdr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Notification Table\n",
    "\"\"\"\n",
    "t2.imm_main_cic_id\n",
    "t2.imm_per_cic_id\n",
    "t2.news_cord_uid\n",
    "src.cidemo_id\n",
    "src.value_of_imm_destination_city\n",
    "t2.news_title\n",
    "t2.news_abstract\n",
    "t2.news_publish_time\n",
    "t2.news_authors\n",
    "\"\"\"\n",
    "\n",
    "#  ** t1: join imm two tables\n",
    "#  ** t2: join news table with t1\n",
    "#  ** t3: join us cities table with t2\n",
    "\n",
    "df_notification = spark.sql(\n",
    "    \"WITH t1 AS \\\n",
    "    (SELECT * \\\n",
    "       FROM immigration_main_information_data imid \\\n",
    "      LEFT JOIN imm_personal ip \\\n",
    "             ON imid.imm_main_cic_id = ip.imm_per_cic_id \\\n",
    "        WHERE imid.imm_year = 2016 \\\n",
    "     ), t2 AS \\\n",
    "        (SELECT * \\\n",
    "           FROM t1 \\\n",
    "         LEFT JOIN news_article_data nad \\\n",
    "                ON t1.imm_arrival_date = nad.news_publish_time \\\n",
    "     ) \\\n",
    "    SELECT t2.imm_main_cic_id \\\n",
    "           t2.imm_per_cic_id \\\n",
    "           t2.news_cord_uid \\\n",
    "           src.cidemo_id \\\n",
    "           src.value_of_imm_destination_city \\\n",
    "           t2.news_title \\\n",
    "           t2.news_abstract \\\n",
    "           t2.news_publish_time \\\n",
    "           t2.news_authors \\\n",
    "       FROM t2 \\\n",
    "     LEFT JOIN (SELECT * FROM us_cities_demographics_data ucdd INNER JOIN imm_destination_city_data idcd ON ucdd.cidemo_state_code = idcd.value_of_alias_imm_destination_city) src \\\n",
    "            ON t2.imm_port = src.code_of_imm_destination_city \\\n",
    "    \"\n",
    ")\n",
    "\n",
    "df_notification.show(n=5, truncate=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import configparser\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "\n",
    "\n",
    "# ******* Access AWS Server *******\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/.aws/credentials'))\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['default']['aws_access_key_id']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['default']['aws_secret_access_key']\n",
    "# **********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_CONN_ID = \"S3_default\"\n",
    "bucket_name = ''\n",
    "s3_prefix = '/data/usCitiesDemographics_data/usCitiesDemo.csv'\n",
    "DEST_BUCKET = 'mydatapool'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = S3Hook(AWS_S3_CONN_ID)\n",
    "# content = hook.list_keys(bucket_name=DEST_BUCKET)\n",
    "f = hook.get_key(key=s3_prefix, bucket_name=DEST_BUCKET)\n",
    "ff = f.get()['Body'].read().decode('utf-8')\n",
    "ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.get_key(key=s3_prefix, bucket_name=DEST_BUCKET)\n",
    "file_c = hook.get()['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = AwsBaseHook(aws_conn_id='aws_conn', client_type='s3').get_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __secret_key__() -> str:\n",
    "    \"\"\"__secret_key__ _summary_\n",
    "\n",
    "        Purpose:\n",
    "            Building all access AWS server connection key and credentials.\n",
    "\n",
    "        Returns:\n",
    "            _type_: access key, secret key\n",
    "        \"\"\"\n",
    "\n",
    "    hook = AwsBaseHook(aws_conn_id='aws_conn',\n",
    "                       client_type='s3').get_credentials()\n",
    "\n",
    "    return hook.access_key, hook.secret_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = __secret_key__()\n",
    "print(a, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "hook = S3Hook(aws_conn_id='aws_conn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/job_flow/\"\n",
    "files = dict([os.path.join(root, file_), file_.split(\".\")[0]] for root, dirs, files in os.walk(filepath) for file_ in files)\n",
    "for k, v in files.items():\n",
    "    print(files[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_STEP_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "file_1 = dict([os.path.join(root, file_), file_.split(\".\")[0]] for root, dirs, files in os.walk(SPARK_STEP_FILE_PATH) for file_ in files if file_.endswith('.json'))\n",
    "file_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pprint import pprint\n",
    "pprint(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "import json\n",
    "AWS_CONN_ID = 'aws_conn'\n",
    "hook = S3Hook(aws_conn_id=AWS_CONN_ID)\n",
    "file_content = hook.get_key(\n",
    "    key='job_flow/job_flow_overrides.json', bucket_name='mydatapool')\n",
    "file_content = file_content.get()['Body'].read().decode('utf-8')\n",
    "json.loads(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "job_json = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/job_flow/job_flow_overrides.json\"\n",
    "f = open(job_json)\n",
    "data  = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_step = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/aws_emr_steps.json\"\n",
    "f = open(job_json)\n",
    "data = json.load(f)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_EMR_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "dict_etl_file_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    ETL_EMR_FILE_PATH) for file_ in files if file_.endswith('.py'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_etl_file_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "filepath = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data\"\n",
    "# ****** local data absolute path which is uploaded to S3 ******\n",
    "filepath_all = [os.path.join(root, file) for root,\n",
    "                dirs, files in os.walk(filepath) for file in files]\n",
    "\n",
    "files = [file_ for root, dirs, files in os.walk(filepath) for file_ in files]\n",
    "\n",
    "# s3 key where is saved upload of destination of aws s3 location\n",
    "s3_key_filename = [re.search(r'/data/*.*', each_filepath)[0]\n",
    "                   for each_filepath in filepath_all]\n",
    "\n",
    "each_file = [re.search(r'(^.+\\.)', files[i])[0] + str(i)\n",
    "             for i in range(len(files))]\n",
    "\n",
    "files_path = list(zip(each_file, s3_key_filename, filepath_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_path[0][0])  # file name + \".\" + index\n",
    "print(files_path[0][1])  # s3 key\n",
    "print(files_path[0][2])  # local absolute path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for i in files_path:\n",
    "    # print(i[0].split(\".\")[0])\n",
    "    # test = list(i[2], i[0].split('.')[0])\n",
    "    test = {i[2]: i[0].split('.')[0]}\n",
    "    print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import XCom\n",
    "from airflow.utils.db import provide_session\n",
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "\n",
    "\n",
    "with DAG(dag_id=\"del_xcom\",\n",
    "         schedule_interval=None,\n",
    "         start_date=datetime.now(),) as dag:\n",
    "\n",
    "        @provide_session\n",
    "        def clean_xcom(session=None, **context):\n",
    "                dag = context[\"dag\"]\n",
    "                dag_id = dag._dag_id\n",
    "                session.query(XCom).filter(XCom.dag_id == dag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 53'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000053?line=0'>1</a>\u001b[0m clean_xcom()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py:70\u001b[0m, in \u001b[0;36mprovide_session.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py?line=67'>68</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py?line=68'>69</a>\u001b[0m     \u001b[39mwith\u001b[39;00m create_session() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py?line=69'>70</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, session\u001b[39m=\u001b[39;49msession, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 52'\u001b[0m in \u001b[0;36mclean_xcom\u001b[0;34m(session, **context)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=3'>4</a>\u001b[0m \u001b[39m@provide_session\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_xcom\u001b[39m(session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcontext):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=5'>6</a>\u001b[0m         dag \u001b[39m=\u001b[39m context[\u001b[39m\"\u001b[39;49m\u001b[39mdag\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=6'>7</a>\u001b[0m         dag_id \u001b[39m=\u001b[39m dag\u001b[39m.\u001b[39m_dag_id\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=7'>8</a>\u001b[0m         session\u001b[39m.\u001b[39mquery(XCom)\u001b[39m.\u001b[39mfilter(XCom\u001b[39m.\u001b[39mdag_id \u001b[39m==\u001b[39m dag_id)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dag'"
     ]
    }
   ],
   "source": [
    "clean_xcom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data\"\n",
    "# ****** local data absolute path which is uploaded to S3 ******\n",
    "filepath_all = [os.path.join(root, file) for root,\n",
    "                dirs, files in os.walk(filepath) for file in files]\n",
    "\n",
    "# ****** Get the task name, s3_ky and file name from the file_path ******\n",
    "files = [file_ for root, dirs, files in os.walk(filepath) for file_ in files]\n",
    "\n",
    "# s3 key where is saved upload of destination of aws s3 location\n",
    "s3_key_filename = [re.search(r'/data/*.*', each_filepath)[0]\n",
    "                   for each_filepath in filepath_all]\n",
    "\n",
    "each_file = [re.search(r'(^.+\\.)', files[i])[0] + str(i)\n",
    "             for i in range(len(files))]\n",
    "\n",
    "files_path = list(zip(each_file, s3_key_filename, filepath_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/usCitiesDemographics_data/usCitiesDemo.csv\n",
      "data/news_data/metadata.csv\n",
      "data/immigration_data/immigration_oct16_sub.sas7bdat\n",
      "data/immigration_data/immigration_jun16_sub.sas7bdat\n",
      "data/immigration_data/immigration_apr16_sub.sas7bdat\n",
      "data/immigration_data/immigration_aug16_sub.sas7bdat\n",
      "data/immigration_data/immigration_may16_sub.sas7bdat\n",
      "data/immigration_data/immigration_feb16_sub.sas7bdat\n",
      "data/immigration_data/immigration_jan16_sub.sas7bdat\n",
      "data/immigration_data/immigration_mar16_sub.sas7bdat\n",
      "data/immigration_data/immigration_jul16_sub.sas7bdat\n",
      "data/immigration_data/immigration_nov16_sub.sas7bdat\n",
      "data/immigration_data/immigration_sep16_sub.sas7bdat\n",
      "data/immigration_data/immigration_labels_descriptions.SAS\n",
      "data/immigration_data/immigration_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "for each_filepath in filepath_all:\n",
    "    print(re.search(r'data/*.*', each_filepath)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_path = os.path.join(\n",
    "    \"mydatapool\", \"data/immigration_data/immigration_apr16_sub.sas7bdat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mydatapool/data/immigration_data/immigration_apr16_sub.sas7bdat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['s-2HJN029357WOV', 's-SS1ELLFXRPXF', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_STEPS = [\n",
    "    {\n",
    "        \"ActionOnFailure\": \"CONTINUE\",\n",
    "        \"HadoopJarStep\": {\n",
    "            \"Args\": [\n",
    "                \"s3-dist-cp\",\n",
    "                \"--src=s3://{{ var.value.Data_Bucket }}/upload_data/jars/spark-sas7bdat-3.0.0-s_2.12.jar\",\n",
    "                \"--dest=/usr/lib/spark/jars\"\n",
    "            ],\n",
    "            \"Jar\": \"command-runner.jar\"\n",
    "        },\n",
    "        \"Name\": \"Upload sas jars file from local to aws s3\"\n",
    "    },\n",
    "    {\n",
    "        \"ActionOnFailure\": \"CONTINUE\",\n",
    "        \"HadoopJarStep\": {\n",
    "            \"Args\": [\n",
    "                \"spark-submit\",\n",
    "                \"--master\",\n",
    "                \"yarn\",\n",
    "                \"--deploy-mode\",\n",
    "                \"cluster\",\n",
    "                \"--conf\",\n",
    "                \"spark.yarn.submit.waitAppCompletion=true\",\n",
    "                \"--name\",\n",
    "                \"data_spark_on_emr\",\n",
    "                \"s3://{{ var.value.Data_Bucket }}/upload_data/script/data_spark_on_emr.py\"\n",
    "            ],\n",
    "            \"Jar\": \"command-runner.jar\"\n",
    "        },\n",
    "        \"Name\": \"For Dealing with data and analytics using Spark on AWS EMR\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "5 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 61'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000060?line=0'>1</a>\u001b[0m [\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mindex(\u001b[39mlen\u001b[39;49m([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m])\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mValueError\u001b[0m: 5 is not in list"
     ]
    }
   ],
   "source": [
    "[1,2,3,3,1,2].index(len([1,2,3,3,1,2])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2916916465.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [19]\u001b[0;36m\u001b[0m\n\u001b[0;31m    {% if loop.index is divisibleby 3 %}\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "{% if loop.index is divisibleby 3 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "# config.read_file(open('s3://mydatapool/config/dl.cfg'))\n",
    "config.read_file(\n",
    "    open('/Users/oneforall_nick/workspace/Udacity_capstone_project/dl.cfg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEST_S3_BUCKET = 's3://destetlbucket'\n",
    "imm_path = os.path.join(\n",
    "    DEST_S3_BUCKET, \"data/immigration_data/immigration_apr16_sub.sas7bdat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://destetlbucket/data/immigration_data/immigration_apr16_sub.sas7bdat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project'\n",
    "dict_boostrap_emr_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    BOOTSTRAP_FILE_PATH) for file_ in files if file_.endswith('.sh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/bootstrap_emr.sh': 'bootstrap_emr'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_boostrap_emr_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oneforall_nick/workspace/Udacity_capstone_project/bootstrap_emr.sh\n",
      "bootstrap_emr\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(BOOTSTRAP_FILE_PATH):\n",
    "    for file_ in files:\n",
    "        if file_.endswith('.sh'):\n",
    "            print(os.path.join(root, file_))\n",
    "            print(file_.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg'\n",
    "dict_config_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    CONFIG_FILE_PATH) for file_ in files if file_.endswith('.cfg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg': 'dl'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_config_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project'\n",
    "dict_boostrap_emr_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    BOOTSTRAP_FILE_PATH) for file_ in files if file_.endswith('.sh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap_emr\n"
     ]
    }
   ],
   "source": [
    "for i in dict_boostrap_emr_info.items():\n",
    "    print(i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg'\n",
    "dict_config_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    CONFIG_FILE_PATH) for file_ in files if file_.endswith('.cfg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg': 'dl'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_config_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_EMR_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "dict_etl_file_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    ETL_EMR_FILE_PATH) for file_ in files if file_.endswith('.py'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/data_spark_on_emr.py': 'data_spark_on_emr'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_etl_file_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********\n",
    "ETL_EMR_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "dict_etl_file_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    ETL_EMR_FILE_PATH) for file_ in files if file_.endswith('.py'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/data_spark_on_emr.py': 'data_spark_on_emr'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_etl_file_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_spark_on_emr\n"
     ]
    }
   ],
   "source": [
    "for i in dict_etl_file_info.items():\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_spark_on_emr\n"
     ]
    }
   ],
   "source": [
    "for file_path, filename in dict_etl_file_info.items():\n",
    "    print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from signal import signal, SIGPIPE, SIG_DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_IGN: 1>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal(SIGPIPE,SIG_DFL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "SAS_JARS_FILEPATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/jars/'\n",
    "dict_SAS_jars_info = dict([os.path.join(root, file_), file_.split(r\".jar\")[0]]for root, dirs, files in os.walk(\n",
    "    SAS_JARS_FILEPATH) for file_ in files if file_.endswith('.jar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/jars/spark-sas7bdat-2.0.0-s_2.11.jar': 'spark-sas7bdat-2.0.0-s_2.11'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_SAS_jars_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 're'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 90'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000089?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_ \u001b[39min\u001b[39;00m files:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000089?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m file_\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.jar\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000089?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(file_\u001b[39m.\u001b[39;49mre(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.]+$\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 're'"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(SAS_JARS_FILEPATH):\n",
    "    for file_ in files:\n",
    "        if file_.endswith('.jar'):\n",
    "            print(file_.re(r'[^\\.]+$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark-sas7bdat-2.0.0-s_2.11', '']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'spark-sas7bdat-2.0.0-s_2.11.jar'\n",
    "\n",
    "# re.compile(r'\\w.*\\.', re.IGNORECASE).search(string).group().split('jar')\n",
    "\n",
    "string.split('.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "# ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "# config.read_file(open('dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "DEST_BOOTSTRAP_BUCKET = '/destetlbucket/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    # service_name='s3',\n",
    "    # region_name='us-west-2',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import Variable\n",
    "Data_Bucket = Variable.get('Data_Bucket')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.select_object_content(\n",
    "    Bucket=DEST_BOOTSTRAP_BUCKET,\n",
    "    Key='dimension_table/df_immigration_personal/imm_person_birth_year=1902/part-00006-4d920f82-bc2b-4f4a-bf17-cba974e4ba7a.c000.snappy.parquet',\n",
    "    Expression='SELECT COUNT(*) FROM S3Object',\n",
    "    ExpressionType='SQL',\n",
    "    InputSerialization={\n",
    "        'Parquet': {}\n",
    "    },\n",
    "    OutputSerialization={\n",
    "        'JSON': {\n",
    "            'RecordDelimiter': ','\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in resp['Payload']:\n",
    "    if 'Records' in e:\n",
    "        print(e['Records']['Payload'].e('utf-8'))\n",
    "    elif 'Stats' in e:\n",
    "        print(e['Stats'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = Path(DEST_BOOTSTRAP_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  s3path import S3Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_path = S3Path(DEST_BOOTSTRAP_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/destetlbucket/dimension_table\n",
      "/destetlbucket/fact_table\n"
     ]
    }
   ],
   "source": [
    "for path in bucket_path.iterdir():\n",
    "    if path.is_dir():\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48f95af79faaae749105c8389524df19196568350894b01545e0b6f755f2644b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
