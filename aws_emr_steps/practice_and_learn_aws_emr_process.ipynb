{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, udf, to_date\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               DateType,\n",
    "                               FloatType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000001?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49m\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000001?line=1'>2</a>\u001b[0m     appName(\u001b[39m\"\u001b[39;49m\u001b[39mdata_spark_on_emr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49m\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000001?line=2'>3</a>\u001b[0m     getOrCreate()\n",
      "File \u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/session.py:224\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/session.py?line=221'>222</a>\u001b[0m     sc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sc\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/session.py?line=222'>223</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/session.py?line=223'>224</a>\u001b[0m     sparkConf \u001b[39m=\u001b[39m SparkConf()\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/session.py?line=224'>225</a>\u001b[0m     \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options\u001b[39m.\u001b[39mitems():\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/session.py?line=225'>226</a>\u001b[0m         sparkConf\u001b[39m.\u001b[39mset(key, value)\n",
      "File \u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py:120\u001b[0m, in \u001b[0;36mSparkConf.__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py?line=115'>116</a>\u001b[0m _jvm \u001b[39m=\u001b[39m _jvm \u001b[39mor\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_jvm\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py?line=117'>118</a>\u001b[0m \u001b[39mif\u001b[39;00m _jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py?line=118'>119</a>\u001b[0m     \u001b[39m# JVM is created, so create self._jconf directly through JVM\u001b[39;00m\n\u001b[0;32m--> <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py?line=119'>120</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jconf \u001b[39m=\u001b[39m _jvm\u001b[39m.\u001b[39;49mSparkConf(loadDefaults)\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py?line=120'>121</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py?line=121'>122</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/conf.py?line=122'>123</a>\u001b[0m     \u001b[39m# JVM is not created, so store data in self._conf first\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py:1568\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1561'>1562</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1562'>1563</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1563'>1564</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1564'>1565</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1566'>1567</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1567'>1568</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1568'>1569</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1570'>1571</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1571'>1572</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py:342\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=339'>340</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=340'>341</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=341'>342</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39;49m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'c'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 10:30:48 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@7bb3b456[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@4d8abbfb[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@72a95408]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:30:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:30:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@453466f6[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@635c576c[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@5ab5e55]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:30:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:31:08 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@3967e1aa[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@2a804d30[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@4c658d40]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:31:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:31:18 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5d001883[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@33ecc679[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@35000b1f]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:31:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    appName(\"data_spark_on_emr\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_spark_on_emr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe1619d10a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.sql.parquet.binaryAsString', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.driver.host', '10.0.0.101'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/spark-warehouse'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.app.id', 'local-1655026623796'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar'),\n",
       " ('spark.app.initial.jar.urls',\n",
       "  'spark://10.0.0.101:53558/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,spark://10.0.0.101:53558/jars/com.epam_parso-2.0.8.jar,spark://10.0.0.101:53558/jars/org.scala-lang_scala-reflect-2.11.8.jar,spark://10.0.0.101:53558/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,spark://10.0.0.101:53558/jars/org.slf4j_slf4j-api-1.7.5.jar'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'data_spark_on_emr'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.startTime', '1655026622590'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.driver.port', '53558'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ALL DATA AND SAVE TO simulation data file, and then analysis those data in spark.\n",
    "# ****** Read data from local file system(be a s3) to spark DataFrame ******\n",
    "# All data format will be saved as a parquet file.\n",
    "\n",
    "# file path: data >> immigration_data\n",
    "# There are three tables: immigration_table, immigration_personal_table and immigration_label_table\n",
    "# ***immigration_table, immigration_personal_table and immigration_label_table schema***\n",
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr  -> imm_year\n",
    "2. i94mon -> imm_month\n",
    "3. i94citi&i94res -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port -> imm_port\n",
    "6. arrdate -> imm_arrival_date:\n",
    "7. i94mode -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "# config.read_file(open('s3://mydatapool/config/dl.cfg'))\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "\n",
    "# s3://mydatapool/config/dl.cfg\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "\n",
    "# Access data from AWS S3\n",
    "SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "# SOURCE_S3_BUCKET = 's3://mydatapool'\n",
    "# Write data to AWS S3\n",
    "DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key\n",
    "AWS_SECRET_ACCESS_KEY\n",
    "aws_secret_access_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 10:36:38 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@4d99b03[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@76713ee6[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@431a86a4]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:36:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"libname library 'Your file location' ;\\r\\nproc format library=library ;\\r\\n\\r\\n/* I94YR - 4 digit year */\\r\\n\\r\\n/* I94MON - Numeric month */\\r\\n\\r\\n/* I94CIT & I94RES - This format shows all the valid and invalid codes for processing */\\r\\n  value i94cntyl\\r\\n   582 =  'MEXICO Air Sea, and Not Reported (I-94, no land arrivals)'\\r\\n   236 =  'AFGHANISTAN'\\r\\n   101 =  'ALBANIA'\\r\\n   316 =  'ALGERIA'\\r\\n   102 =  'ANDORRA'\\r\\n   324 =  'ANGOLA'\\r\\n   529 =  'ANGUILLA'\\r\\n   518 =  'ANTIGUA-BARBUDA'\\r\\n   687 =  'ARGENTINA '\\r\\n   151 =  'ARMENIA'\\r\\n   532 =  'ARUBA'\\r\\n   438 =  'AUSTRALIA'\\r\\n   103 =  'AUSTRIA'\\r\\n   152 =  'AZERBAIJAN'\\r\\n   512 =  'BAHAMAS'\\r\\n   298 =  'BAHRAIN'\\r\\n   274 =  'BANGLADESH'\\r\\n   513 =  'BARBADOS'\\r\\n   104 =  'BELGIUM'\\r\\n   581 =  'BELIZE'\\r\\n   386 =  'BENIN'\\r\\n   509 =  'BERMUDA'\\r\\n   153 =  'BELARUS'\\r\\n   242 =  'BHUTAN'\\r\\n   688 =  'BOLIVIA'\\r\\n   717 =  'BONAIRE, ST EUSTATIUS, SABA' \\r\\n   164 =  'BOSNIA-HERZEGOVINA'\\r\\n   336 =  'BOTSWANA'\\r\\n   689 =  'BRAZIL'\\r\\n   525 =  'BRITISH VIRGIN ISLANDS'\\r\\n   217 =  'BRUNEI'\\r\\n   105 =  'BULGARIA'\\r\\n   393 =  'BURKINA FASO'\\r\\n   243 =  'BURMA'\\r\\n   375 =  'BURUNDI'\\r\\n   310 =  'CAMEROON'\\r\\n   326 =  'CAPE VERDE'\\r\\n   526 =  'CAYMAN ISLANDS'\\r\\n   383 =  'CENTRAL AFRICAN REPUBLIC'\\r\\n   384 =  'CHAD'\\r\\n   690 =  'CHILE'\\r\\n   245 =  'CHINA, PRC'\\r\\n   721 =  'CURACAO' \\r\\n   270 =  'CHRISTMAS ISLAND'\\r\\n   271 =  'COCOS ISLANDS'\\r\\n   691 =  'COLOMBIA'\\r\\n   317 =  'COMOROS'\\r\\n   385 =  'CONGO'\\r\\n   467 =  'COOK ISLANDS'\\r\\n   575 =  'COSTA RICA'\\r\\n   165 =  'CROATIA'\\r\\n   584 =  'CUBA'\\r\\n   218 =  'CYPRUS'\\r\\n   140 =  'CZECH REPUBLIC'\\r\\n   723 =  'FAROE ISLANDS (PART OF DENMARK)'  \\r\\n   108 =  'DENMARK'\\r\\n   322 =  'DJIBOUTI'\\r\\n   519 =  'DOMINICA'\\r\\n   585 =  'DOMINICAN REPUBLIC'\\r\\n   240 =  'EAST TIMOR'\\r\\n   692 =  'ECUADOR'\\r\\n   368 =  'EGYPT'\\r\\n   576 =  'EL SALVADOR'\\r\\n   399 =  'EQUATORIAL GUINEA'\\r\\n   372 =  'ERITREA'\\r\\n   109 =  'ESTONIA'\\r\\n   369 =  'ETHIOPIA'\\r\\n   604 =  'FALKLAND ISLANDS'\\r\\n   413 =  'FIJI'\\r\\n   110 =  'FINLAND'\\r\\n   111 =  'FRANCE'\\r\\n   601 =  'FRENCH GUIANA'\\r\\n   411 =  'FRENCH POLYNESIA'\\r\\n   387 =  'GABON'\\r\\n   338 =  'GAMBIA'\\r\\n   758 =  'GAZA STRIP' \\r\\n   154 =  'GEORGIA'\\r\\n   112 =  'GERMANY'\\r\\n   339 =  'GHANA'\\r\\n   143 =  'GIBRALTAR'\\r\\n   113 =  'GREECE'\\r\\n   520 =  'GRENADA'\\r\\n   507 =  'GUADELOUPE'\\r\\n   577 =  'GUATEMALA'\\r\\n   382 =  'GUINEA'\\r\\n   327 =  'GUINEA-BISSAU'\\r\\n   603 =  'GUYANA'\\r\\n   586 =  'HAITI'\\r\\n   726 =  'HEARD AND MCDONALD IS.'\\r\\n   149 =  'HOLY SEE/VATICAN'\\r\\n   528 =  'HONDURAS'\\r\\n   206 =  'HONG KONG'\\r\\n   114 =  'HUNGARY'\\r\\n   115 =  'ICELAND'\\r\\n   213 =  'INDIA'\\r\\n   759 =  'INDIAN OCEAN AREAS (FRENCH)' \\r\\n   729 =  'INDIAN OCEAN TERRITORY' \\r\\n   204 =  'INDONESIA'\\r\\n   249 =  'IRAN'\\r\\n   250 =  'IRAQ'\\r\\n   116 =  'IRELAND'\\r\\n   251 =  'ISRAEL'\\r\\n   117 =  'ITALY'\\r\\n   388 =  'IVORY COAST'\\r\\n   514 =  'JAMAICA'\\r\\n   209 =  'JAPAN'\\r\\n   253 =  'JORDAN'\\r\\n   201 =  'KAMPUCHEA'\\r\\n   155 =  'KAZAKHSTAN'\\r\\n   340 =  'KENYA'\\r\\n   414 =  'KIRIBATI'\\r\\n   732 =  'KOSOVO' \\r\\n   272 =  'KUWAIT'\\r\\n   156 =  'KYRGYZSTAN'\\r\\n   203 =  'LAOS'\\r\\n   118 =  'LATVIA'\\r\\n   255 =  'LEBANON'\\r\\n   335 =  'LESOTHO'\\r\\n   370 =  'LIBERIA'\\r\\n   381 =  'LIBYA'\\r\\n   119 =  'LIECHTENSTEIN'\\r\\n   120 =  'LITHUANIA'\\r\\n   121 =  'LUXEMBOURG'\\r\\n   214 =  'MACAU'\\r\\n   167 =  'MACEDONIA'\\r\\n   320 =  'MADAGASCAR'\\r\\n   345 =  'MALAWI'\\r\\n   273 =  'MALAYSIA'\\r\\n   220 =  'MALDIVES'\\r\\n   392 =  'MALI'\\r\\n   145 =  'MALTA'\\r\\n   472 =  'MARSHALL ISLANDS'\\r\\n   511 =  'MARTINIQUE'\\r\\n   389 =  'MAURITANIA'\\r\\n   342 =  'MAURITIUS'\\r\\n   760 =  'MAYOTTE (AFRICA - FRENCH)' \\r\\n   473 =  'MICRONESIA, FED. STATES OF'\\r\\n   157 =  'MOLDOVA'\\r\\n   122 =  'MONACO'\\r\\n   299 =  'MONGOLIA'\\r\\n   735 =  'MONTENEGRO' \\r\\n   521 =  'MONTSERRAT'\\r\\n   332 =  'MOROCCO'\\r\\n   329 =  'MOZAMBIQUE'\\r\\n   371 =  'NAMIBIA'\\r\\n   440 =  'NAURU'\\r\\n   257 =  'NEPAL'\\r\\n   123 =  'NETHERLANDS'\\r\\n   508 =  'NETHERLANDS ANTILLES'\\r\\n   409 =  'NEW CALEDONIA'\\r\\n   464 =  'NEW ZEALAND'\\r\\n   579 =  'NICARAGUA'\\r\\n   390 =  'NIGER'\\r\\n   343 =  'NIGERIA'\\r\\n   470 =  'NIUE'\\r\\n   275 =  'NORTH KOREA'\\r\\n   124 =  'NORWAY'\\r\\n   256 =  'OMAN'\\r\\n   258 =  'PAKISTAN'\\r\\n   474 =  'PALAU'\\r\\n   743 =  'PALESTINE' \\r\\n   504 =  'PANAMA'\\r\\n   441 =  'PAPUA NEW GUINEA'\\r\\n   693 =  'PARAGUAY'\\r\\n   694 =  'PERU'\\r\\n   260 =  'PHILIPPINES'\\r\\n   416 =  'PITCAIRN ISLANDS'\\r\\n   107 =  'POLAND'\\r\\n   126 =  'PORTUGAL'\\r\\n   297 =  'QATAR'\\r\\n   748 =  'REPUBLIC OF SOUTH SUDAN'\\r\\n   321 =  'REUNION'\\r\\n   127 =  'ROMANIA'\\r\\n   158 =  'RUSSIA'\\r\\n   376 =  'RWANDA'\\r\\n   128 =  'SAN MARINO'\\r\\n   330 =  'SAO TOME AND PRINCIPE'\\r\\n   261 =  'SAUDI ARABIA'\\r\\n   391 =  'SENEGAL'\\r\\n   142 =  'SERBIA AND MONTENEGRO'\\r\\n   745 =  'SERBIA' \\r\\n   347 =  'SEYCHELLES'\\r\\n   348 =  'SIERRA LEONE'\\r\\n   207 =  'SINGAPORE'\\r\\n   141 =  'SLOVAKIA'\\r\\n   166 =  'SLOVENIA'\\r\\n   412 =  'SOLOMON ISLANDS'\\r\\n   397 =  'SOMALIA'\\r\\n   373 =  'SOUTH AFRICA'\\r\\n   276 =  'SOUTH KOREA'\\r\\n   129 =  'SPAIN'\\r\\n   244 =  'SRI LANKA'\\r\\n   346 =  'ST. HELENA'\\r\\n   522 =  'ST. KITTS-NEVIS'\\r\\n   523 =  'ST. LUCIA'\\r\\n   502 =  'ST. PIERRE AND MIQUELON'\\r\\n   524 =  'ST. VINCENT-GRENADINES'\\r\\n   716 =  'SAINT BARTHELEMY' \\r\\n   736 =  'SAINT MARTIN' \\r\\n   749 =  'SAINT MAARTEN' \\r\\n   350 =  'SUDAN'\\r\\n   602 =  'SURINAME'\\r\\n   351 =  'SWAZILAND'\\r\\n   130 =  'SWEDEN'\\r\\n   131 =  'SWITZERLAND'\\r\\n   262 =  'SYRIA'\\r\\n   268 =  'TAIWAN'\\r\\n   159 =  'TAJIKISTAN'\\r\\n   353 =  'TANZANIA'\\r\\n   263 =  'THAILAND'\\r\\n   304 =  'TOGO'\\r\\n   417 =  'TONGA'\\r\\n   516 =  'TRINIDAD AND TOBAGO'\\r\\n   323 =  'TUNISIA'\\r\\n   264 =  'TURKEY'\\r\\n   161 =  'TURKMENISTAN'\\r\\n   527 =  'TURKS AND CAICOS ISLANDS'\\r\\n   420 =  'TUVALU'\\r\\n   352 =  'UGANDA'\\r\\n   162 =  'UKRAINE'\\r\\n   296 =  'UNITED ARAB EMIRATES'\\r\\n   135 =  'UNITED KINGDOM'\\r\\n   695 =  'URUGUAY'\\r\\n   163 =  'UZBEKISTAN'\\r\\n   410 =  'VANUATU'\\r\\n   696 =  'VENEZUELA'\\r\\n   266 =  'VIETNAM'\\r\\n   469 =  'WALLIS AND FUTUNA ISLANDS'\\r\\n   757 =  'WEST INDIES (FRENCH)' \\r\\n   333 =  'WESTERN SAHARA'\\r\\n   465 =  'WESTERN SAMOA'\\r\\n   216 =  'YEMEN'\\r\\n   139 =  'YUGOSLAVIA'\\r\\n   301 =  'ZAIRE'\\r\\n   344 =  'ZAMBIA'\\r\\n   315 =  'ZIMBABWE'\\r\\n   403 =  'INVALID: AMERICAN SAMOA'\\r\\n   712 =  'INVALID: ANTARCTICA' \\r\\n   700 =  'INVALID: BORN ON BOARD SHIP'\\r\\n   719 =  'INVALID: BOUVET ISLAND (ANTARCTICA/NORWAY TERR.)'\\r\\n   574 =  'INVALID: CANADA'\\r\\n   720 =  'INVALID: CANTON AND ENDERBURY ISLS' \\r\\n   106 =  'INVALID: CZECHOSLOVAKIA'\\r\\n   739 =  'INVALID: DRONNING MAUD LAND (ANTARCTICA-NORWAY)' \\r\\n   394 =  'INVALID: FRENCH SOUTHERN AND ANTARCTIC'\\r\\n   501 =  'INVALID: GREENLAND'\\r\\n   404 =  'INVALID: GUAM'\\r\\n   730 =  'INVALID: INTERNATIONAL WATERS' \\r\\n   731 =  'INVALID: JOHNSON ISLAND' \\r\\n   471 =  'INVALID: MARIANA ISLANDS, NORTHERN'\\r\\n   737 =  'INVALID: MIDWAY ISLANDS' \\r\\n   753 =  'INVALID: MINOR OUTLYING ISLANDS - USA'\\r\\n   740 =  'INVALID: NEUTRAL ZONE (S. ARABIA/IRAQ)' \\r\\n   710 =  'INVALID: NON-QUOTA IMMIGRANT'\\r\\n   505 =  'INVALID: PUERTO RICO'\\r\\n    0  =  'INVALID: STATELESS'\\r\\n   705 =  'INVALID: STATELESS'\\r\\n   583 =  'INVALID: UNITED STATES'\\r\\n   407 =  'INVALID: UNITED STATES'\\r\\n   999 =  'INVALID: UNKNOWN'\\r\\n   239 =  'INVALID: UNKNOWN COUNTRY'\\r\\n   134 =  'INVALID: USSR'\\r\\n   506 =  'INVALID: U.S. VIRGIN ISLANDS'\\r\\n   755 =  'INVALID: WAKE ISLAND'  \\r\\n   311 =  'Collapsed Tanzania (should not show)'\\r\\n   741 =  'Collapsed Curacao (should not show)'\\r\\n    54 =  'No Country Code (54)'\\r\\n   100 =  'No Country Code (100)'\\r\\n   187 =  'No Country Code (187)'\\r\\n   190 =  'No Country Code (190)'\\r\\n   200 =  'No Country Code (200)'\\r\\n   219 =  'No Country Code (219)'\\r\\n   238 =  'No Country Code (238)'\\r\\n   277 =  'No Country Code (277)'\\r\\n   293 =  'No Country Code (293)'\\r\\n   300 =  'No Country Code (300)'\\r\\n   319 =  'No Country Code (319)'\\r\\n   365 =  'No Country Code (365)'\\r\\n   395 =  'No Country Code (395)'\\r\\n   400 =  'No Country Code (400)'\\r\\n   485 =  'No Country Code (485)'\\r\\n   503 =  'No Country Code (503)'\\r\\n   589 =  'No Country Code (589)'\\r\\n   592 =  'No Country Code (592)'\\r\\n   791 =  'No Country Code (791)'\\r\\n   849 =  'No Country Code (849)'\\r\\n   914 =  'No Country Code (914)'\\r\\n   944 =  'No Country Code (944)'\\r\\n   996 =  'No Country Code (996)' ;\\r\\n\\r\\n\\r\\n/* I94PORT - This format shows all the valid and invalid codes for processing */\\r\\n  value $i94prtl\\r\\n   'ALC'='ALCAN, AK             '\\r\\n   'ANC'='ANCHORAGE, AK         '\\r\\n   'BAR'='BAKER AAF - BAKER ISLAND, AK'\\r\\n   'DAC'='DALTONS CACHE, AK     '\\r\\n   'PIZ'='DEW STATION PT LAY DEW, AK'\\r\\n   'DTH'='DUTCH HARBOR, AK      '\\r\\n   'EGL'='EAGLE, AK             '\\r\\n   'FRB'='FAIRBANKS, AK         '\\r\\n   'HOM'='HOMER, AK             '           \\r\\n   'HYD'='HYDER, AK             '\\r\\n   'JUN'='JUNEAU, AK            '\\r\\n   '5KE'='KETCHIKAN, AK'\\r\\n   'KET'='KETCHIKAN, AK         '\\r\\n   'MOS'='MOSES POINT INTERMEDIATE, AK'\\r\\n   'NIK'='NIKISKI, AK           '\\r\\n   'NOM'='NOM, AK               '\\r\\n   'PKC'='POKER CREEK, AK       '\\r\\n   'ORI'='PORT LIONS SPB, AK'\\r\\n   'SKA'='SKAGWAY, AK           '\\r\\n   'SNP'='ST. PAUL ISLAND, AK'\\r\\n   'TKI'='TOKEEN, AK'\\r\\n   'WRA'='WRANGELL, AK          '\\r\\n   'HSV'='MADISON COUNTY - HUNTSVILLE, AL'\\r\\n   'MOB'='MOBILE, AL            '\\r\\n   'LIA'='LITTLE ROCK, AR (BPS)'\\r\\n   'ROG'='ROGERS ARPT, AR'\\r\\n   'DOU'='DOUGLAS, AZ           '\\r\\n   'LUK'='LUKEVILLE, AZ         '\\r\\n   'MAP'='MARIPOSA AZ           '\\r\\n   'NAC'='NACO, AZ              '\\r\\n   'NOG'='NOGALES, AZ           '\\r\\n   'PHO'='PHOENIX, AZ           '\\r\\n   'POR'='PORTAL, AZ'\\r\\n   'SLU'='SAN LUIS, AZ          '\\r\\n   'SAS'='SASABE, AZ            '\\r\\n   'TUC'='TUCSON, AZ            '\\r\\n   'YUI'='YUMA, AZ              ' \\r\\n   'AND'='ANDRADE, CA           '\\r\\n   'BUR'='BURBANK, CA'\\r\\n   'CAL'='CALEXICO, CA          '\\r\\n   'CAO'='CAMPO, CA             ' \\r\\n   'FRE'='FRESNO, CA            '\\r\\n   'ICP'='IMPERIAL COUNTY, CA   '\\r\\n   'LNB'='LONG BEACH, CA         '\\r\\n   'LOS'='LOS ANGELES, CA       '\\r\\n   'BFL'='MEADOWS FIELD - BAKERSFIELD, CA'\\r\\n   'OAK'='OAKLAND, CA ' \\r\\n   'ONT'='ONTARIO, CA'\\r\\n   'OTM'='OTAY MESA, CA          '\\r\\n   'BLT'='PACIFIC, HWY. STATION, CA '\\r\\n   'PSP'='PALM SPRINGS, CA'\\r\\n   'SAC'='SACRAMENTO, CA        '\\r\\n   'SLS'='SALINAS, CA (BPS)'\\r\\n   'SDP'='SAN DIEGO, CA'\\r\\n   'SFR'='SAN FRANCISCO, CA     '\\r\\n   'SNJ'='SAN JOSE, CA          '\\r\\n   'SLO'='SAN LUIS OBISPO, CA   '\\r\\n   'SLI'='SAN LUIS OBISPO, CA (BPS)'\\r\\n   'SPC'='SAN PEDRO, CA         '\\r\\n   'SYS'='SAN YSIDRO, CA        '\\r\\n   'SAA'='SANTA ANA, CA         '\\r\\n   'STO'='STOCKTON, CA (BPS)'\\r\\n   'TEC'='TECATE, CA            '\\r\\n   'TRV'='TRAVIS-AFB, CA        '\\r\\n   'APA'='ARAPAHOE COUNTY, CO'\\r\\n   'ASE'='ASPEN, CO #ARPT'\\r\\n   'COS'='COLORADO SPRINGS, CO'\\r\\n   'DEN'='DENVER, CO            '\\r\\n   'DRO'='LA PLATA - DURANGO, CO'\\r\\n   'BDL'='BRADLEY INTERNATIONAL, CT'\\r\\n   'BGC'='BRIDGEPORT, CT        '\\r\\n   'GRT'='GROTON, CT            '\\r\\n   'HAR'='HARTFORD, CT          '\\r\\n   'NWH'='NEW HAVEN, CT         '\\r\\n   'NWL'='NEW LONDON, CT        '\\r\\n   'TST'='NEWINGTON DATA CENTER TEST, CT'\\r\\n   'WAS'='WASHINGTON DC         '\\r\\n   'DOV'='DOVER AFB, DE'\\r\\n   'DVD'='DOVER-AFB, DE         '\\r\\n   'WLL'='WILMINGTON, DE        '\\r\\n   'BOC'='BOCAGRANDE, FL        '\\r\\n   'SRQ'='BRADENTON - SARASOTA, FL'\\r\\n   'CAN'='CAPE CANAVERAL, FL    '\\r\\n   'DAB'='DAYTONA BEACH INTERNATIONAL, FL'\\r\\n   'FRN'='FERNANDINA, FL        '\\r\\n   'FTL'='FORT LAUDERDALE, FL   '\\r\\n   'FMY'='FORT MYERS, FL        '\\r\\n   'FPF'='FORT PIERCE, FL       '\\r\\n   'HUR'='HURLBURT FIELD, FL'\\r\\n   'GNV'='J R ALISON MUNI - GAINESVILLE, FL'\\r\\n   'JAC'='JACKSONVILLE, FL      '\\r\\n   'KEY'='KEY WEST, FL          '\\r\\n   'LEE'='LEESBURG MUNICIPAL AIRPORT, FL'\\r\\n   'MLB'='MELBOURNE, FL'\\r\\n   'MIA'='MIAMI, FL             '\\r\\n   'APF'='NAPLES, FL #ARPT'\\r\\n   'OPF'='OPA LOCKA, FL'\\r\\n   'ORL'='ORLANDO, FL           '\\r\\n   'PAN'='PANAMA CITY, FL       '\\r\\n   'PEN'='PENSACOLA, FL         '\\r\\n   'PCF'='PORT CANAVERAL, FL    '\\r\\n   'PEV'='PORT EVERGLADES, FL   '\\r\\n   'PSJ'='PORT ST JOE, FL       '\\r\\n   'SFB'='SANFORD, FL           '\\r\\n   'SGJ'='ST AUGUSTINE ARPT, FL'\\r\\n   'SAU'='ST AUGUSTINE, FL      '\\r\\n   'FPR'='ST LUCIE COUNTY, FL'\\r\\n   'SPE'='ST PETERSBURG, FL     '\\r\\n   'TAM'='TAMPA, FL             '\\r\\n   'WPB'='WEST PALM BEACH, FL   '\\r\\n   'ATL'='ATLANTA, GA           '\\r\\n   'BRU'='BRUNSWICK, GA         '\\r\\n   'AGS'='BUSH FIELD - AUGUSTA, GA'\\r\\n   'SAV'='SAVANNAH, GA          '\\r\\n   'AGA'='AGANA, GU             '\\r\\n   'HHW'='HONOLULU, HI          '\\r\\n   'OGG'='KAHULUI - MAUI, HI'\\r\\n   'KOA'='KEAHOLE-KONA, HI      '\\r\\n   'LIH'='LIHUE, HI             '\\r\\n   'CID'='CEDAR RAPIDS/IOWA CITY, IA'\\r\\n   'DSM'='DES MOINES, IA'\\r\\n   'BOI'='AIR TERM. (GOWEN FLD) BOISE, ID'\\r\\n   'EPI'='EASTPORT, ID          '\\r\\n   'IDA'='FANNING FIELD - IDAHO FALLS, ID'\\r\\n   'PTL'='PORTHILL, ID          '\\r\\n   'SPI'='CAPITAL - SPRINGFIELD, IL'\\r\\n   'CHI'='CHICAGO, IL           '\\r\\n   'DPA'='DUPAGE COUNTY, IL'\\r\\n   'PIA'='GREATER PEORIA, IL'\\r\\n   'RFD'='GREATER ROCKFORD, IL'\\r\\n   'UGN'='MEMORIAL - WAUKEGAN, IL'\\r\\n   'GAR'='GARY, IN              '\\r\\n   'HMM'='HAMMOND, IN           '\\r\\n   'INP'='INDIANAPOLIS, IN      '\\r\\n   'MRL'='MERRILLVILLE, IN      '\\r\\n   'SBN'='SOUTH BEND, IN'\\r\\n   'ICT'='MID-CONTINENT - WITCHITA, KS'\\r\\n   'LEX'='BLUE GRASS - LEXINGTON, KY'\\r\\n   'LOU'='LOUISVILLE, KY        '\\r\\n   'BTN'='BATON ROUGE, LA       '\\r\\n   'LKC'='LAKE CHARLES, LA      '\\r\\n   'LAK'='LAKE CHARLES, LA (BPS)'\\r\\n   'MLU'='MONROE, LA'\\r\\n   'MGC'='MORGAN CITY, LA       '\\r\\n   'NOL'='NEW ORLEANS, LA       '\\r\\n   'BOS'='BOSTON, MA            '\\r\\n   'GLO'='GLOUCESTER, MA        '\\r\\n   'BED'='HANSCOM FIELD - BEDFORD, MA'\\r\\n   'LYN'='LYNDEN, WA            '\\r\\n   'ADW'='ANDREWS AFB, MD'\\r\\n   'BAL'='BALTIMORE, MD         '\\r\\n   'MKG'='MUSKEGON, MD'\\r\\n   'PAX'='PATUXENT RIVER, MD    '\\r\\n   'BGM'='BANGOR, ME            '\\r\\n   'BOO'='BOOTHBAY HARBOR, ME   '\\r\\n   'BWM'='BRIDGEWATER, ME       '\\r\\n   'BCK'='BUCKPORT, ME          '\\r\\n   'CLS'='CALAIS, ME   '\\r\\n   'CRB'='CARIBOU, ME           '\\r\\n   'COB'='COBURN GORE, ME       '\\r\\n   'EST'='EASTCOURT, ME         '\\r\\n   'EPT'='EASTPORT MUNICIPAL, ME'\\r\\n   'EPM'='EASTPORT, ME          '\\r\\n   'FOR'='FOREST CITY, ME       '\\r\\n   'FTF'='FORT FAIRFIELD, ME    '\\r\\n   'FTK'='FORT KENT, ME         '\\r\\n   'HML'='HAMIIN, ME            '\\r\\n   'HTM'='HOULTON, ME           '\\r\\n   'JKM'='JACKMAN, ME           '\\r\\n   'KAL'='KALISPEL, MT          '\\r\\n   'LIM'='LIMESTONE, ME         '\\r\\n   'LUB'='LUBEC, ME             '\\r\\n   'MAD'='MADAWASKA, ME         '\\r\\n   'POM'='PORTLAND, ME          '\\r\\n   'RGM'='RANGELEY, ME (BPS)'\\r\\n   'SBR'='SOUTH BREWER, ME      '\\r\\n   'SRL'='ST AURELIE, ME        '\\r\\n   'SPA'='ST PAMPILE, ME        '\\r\\n   'VNB'='VAN BUREN, ME         '\\r\\n   'VCB'='VANCEBORO, ME         '\\r\\n   'AGN'='ALGONAC, MI           '\\r\\n   'ALP'='ALPENA, MI            '\\r\\n   'BCY'='BAY CITY, MI          '\\r\\n   'DET'='DETROIT, MI           '\\r\\n   'GRP'='GRAND RAPIDS, MI'\\r\\n   'GRO'='GROSSE ISLE, MI       '\\r\\n   'ISL'='ISLE ROYALE, MI       '\\r\\n   'MRC'='MARINE CITY, MI       '\\r\\n   'MRY'='MARYSVILLE, MI        '\\r\\n   'PTK'='OAKLAND COUNTY - PONTIAC, MI'\\r\\n   'PHU'='PORT HURON, MI        '\\r\\n   'RBT'='ROBERTS LANDING, MI   '\\r\\n   'SAG'='SAGINAW, MI           '\\r\\n   'SSM'='SAULT STE. MARIE, MI  '\\r\\n   'SCL'='ST CLAIR, MI          '\\r\\n   'YIP'='WILLOW RUN - YPSILANTI, MI'\\r\\n   'BAU'='BAUDETTE, MN          '\\r\\n   'CAR'='CARIBOU MUNICIPAL AIRPORT, MN'\\r\\n   'GTF'='Collapsed into INT, MN'\\r\\n   'INL'='Collapsed into INT, MN'\\r\\n   'CRA'='CRANE LAKE, MN        '\\r\\n   'MIC'='CRYSTAL MUNICIPAL AIRPORT, MN'\\r\\n   'DUL'='DULUTH, MN            '\\r\\n   'ELY'='ELY, MN               '\\r\\n   'GPM'='GRAND PORTAGE, MN     '\\r\\n   'SVC'='GRANT COUNTY - SILVER CITY, MN'\\r\\n   'INT'='INT''L FALLS, MN      '\\r\\n   'LAN'='LANCASTER, MN         '\\r\\n   'MSP'='MINN./ST PAUL, MN     '\\r\\n   'LIN'='NORTHERN SVC CENTER, MN   '\\r\\n   'NOY'='NOYES, MN             '\\r\\n   'PIN'='PINE CREEK, MN        '\\r\\n   '48Y'='PINECREEK BORDER ARPT, MN'\\r\\n   'RAN'='RAINER, MN            '\\r\\n   'RST'='ROCHESTER, MN'\\r\\n   'ROS'='ROSEAU, MN            '\\r\\n   'SPM'='ST PAUL, MN           '\\r\\n   'WSB'='WARROAD INTL, SPB, MN'\\r\\n   'WAR'='WARROAD, MN           '\\r\\n   'KAN'='KANSAS CITY, MO       '\\r\\n   'SGF'='SPRINGFIELD-BRANSON, MO'\\r\\n   'STL'='ST LOUIS, MO          '\\r\\n   'WHI'='WHITETAIL, MT         '\\r\\n   'WHM'='WILD HORSE, MT        '\\r\\n   'GPT'='BILOXI REGIONAL, MS'\\r\\n   'GTR'='GOLDEN TRIANGLE LOWNDES CNTY, MS'\\r\\n   'GUL'='GULFPORT, MS          '\\r\\n   'PAS'='PASCAGOULA, MS        '\\r\\n   'JAN'='THOMPSON FIELD - JACKSON, MS'\\r\\n   'BIL'='BILLINGS, MT          '\\r\\n   'BTM'='BUTTE, MT             '\\r\\n   'CHF'='CHIEF MT, MT          '\\r\\n   'CTB'='CUT BANK MUNICIPAL, MT'\\r\\n   'CUT'='CUT BANK, MT          '\\r\\n   'DLB'='DEL BONITA, MT        '\\r\\n   'EUR'='EUREKA, MT (BPS)'\\r\\n   'BZN'='GALLATIN FIELD - BOZEMAN, MT'\\r\\n   'FCA'='GLACIER NATIONAL PARK, MT'\\r\\n   'GGW'='GLASGOW, MT           '\\r\\n   'GRE'='GREAT FALLS, MT       '\\r\\n   'HVR'='HAVRE, MT             '\\r\\n   'HEL'='HELENA, MT            '\\r\\n   'LWT'='LEWISTON, MT          '\\r\\n   'MGM'='MORGAN, MT            '\\r\\n   'OPH'='OPHEIM, MT            '\\r\\n   'PIE'='PIEGAN, MT            '\\r\\n   'RAY'='RAYMOND, MT           '\\r\\n   'ROO'='ROOSVILLE, MT         '\\r\\n   'SCO'='SCOBEY, MT            '\\r\\n   'SWE'='SWEETGTASS, MT        '\\r\\n   'TRL'='TRIAL CREEK, MT       '\\r\\n   'TUR'='TURNER, MT            '\\r\\n   'WCM'='WILLOW CREEK, MT      '\\r\\n   'CLT'='CHARLOTTE, NC         '\\r\\n   'FAY'='FAYETTEVILLE, NC'\\r\\n   'MRH'='MOREHEAD CITY, NC     '\\r\\n   'FOP'='MORRIS FIELDS AAF, NC'\\r\\n   'GSO'='PIEDMONT TRIAD INTL AIRPORT, NC'\\r\\n   'RDU'='RALEIGH/DURHAM, NC    '\\r\\n   'SSC'='SHAW AFB - SUMTER, NC'\\r\\n   'WIL'='WILMINGTON, NC        '\\r\\n   'AMB'='AMBROSE, ND           '\\r\\n   'ANT'='ANTLER, ND            '\\r\\n   'CRY'='CARBURY, ND           '\\r\\n   'DNS'='DUNSEITH, ND          '\\r\\n   'FAR'='FARGO, ND             '\\r\\n   'FRT'='FORTUNA, ND           '\\r\\n   'GRF'='GRAND FORKS, ND       '\\r\\n   'HNN'='HANNAH, ND            '\\r\\n   'HNS'='HANSBORO, ND          '\\r\\n   'MAI'='MAIDA, ND             '\\r\\n   'MND'='MINOT, ND             '\\r\\n   'NEC'='NECHE, ND             '\\r\\n   'NOO'='NOONAN, ND            '\\r\\n   'NRG'='NORTHGATE, ND         '\\r\\n   'PEM'='PEMBINA, ND           '\\r\\n   'SAR'='SARLES, ND            '\\r\\n   'SHR'='SHERWOOD, ND          '\\r\\n   'SJO'='ST JOHN, ND           '\\r\\n   'WAL'='WALHALLA, ND          '\\r\\n   'WHO'='WESTHOPE, ND          '\\r\\n   'WND'='WILLISTON, ND         '\\r\\n   'OMA'='OMAHA, NE             '\\r\\n   'LEB'='LEBANON, NH           '\\r\\n   'MHT'='MANCHESTER, NH'\\r\\n   'PNH'='PITTSBURG, NH         '\\r\\n   'PSM'='PORTSMOUTH, NH        '\\r\\n   'BYO'='BAYONNE, NJ           '\\r\\n   'CNJ'='CAMDEN, NJ            '\\r\\n   'HOB'='HOBOKEN, NJ           '\\r\\n   'JER'='JERSEY CITY, NJ       '\\r\\n   'WRI'='MC GUIRE AFB - WRIGHTSOWN, NJ'\\r\\n   'MMU'='MORRISTOWN, NJ'\\r\\n   'NEW'='NEWARK/TETERBORO, NJ  '\\r\\n   'PER'='PERTH AMBOY, NJ       '\\r\\n   'ACY'='POMONA FIELD - ATLANTIC CITY, NJ'\\r\\n   'ALA'='ALAMAGORDO, NM (BPS)'\\r\\n   'ABQ'='ALBUQUERQUE, NM       '\\r\\n   'ANP'='ANTELOPE WELLS, NM    '\\r\\n   'CRL'='CARLSBAD, NM          '\\r\\n   'COL'='COLUMBUS, NM          '\\r\\n   'CDD'='CRANE LAKE - ST. LOUIS CNTY, NM'\\r\\n   'DNM'='DEMING, NM (BPS)'\\r\\n   'LAS'='LAS CRUCES, NM        '\\r\\n   'LOB'='LORDSBURG, NM (BPS)'\\r\\n   'RUI'='RUIDOSO, NM'\\r\\n   'STR'='SANTA TERESA, NM      '\\r\\n   'RNO'='CANNON INTL - RENO/TAHOE, NV'\\r\\n   'FLX'='FALLON MUNICIPAL AIRPORT, NV'\\r\\n   'LVG'='LAS VEGAS, NV         '\\r\\n   'REN'='RENO, NV              '\\r\\n   'ALB'='ALBANY, NY            '\\r\\n   'AXB'='ALEXANDRIA BAY, NY    '\\r\\n   'BUF'='BUFFALO, NY           '\\r\\n   'CNH'='CANNON CORNERS, NY'\\r\\n   'CAP'='CAPE VINCENT, NY      '\\r\\n   'CHM'='CHAMPLAIN, NY         '\\r\\n   'CHT'='CHATEAUGAY, NY        '\\r\\n   'CLA'='CLAYTON, NY           '\\r\\n   'FTC'='FORT COVINGTON, NY    '\\r\\n   'LAG'='LA GUARDIA, NY        '\\r\\n   'LEW'='LEWISTON, NY          '\\r\\n   'MAS'='MASSENA, NY           '\\r\\n   'MAG'='MCGUIRE AFB, NY       '\\r\\n   'MOO'='MOORES, NY            '\\r\\n   'MRR'='MORRISTOWN, NY        '\\r\\n   'NYC'='NEW YORK, NY          '\\r\\n   'NIA'='NIAGARA FALLS, NY     '\\r\\n   'OGD'='OGDENSBURG, NY        '\\r\\n   'OSW'='OSWEGO, NY            '\\r\\n   'ELM'='REGIONAL ARPT - HORSEHEAD, NY'\\r\\n   'ROC'='ROCHESTER, NY         '\\r\\n   'ROU'='ROUSES POINT, NY      '\\r\\n   'SWF'='STEWART - ORANGE CNTY, NY'\\r\\n   'SYR'='SYRACUSE, NY          '\\r\\n   'THO'='THOUSAND ISLAND BRIDGE, NY'\\r\\n   'TRO'='TROUT RIVER, NY       '\\r\\n   'WAT'='WATERTOWN, NY         '\\r\\n   'HPN'='WESTCHESTER - WHITE PLAINS, NY'\\r\\n   'WRB'='WHIRLPOOL BRIDGE, NY'\\r\\n   'YOU'='YOUNGSTOWN, NY        '\\r\\n   'AKR'='AKRON, OH             '\\r\\n   'ATB'='ASHTABULA, OH         '\\r\\n   'CIN'='CINCINNATI, OH        '\\r\\n   'CLE'='CLEVELAND, OH         '\\r\\n   'CLM'='COLUMBUS, OH          '\\r\\n   'LOR'='LORAIN, OH            '\\r\\n   'MBO'='MARBLE HEADS, OH      '\\r\\n   'SDY'='SANDUSKY, OH          '\\r\\n   'TOL'='TOLEDO, OH            '\\r\\n   'OKC'='OKLAHOMA CITY, OK     '\\r\\n   'TUL'='TULSA, OK'\\r\\n   'AST'='ASTORIA, OR           '\\r\\n   'COO'='COOS BAY, OR          '\\r\\n   'HIO'='HILLSBORO, OR'\\r\\n   'MED'='MEDFORD, OR           '\\r\\n   'NPT'='NEWPORT, OR           '\\r\\n   'POO'='PORTLAND, OR          '\\r\\n   'PUT'='PUT-IN-BAY, OH        '\\r\\n   'RDM'='ROBERTS FIELDS - REDMOND, OR'\\r\\n   'ERI'='ERIE, PA              '\\r\\n   'MDT'='HARRISBURG, PA'\\r\\n   'HSB'='HARRISONBURG, PA      '\\r\\n   'PHI'='PHILADELPHIA, PA      '\\r\\n   'PIT'='PITTSBURG, PA         '\\r\\n   'AGU'='AGUADILLA, PR         '\\r\\n   'BQN'='BORINQUEN - AGUADILLO, PR'\\r\\n   'JCP'='CULEBRA - BENJAMIN RIVERA, PR'\\r\\n   'ENS'='ENSENADA, PR          '\\r\\n   'FAJ'='FAJARDO, PR           '\\r\\n   'HUM'='HUMACAO, PR           '\\r\\n   'JOB'='JOBOS, PR             '\\r\\n   'MAY'='MAYAGUEZ, PR          '\\r\\n   'PON'='PONCE, PR             '\\r\\n   'PSE'='PONCE-MERCEDITA, PR'\\r\\n   'SAJ'='SAN JUAN, PR          '\\r\\n   'VQS'='VIEQUES-ARPT, PR'\\r\\n   'PRO'='PROVIDENCE, RI        '\\r\\n   'PVD'='THEODORE FRANCIS - WARWICK, RI'\\r\\n   'CHL'='CHARLESTON, SC        '\\r\\n   'CAE'='COLUMBIA, SC #ARPT'\\r\\n   'GEO'='GEORGETOWN, SC        '\\r\\n   'GSP'='GREENVILLE, SC'\\r\\n   'GRR'='GREER, SC'\\r\\n   'MYR'='MYRTLE BEACH, SC'\\r\\n   'SPF'='BLACK HILLS, SPEARFISH, SD'\\r\\n   'HON'='HOWES REGIONAL ARPT - HURON, SD'\\r\\n   'SAI'='SAIPAN, SPN           '\\r\\n   'TYS'='MC GHEE TYSON - ALCOA, TN'\\r\\n   'MEM'='MEMPHIS, TN           '\\r\\n   'NSV'='NASHVILLE, TN         '\\r\\n   'TRI'='TRI CITY ARPT, TN'\\r\\n   'ADS'='ADDISON AIRPORT- ADDISON, TX'\\r\\n   'ADT'='AMISTAD DAM, TX       '\\r\\n   'ANZ'='ANZALDUAS, TX'\\r\\n   'AUS'='AUSTIN, TX            '\\r\\n   'BEA'='BEAUMONT, TX          '\\r\\n   'BBP'='BIG BEND PARK, TX (BPS)'\\r\\n   'SCC'='BP SPEC COORD. CTR, TX'\\r\\n   'BTC'='BP TACTICAL UNIT, TX  ' \\r\\n   'BOA'='BRIDGE OF AMERICAS, TX'\\r\\n   'BRO'='BROWNSVILLE, TX       '\\r\\n   'CRP'='CORPUS CHRISTI, TX    '\\r\\n   'DAL'='DALLAS, TX            '\\r\\n   'DLR'='DEL RIO, TX           '\\r\\n   'DNA'='DONNA, TX'\\r\\n   'EGP'='EAGLE PASS, TX        '\\r\\n   'ELP'='EL PASO, TX           '\\r\\n   'FAB'='FABENS, TX            '\\r\\n   'FAL'='FALCON HEIGHTS, TX    '\\r\\n   'FTH'='FORT HANCOCK, TX      '\\r\\n   'AFW'='FORT WORTH ALLIANCE, TX'\\r\\n   'FPT'='FREEPORT, TX          '\\r\\n   'GAL'='GALVESTON, TX         '\\r\\n   'HLG'='HARLINGEN, TX         '\\r\\n   'HID'='HIDALGO, TX           '\\r\\n   'HOU'='HOUSTON, TX           '\\r\\n   'SGR'='HULL FIELD, SUGAR LAND ARPT, TX'\\r\\n   'LLB'='JUAREZ-LINCOLN BRIDGE, TX'\\r\\n   'LCB'='LAREDO COLUMBIA BRIDGE, TX'\\r\\n   'LRN'='LAREDO NORTH, TX      '\\r\\n   'LAR'='LAREDO, TX            '\\r\\n   'LSE'='LOS EBANOS, TX        '\\r\\n   'IND'='LOS INDIOS, TX'\\r\\n   'LOI'='LOS INDIOS, TX        '\\r\\n   'MRS'='MARFA, TX (BPS)'\\r\\n   'MCA'='MCALLEN, TX           '\\r\\n   'MAF'='ODESSA REGIONAL, TX'\\r\\n   'PDN'='PASO DEL NORTE,TX     '\\r\\n   'PBB'='PEACE BRIDGE, NY      '\\r\\n   'PHR'='PHARR, TX             '\\r\\n   'PAR'='PORT ARTHUR, TX       '\\r\\n   'ISB'='PORT ISABEL, TX       '\\r\\n   'POE'='PORT OF EL PASO, TX   '\\r\\n   'PRE'='PRESIDIO, TX          '\\r\\n   'PGR'='PROGRESO, TX          '\\r\\n   'RIO'='RIO GRANDE CITY, TX   '\\r\\n   'ROM'='ROMA, TX              '\\r\\n   'SNA'='SAN ANTONIO, TX       '\\r\\n   'SNN'='SANDERSON, TX         '\\r\\n   'VIB'='VETERAN INTL BRIDGE, TX'\\r\\n   'YSL'='YSLETA, TX            '\\r\\n   'CHA'='CHARLOTTE AMALIE, VI  '\\r\\n   'CHR'='CHRISTIANSTED, VI     '\\r\\n   'CRU'='CRUZ BAY, ST JOHN, VI '\\r\\n   'FRK'='FREDERIKSTED, VI      '\\r\\n   'STT'='ST THOMAS, VI         '\\r\\n   'LGU'='CACHE AIRPORT - LOGAN, UT'\\r\\n   'SLC'='SALT LAKE CITY, UT    '\\r\\n   'CHO'='ALBEMARLE CHARLOTTESVILLE, VA'\\r\\n   'DAA'='DAVISON AAF - FAIRFAX CNTY, VA'\\r\\n   'HOP'='HOPEWELL, VA          '\\r\\n   'HEF'='MANASSAS, VA #ARPT'\\r\\n   'NWN'='NEWPORT, VA           '\\r\\n   'NOR'='NORFOLK, VA           '\\r\\n   'RCM'='RICHMOND, VA          '\\r\\n   'ABS'='ALBURG SPRINGS, VT    '\\r\\n   'ABG'='ALBURG, VT            '\\r\\n   'BEB'='BEEBE PLAIN, VT       '\\r\\n   'BEE'='BEECHER FALLS, VT     '\\r\\n   'BRG'='BURLINGTON, VT        '\\r\\n   'CNA'='CANAAN, VT            '\\r\\n   'DER'='DERBY LINE, VT (I-91) '\\r\\n   'DLV'='DERBY LINE, VT (RT. 5)'\\r\\n   'ERC'='EAST RICHFORD, VT     '\\r\\n   'HIG'='HIGHGATE SPRINGS, VT  '\\r\\n   'MOR'='MORSES LINE, VT       '\\r\\n   'NPV'='NEWPORT, VT           '\\r\\n   'NRT'='NORTH TROY, VT        '\\r\\n   'NRN'='NORTON, VT            '\\r\\n   'PIV'='PINNACLE ROAD, VT     '\\r\\n   'RIF'='RICHFORT, VT          '\\r\\n   'STA'='ST ALBANS, VT         '\\r\\n   'SWB'='SWANTON, VT (BP - SECTOR HQ)'\\r\\n   'WBE'='WEST BERKSHIRE, VT    '\\r\\n   'ABE'='ABERDEEN, WA          '\\r\\n   'ANA'='ANACORTES, WA         '\\r\\n   'BEL'='BELLINGHAM, WA        '\\r\\n   'BLI'='BELLINGHAM, WASHINGTON #INTL'\\r\\n   'BLA'='BLAINE, WA            '\\r\\n   'BWA'='BOUNDARY, WA          '\\r\\n   'CUR'='CURLEW, WA (BPS)'\\r\\n   'DVL'='DANVILLE, WA          '\\r\\n   'EVE'='EVERETT, WA           '\\r\\n   'FER'='FERRY, WA             '\\r\\n   'FRI'='FRIDAY HARBOR, WA     '\\r\\n   'FWA'='FRONTIER, WA          '\\r\\n   'KLM'='KALAMA, WA            '\\r\\n   'LAU'='LAURIER, WA           '\\r\\n   'LON'='LONGVIEW, WA          '\\r\\n   'MET'='METALINE FALLS, WA    '\\r\\n   'MWH'='MOSES LAKE GRANT COUNTY ARPT, WA'\\r\\n   'NEA'='NEAH BAY, WA          '\\r\\n   'NIG'='NIGHTHAWK, WA         '\\r\\n   'OLY'='OLYMPIA, WA           '\\r\\n   'ORO'='OROVILLE, WA          '\\r\\n   'PWB'='PASCO, WA             '\\r\\n   'PIR'='POINT ROBERTS, WA     '\\r\\n   'PNG'='PORT ANGELES, WA      '\\r\\n   'PTO'='PORT TOWNSEND, WA     '\\r\\n   'SEA'='SEATTLE, WA           '\\r\\n   'SPO'='SPOKANE, WA           '\\r\\n   'SUM'='SUMAS, WA             '\\r\\n   'TAC'='TACOMA, WA            '\\r\\n   'PSC'='TRI-CITIES - PASCO, WA'\\r\\n   'VAN'='VANCOUVER, WA         '\\r\\n   'AGM'='ALGOMA, WI            '\\r\\n   'BAY'='BAYFIELD, WI          '\\r\\n   'GRB'='GREEN BAY, WI         '\\r\\n   'MNW'='MANITOWOC, WI         '\\r\\n   'MIL'='MILWAUKEE, WI         '\\r\\n   'MSN'='TRUAX FIELD - DANE COUNTY, WI'\\r\\n   'CHS'='CHARLESTON, WV        '\\r\\n   'CLK'='CLARKSBURG, WV        '\\r\\n   'BLF'='MERCER COUNTY, WV'\\r\\n   'CSP'='CASPER, WY            '\\r\\n   'XXX'='NOT REPORTED/UNKNOWN  ' \\r\\n   '888'='UNIDENTIFED AIR / SEAPORT'\\r\\n   'UNK'='UNKNOWN POE           '\\r\\n   'CLG'='CALGARY, CANADA       '\\r\\n   'EDA'='EDMONTON, CANADA      '\\r\\n   'YHC'='HAKAI PASS, CANADA'\\r\\n   'HAL'='Halifax, NS, Canada   '\\r\\n   'MON'='MONTREAL, CANADA      '\\r\\n   'OTT'='OTTAWA, CANADA        '\\r\\n   'YXE'='SASKATOON, CANADA'\\r\\n   'TOR'='TORONTO, CANADA       '\\r\\n   'VCV'='VANCOUVER, CANADA     '\\r\\n   'VIC'='VICTORIA, CANADA      '\\r\\n   'WIN'='WINNIPEG, CANADA      '\\r\\n   'AMS'='AMSTERDAM-SCHIPHOL, NETHERLANDS'\\r\\n   'ARB'='ARUBA, NETH ANTILLES  '\\r\\n   'BAN'='BANKOK, THAILAND      '\\r\\n   'BEI'='BEICA #ARPT, ETHIOPIA'\\r\\n   'PEK'='BEIJING CAPITAL INTL, PRC'\\r\\n   'BDA'='KINDLEY FIELD, BERMUDA'\\r\\n   'BOG'='BOGOTA, EL DORADO #ARPT, COLOMBIA'\\r\\n   'EZE'='BUENOS AIRES, MINISTRO PIST, ARGENTINA'\\r\\n   'CUN'='CANCUN, MEXICO'\\r\\n   'CRQ'='CARAVELAS, BA #ARPT, BRAZIL'\\r\\n   'MVD'='CARRASCO, URUGUAY'\\r\\n   'DUB'='DUBLIN, IRELAND       '\\r\\n   'FOU'='FOUGAMOU #ARPT, GABON'\\r\\n   'FBA'='FREEPORT, BAHAMAS      '\\r\\n   'MTY'='GEN M. ESCOBEDO, Monterrey, MX'\\r\\n   'HMO'='GEN PESQUEIRA GARCIA, MX'\\r\\n   'GCM'='GRAND CAYMAN, CAYMAN ISLAND'\\r\\n   'GDL'='GUADALAJARA, MIGUEL HIDAL, MX'\\r\\n   'HAM'='HAMILTON, BERMUDA     '\\r\\n   'ICN'='INCHON, SEOUL KOREA'\\r\\n   'IWA'='INVALID - IWAKUNI, JAPAN'\\r\\n   'CND'='KOGALNICEANU, ROMANIA'\\r\\n   'LAH'='LABUHA ARPT, INDONESIA'\\r\\n   'DUR'='LOUIS BOTHA, SOUTH AFRICA'\\r\\n   'MAL'='MANGOLE ARPT, INDONESIA'\\r\\n   'MDE'='MEDELLIN, COLOMBIA'\\r\\n   'MEX'='JUAREZ INTL, MEXICO CITY, MX'\\r\\n   'LHR'='MIDDLESEX, ENGLAND'\\r\\n   'NBO'='NAIROBI, KENYA        '\\r\\n   'NAS'='NASSAU, BAHAMAS       '\\r\\n   'NCA'='NORTH CAICOS, TURK & CAIMAN'\\r\\n   'PTY'='OMAR TORRIJOS, PANAMA'\\r\\n   'SPV'='PAPUA, NEW GUINEA'\\r\\n   'UIO'='QUITO (MARISCAL SUCR), ECUADOR'\\r\\n   'RIT'='ROME, ITALY           '\\r\\n   'SNO'='SAKON NAKHON #ARPT, THAILAND'\\r\\n   'SLP'='SAN LUIS POTOSI #ARPT, MEXICO'\\r\\n   'SAN'='SAN SALVADOR, EL SALVADOR'\\r\\n   'SRO'='SANTANA RAMOS #ARPT, COLOMBIA'\\r\\n   'GRU'='GUARULHOS INTL, SAO PAULO, BRAZIL'\\r\\n   'SHA'='SHANNON, IRELAND      '\\r\\n   'HIL'='SHILLAVO, ETHIOPIA'\\r\\n   'TOK'='TOROKINA #ARPT, PAPUA, NEW GUINEA'\\r\\n   'VER'='VERACRUZ, MEXICO'\\r\\n   'LGW'='WEST SUSSEX, ENGLAND  '\\r\\n   'ZZZ'='MEXICO Land (Banco de Mexico) '\\r\\n   'CHN'='No PORT Code (CHN)'\\r\\n   'CNC'='CANNON CORNERS, NY'\\r\\n   'MAA'='Abu Dhabi'\\r\\n   'AG0'='MAGNOLIA, AR'\\r\\n   'BHM'='BAR HARBOR, ME'\\r\\n   'BHX'='BIRMINGHAM, AL'\\r\\n   'CAK'='AKRON, OH'\\r\\n   'FOK'='SUFFOLK COUNTY, NY'\\r\\n   'LND'='LANDER, WY'\\r\\n   'MAR'='MARFA, TX'\\r\\n   'MLI'='MOLINE, IL'\\r\\n   'RIV'='RIVERSIDE, CA'\\r\\n   'RME'='ROME, NY'\\r\\n   'VNY'='VAN NUYS, CA'\\r\\n   'YUM'='YUMA, AZ'\\r\\n   'FRG'='Collapsed (FOK) 06/15'\\r\\n   'HRL'='Collapsed (HLG) 06/15'\\r\\n   'ISP'='Collapsed (FOK) 06/15'\\r\\n   'JSJ'='Collapsed (SAJ) 06/15'\\r\\n   'BUS'='Collapsed (BUF) 06/15'\\r\\n   'IAG'='Collapsed (NIA) 06/15'\\r\\n   'PHN'='Collapsed (PHU) 06/15'\\r\\n   'STN'='Collapsed (STR) 06/15'\\r\\n   'VMB'='Collapsed (VNB) 06/15'\\r\\n   'T01'='Collapsed (SEA) 06/15'\\r\\n   'PHF'='No PORT Code (PHF)'\\r\\n   'DRV'='No PORT Code (DRV)'\\r\\n   'FTB'='No PORT Code (FTB)'\\r\\n   'GAC'='No PORT Code (GAC)'\\r\\n   'GMT'='No PORT Code (GMT)'\\r\\n   'JFA'='No PORT Code (JFA)'\\r\\n   'JMZ'='No PORT Code (JMZ)'\\r\\n   'NC8'='No PORT Code (NC8)'\\r\\n   'NYL'='No PORT Code (NYL)'\\r\\n   'OAI'='No PORT Code (OAI)'\\r\\n   'PCW'='No PORT Code (PCW)'\\r\\n   'WA5'='No PORT Code (WAS)'\\r\\n   'WTR'='No PORT Code (WTR)'\\r\\n   'X96'='No PORT Code (X96)'\\r\\n   'XNA'='No PORT Code (XNA)'\\r\\n   'YGF'='No PORT Code (YGF)'\\r\\n   '5T6'='No PORT Code (5T6)'\\r\\n   '060'='No PORT Code (60)'\\r\\n   'SP0'='No PORT Code (SP0)'\\r\\n   'W55'='No PORT Code (W55)'\\r\\n   'X44'='No PORT Code (X44)'\\r\\n   'AUH'='No PORT Code (AUH)'\\r\\n   'RYY'='No PORT Code (RYY)'\\r\\n   'SUS'='No PORT Code (SUS)'\\r\\n   '74S'='No PORT Code (74S)'\\r\\n   'ATW'='No PORT Code (ATW)'\\r\\n   'CPX'='No PORT Code (CPX)'\\r\\n   'MTH'='No PORT Code (MTH)'\\r\\n   'PFN'='No PORT Code (PFN)'\\r\\n   'SCH'='No PORT Code (SCH)'\\r\\n   'ASI'='No PORT Code (ASI)'\\r\\n   'BKF'='No PORT Code (BKF)'\\r\\n   'DAY'='No PORT Code (DAY)'\\r\\n   'Y62'='No PORT Code (Y62)'\\r\\n   'AG'='No PORT Code (AG)'\\r\\n   'BCM'='No PORT Code (BCM)'\\r\\n   'DEC'='No PORT Code (DEC)'\\r\\n   'PLB'='No PORT Code (PLB)'\\r\\n   'CXO'='No PORT Code (CXO)'\\r\\n   'JBQ'='No PORT Code (JBQ)'\\r\\n   'JIG'='No PORT Code (JIG)'\\r\\n   'OGS'='No PORT Code (OGS)'\\r\\n   'TIW'='No PORT Code (TIW)'\\r\\n   'OTS'='No PORT Code (OTS)'\\r\\n   'AMT'='No PORT Code (AMT)'\\r\\n   'EGE'='No PORT Code (EGE)'\\r\\n   'GPI'='No PORT Code (GPI)'\\r\\n   'NGL'='No PORT Code (NGL)'\\r\\n   'OLM'='No PORT Code (OLM)'\\r\\n   '.GA'='No PORT Code (.GA)'\\r\\n   'CLX'='No PORT Code (CLX)'\\r\\n   'CP '='No PORT Code (CP)'\\r\\n   'FSC'='No PORT Code (FSC)'\\r\\n   'NK' ='No PORT Code (NK)'\\r\\n   'ADU' ='No PORT Code (ADU)'\\r\\n   'AKT' ='No PORT Code (AKT)'\\r\\n   'LIT' ='No PORT Code (LIT)'\\r\\n   'A2A' ='No PORT Code (A2A)'\\r\\n   'OSN' ='No PORT Code (OSN)'\\r\\n;\\r\\n\\r\\n\\r\\n/* ARRDATE is the Arrival Date in the USA. It is a SAS date numeric field that a \\r\\n   permament format has not been applied.  Please apply whichever date format \\r\\n   works for you. */\\r\\n\\r\\n\\r\\n/* I94MODE - There are missing values as well as not reported (9) */\\r\\nvalue i94model\\r\\n1 = 'Air'\\r\\n2 = 'Sea'\\r\\n3 = 'Land'\\r\\n9 = 'Not reported' ;\\r\\n\\r\\n\\r\\n/* I94ADDR - There is lots of invalid codes in this variable and the list below \\r\\n   shows what we have found to be valid, everything else goes into 'other' */\\r\\nvalue i94addrl\\r\\n'AL'='ALABAMA'\\r\\n'AK'='ALASKA'\\r\\n'AZ'='ARIZONA'\\r\\n'AR'='ARKANSAS'\\r\\n'CA'='CALIFORNIA'\\r\\n'CO'='COLORADO'\\r\\n'CT'='CONNECTICUT'\\r\\n'DE'='DELAWARE'\\r\\n'DC'='DIST. OF COLUMBIA'\\r\\n'FL'='FLORIDA'\\r\\n'GA'='GEORGIA'\\r\\n'GU'='GUAM'\\r\\n'HI'='HAWAII'\\r\\n'ID'='IDAHO'\\r\\n'IL'='ILLINOIS'\\r\\n'IN'='INDIANA'\\r\\n'IA'='IOWA'\\r\\n'KS'='KANSAS'\\r\\n'KY'='KENTUCKY'\\r\\n'LA'='LOUISIANA'\\r\\n'ME'='MAINE'\\r\\n'MD'='MARYLAND'\\r\\n'MA'='MASSACHUSETTS'\\r\\n'MI'='MICHIGAN'\\r\\n'MN'='MINNESOTA'\\r\\n'MS'='MISSISSIPPI'\\r\\n'MO'='MISSOURI'\\r\\n'MT'='MONTANA'\\r\\n'NC'='N. CAROLINA'\\r\\n'ND'='N. DAKOTA'\\r\\n'NE'='NEBRASKA'\\r\\n'NV'='NEVADA'\\r\\n'NH'='NEW HAMPSHIRE'\\r\\n'NJ'='NEW JERSEY'\\r\\n'NM'='NEW MEXICO'\\r\\n'NY'='NEW YORK'\\r\\n'OH'='OHIO'\\r\\n'OK'='OKLAHOMA'\\r\\n'OR'='OREGON'\\r\\n'PA'='PENNSYLVANIA'\\r\\n'PR'='PUERTO RICO'\\r\\n'RI'='RHODE ISLAND'\\r\\n'SC'='S. CAROLINA'\\r\\n'SD'='S. DAKOTA'\\r\\n'TN'='TENNESSEE'\\r\\n'TX'='TEXAS'\\r\\n'UT'='UTAH'\\r\\n'VT'='VERMONT'\\r\\n'VI'='VIRGIN ISLANDS'\\r\\n'VA'='VIRGINIA'\\r\\n'WV'='W. VIRGINIA'\\r\\n'WA'='WASHINGTON'\\r\\n'WI'='WISCONSON'\\r\\n'WY'='WYOMING' \\r\\n'99'='All Other Codes' ;\\r\\n\\r\\n/* DEPDATE is the Departure Date from the USA. It is a SAS date numeric field that\\r\\n   a permament format has not been applied.  Please apply whichever date format\\r\\n   works for you. */\\r\\n\\r\\n\\r\\n/* I94BIR - Age of Respondent in Years */\\r\\n\\r\\n\\r\\n/* I94VISA - Visa codes collapsed into three categories:\\r\\n   1 = Business\\r\\n   2 = Pleasure\\r\\n   3 = Student\\r\\n*/\\r\\n\\r\\n\\r\\n/* COUNT - Used for summary statistics */\\r\\n\\r\\n\\r\\n/* DTADFILE - Character Date Field - Date added to I-94 Files - CIC does not use */\\r\\n\\r\\n\\r\\n/* VISAPOST - Department of State where where Visa was issued - CIC does not use */\\r\\n\\r\\n\\r\\n/* OCCUP - Occupation that will be performed in U.S. - CIC does not use */\\r\\n\\r\\n\\r\\n/* ENTDEPA - Arrival Flag - admitted or paroled into the U.S. - CIC does not use */\\r\\n\\r\\n\\r\\n/* ENTDEPD - Departure Flag - Departed, lost I-94 or is deceased - CIC does not use */\\r\\n\\r\\n\\r\\n/* ENTDEPU - Update Flag - Either apprehended, overstayed, adjusted to perm residence - CIC does not use */\\r\\n\\r\\n\\r\\n/* MATFLAG - Match flag - Match of arrival and departure records */\\r\\n\\r\\n\\r\\n/* BIRYEAR - 4 digit year of birth */\\r\\n\\r\\n\\r\\n/* DTADDTO - Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use */\\r\\n\\r\\n\\r\\n/* GENDER - Non-immigrant sex */\\r\\n\\r\\n\\r\\n/* INSNUM - INS number */\\r\\n\\r\\n\\r\\n/* AIRLINE - Airline used to arrive in U.S. */\\r\\n\\r\\n\\r\\n/* ADMNUM - Admission Number */\\r\\n\\r\\n\\r\\n/* FLTNO - Flight number of Airline used to arrive in U.S. */\\r\\n\\r\\n\\r\\n/* VISATYPE - Class of admission legally admitting the non-immigrant to temporarily stay in U.S. */\\r\\nrun ;\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/14 10:36:48 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@350a50ef[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@2013b5fc[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@685a337e]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:36:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:36:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@194f0f79[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@5fe6b303[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@380c1d38]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:36:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:37:08 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1ad80167[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@3800ccf7[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@59d52849]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:37:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:37:18 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@458c8f99[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@5b94dc88[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@11d2da4]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:37:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:37:28 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6a4c3f24[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@62ae4a3d[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@437fe1b6]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:37:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:37:38 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1bd64c75[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@7dd6f8ec[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@76e85d6d]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:37:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:37:48 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@5507ebb8[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@623ceb53[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@4ba0b685]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:37:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:37:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@7318efb9[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@1683dc52[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@7c33bf11]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:37:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:38:08 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@11456f06[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@87a5f70[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@3c2b0ce1]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:38:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:38:18 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@545982e8[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@43f90d0d[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@797e772b]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:38:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:38:28 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@7595ae2f[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@787a9d2f[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@60ac1e0a]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:38:28 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:38:38 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@721a0fa0[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@1053befa[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@486f8e09]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:38:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:38:48 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@76594fe4[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@260fbf74[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@4cac31a]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:38:48 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:38:58 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6b5a0221[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@4e44ffdb[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@725ebf98]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:38:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:39:08 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1ff9c950[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@38bf114c[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@3cf5770]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:39:08 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:39:18 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@6c67b040[Not completed, task = java.util.concurrent.Executors$RunnableAdapter@4f287e68[Wrapped task = org.apache.spark.rpc.netty.NettyRpcEnv$$anon$1@1eca8d94]] rejected from java.util.concurrent.ScheduledThreadPoolExecutor@1cbfa0c7[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "22/06/14 10:39:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1005)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:212)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:555)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:559)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 11 more\n",
      "22/06/14 10:39:18 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n",
      "22/06/14 10:39:28 ERROR Utils: Uncaught exception in thread shutdown-hook-0\n",
      "java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.executor.Executor.stop(Executor.scala:333)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$stopHookReference$1(Executor.scala:76)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<init>(ShuffleBlockPusher.scala:465)\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<clinit>(ShuffleBlockPusher.scala)\n",
      "\t... 16 more\n",
      "22/06/14 10:39:28 WARN ShutdownHookManager: ShutdownHook '' failed, java.util.concurrent.ExecutionException: java.lang.ExceptionInInitializerError\n",
      "java.util.concurrent.ExecutionException: java.lang.ExceptionInInitializerError\n",
      "\tat java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\n",
      "\tat java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager.executeShutdown(ShutdownHookManager.java:124)\n",
      "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:95)\n",
      "Caused by: java.lang.ExceptionInInitializerError\n",
      "\tat org.apache.spark.executor.Executor.stop(Executor.scala:333)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$stopHookReference$1(Executor.scala:76)\n",
      "\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2019)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.util.Try$.apply(Try.scala:213)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n",
      "\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: java.lang.NullPointerException\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<init>(ShuffleBlockPusher.scala:465)\n",
      "\tat org.apache.spark.shuffle.ShuffleBlockPusher$.<clinit>(ShuffleBlockPusher.scala)\n",
      "\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "s3_object = s3.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "text = s3_object['Body'].read()\n",
    "context = text.decode(encoding ='utf-8')\n",
    "# for obj in s3_object.objects.all():\n",
    "#     print(obj.key)\n",
    "\n",
    "context = context.replace('\\t', '')\n",
    "context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ****** Read data from local file system(be a s3) to spark DataFrame ******\n",
    ">>> s3_session = boto3.Session(aws_access_key_id='ASIAUW7BQYXKBE3COG4H',\n",
    "...                                    aws_secret_access_key='Bsqf1Tg/ajFm0bzsXiYs1zPxAeF65m5ZYNVI7E2b',\n",
    "...                                    aws_session_token='FwoGZXIvYXdzEIv//////////wEaDOPwODxNOGtesK0FRiLVAcHA4fdotLDXjivG9ANe54T0wdGYPdIKv8YAF1oeYdR1ThW9fo4J8Alx0DoSkOElGsCIZy7BHmJtl0LlpLlJ3wryLbZNJZ6AKODGit5OL8/E+4HgOtaJkPBUdtuwdVyg1IexI3qEE6v3f14axyNe32tt1MnsBOPHWniw3NpDSRJz/PksFf4VWkyCT6u5e9ebEWabfsAIZb3PBxhGGv0tZnA+4TZT2NQ+qYyOPFPUgCUiQalzDI7fN23XYGbItVMvP5l0hqAYaygqgLiL0gF57tGw51AL7ij515+VBjItwr531DHqobzWTNBl0EL1ldlXTYCaK8xx9wBWBYblPAdP0v7G3MNJrskxTz2l',\n",
    "...                                    region_name = 'us-west-2')\n",
    ">>> s3 = s3_session.resource('s3')\n",
    ">>> s3_object = s3.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    ">>> text = s3_object['Body'].read()\n",
    ">>> context = text.decode(encoding ='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=31'>32</a>\u001b[0m \u001b[39m# TODO:OK: three columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=32'>33</a>\u001b[0m \u001b[39m# imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=33'>34</a>\u001b[0m \u001b[39m# df_imm_city_res_label = spark.sparkContext.parallelize(imm_cit_res_three).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\", \"value_of_imm_cntyl_organizations\"]) \\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=54'>55</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=55'>56</a>\u001b[0m \u001b[39m# TODO:OK: two columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=56'>57</a>\u001b[0m imm_addr_two, imm_addr_three \u001b[39m=\u001b[39m code_mapping(context, \u001b[39m\"\u001b[39m\u001b[39mi94addrl\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=57'>58</a>\u001b[0m df_imm_address \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msparkContext\u001b[39m.\u001b[39mparallelize(imm_addr_two)\u001b[39m.\u001b[39mtoDF([\u001b[39m\"\u001b[39m\u001b[39mcode_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mvalue_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m]) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=58'>59</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mcode_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mcode_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mString\u001b[39m\u001b[39m\"\u001b[39m)) \\\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=59'>60</a>\u001b[0m     \u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mvalue_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m, col(\u001b[39m\"\u001b[39m\u001b[39mvalue_of_imm_address\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mcast(\u001b[39m\"\u001b[39m\u001b[39mString\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=60'>61</a>\u001b[0m df_imm_address\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=63'>64</a>\u001b[0m \u001b[39m# TODO:OK: two columns\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=64'>65</a>\u001b[0m \u001b[39m# imm_visa = {'1': 'Business',\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=65'>66</a>\u001b[0m \u001b[39m#             '2': 'Pleasure',\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000007?line=66'>67</a>\u001b[0m \u001b[39m#             '3': 'Student'}\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "data_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "import boto3\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=os.environ[\"AWS_ACCESS_KEY_ID\"],\n",
    "    aws_secret_access_key=os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n",
    ")\n",
    "\n",
    "s3 = session.resource('s3')\n",
    "\n",
    "s3_object = s3.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "text = s3_object['Body'].read()\n",
    "context = text.decode(encoding ='utf-8')\n",
    "# for obj in s3_object.objects.all():\n",
    "#     print(obj.key)\n",
    "\n",
    "context = context.replace('\\t', '')\n",
    "\n",
    "\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(\n",
    "        ';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\")\n",
    "                         for line in content_line_split]\n",
    "    content_two_dims = [i.strip().split('=') for i in content_line_list[1:]]\n",
    "    content_three_dims = [[i[0].strip(), i[1].strip().split(', ')[:][0], e]\n",
    "                          for i in content_two_dims if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_two_dims, content_three_dims\n",
    "\n",
    "# TODO:OK: three columns\n",
    "# imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")\n",
    "# df_imm_city_res_label = spark.sparkContext.parallelize(imm_cit_res_three).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\", \"value_of_imm_cntyl_organizations\"]) \\\n",
    "#     .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\")) \\\n",
    "#     .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\")) \\\n",
    "#     .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl_organizations\").cast(\"String\")) \\\n",
    "\n",
    "# df_imm_city_res_label.show()\n",
    "\n",
    "# TODO:OK: three columns\n",
    "# imm_port_two, imm_port_three = code_mapping(context, \"i94prtl\")\n",
    "# df_imm_destination_city = spark.sparkContext.parallelize(imm_port_three).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"]) \\\n",
    "#                                                 .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "#                                                 .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "#                                                 .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\"))\n",
    "# df_imm_destination_city.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "# imm_mode_two, imm_mode_three = code_mapping(context, \"i94model\")\n",
    "# df_imm_travel_code = spark.sparkContext.parallelize(imm_mode_two).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"]) \\\n",
    "#                                            .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Integer\")) \\\n",
    "#                                            .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))\n",
    "# df_imm_travel_code.show()\n",
    "\n",
    "# TODO:OK: two columns\n",
    "imm_addr_two, imm_addr_three = code_mapping(context, \"i94addrl\")\n",
    "df_imm_address = spark.sparkContext.parallelize(imm_addr_two).toDF([\"code_of_imm_address\", \"value_of_imm_address\"]) \\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\"))\n",
    "df_imm_address.show()\n",
    "\n",
    "\n",
    "# TODO:OK: two columns\n",
    "# imm_visa = {'1': 'Business',\n",
    "#             '2': 'Pleasure',\n",
    "#             '3': 'Student'}\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(imm_visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"]) \\\n",
    "                                    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\"))\n",
    "# df_imm_visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: immigration_label\n",
    "\n",
    "- df_imm_city_res_label\n",
    "- df_imm_destination_city\n",
    "- df_imm_travel_code\n",
    "- df_imm_address\n",
    "- df_imm_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_label_data_read = SAS7BDAT(label_data)\n",
    "\n",
    "# t = list(imm_label_data_read)\n",
    "# TODO: make sure that is a correct data\n",
    "\n",
    "# def read_data_to_df(call_func):\n",
    "#     # def wrapper(label_data):\n",
    "#     def wrapper(*args, **kwargs):\n",
    "#         # return call_func(*args)\n",
    "#         print(\"Reading data...\")\n",
    "#         with open(label_data) as f:\n",
    "#             context = f.read().replace('\\t', '')\n",
    "#         call_func(context, *args, **kwargs)\n",
    "#         print(\"Data reading finished.\")\n",
    "#     return wrapper\n",
    "\n",
    "\n",
    "with open(label_data) as f:\n",
    "    context = f.read().replace('\\t', '')\n",
    "    # content_mapping = context[context.index('i94prtl'):]\n",
    "    # content_line_split = content_mapping[:content_mapping.index(\n",
    "    #     ';')].split('\\n')\n",
    "    # content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    # content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    # # content_dict = [[i[0].strip(), i[1].strip(), [e for e in i[1].strip().split(', ')[1:]]] for i in content_dict if len(i) == 2]\n",
    "    # content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "\n",
    "\n",
    "# @read_data_to_df\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_dict\n",
    "\n",
    "i94cit_res = code_mapping(context, \"i94cntyl\")\n",
    "i94port = code_mapping(context, \"i94prtl\")\n",
    "i94mode = code_mapping(context, \"i94model\")\n",
    "i94addr = code_mapping(context, \"i94addrl\")\n",
    "i94visa = {'1': 'Business',\n",
    "           '2': 'Pleasure',\n",
    "           '3': 'Student'}\n",
    "\n",
    "# for i in i94port:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"code1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Writing into aws s3 and format parquet file.\n",
    "df_imm_city_res_label = spark.sparkContext.parallelize(i94cit_res.items()).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\"])\\\n",
    "    .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\"))\n",
    "\n",
    "df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"])\\\n",
    "    .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\"))\n",
    "\n",
    "df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city_data\")\n",
    "df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city_data\")\n",
    "\n",
    "# df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city\")\n",
    "# df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city WHERE code_of_imm_destination_city IS NOT NULL\")\n",
    "\n",
    "df_imm_travel_code = spark.sparkContext.parallelize(i94mode.items()).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"])\\\n",
    "    .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))\n",
    "\n",
    "df_imm_address = spark.sparkContext.parallelize(i94addr.items()).toDF([\"code_of_imm_address\", \"value_of_imm_address\"])\\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\"))\\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\"))\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(i94visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"])\\\n",
    "    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\"))\n",
    "\n",
    "\n",
    "\n",
    "# df_imm_city_res_label.createOrReplaceTempView(\"imm_city_res_label\")\n",
    "\n",
    "# df_imm_city_res_label = spark.sql(\"SELECT * FROM imm_city_res_label\")\n",
    "# df_imm_city_res.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm_destination_city_tmp.show(n=5, truncate=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check data source\n",
    "for column in df_imm_city_res_label.columns:\n",
    "    n = df_imm_city_res_label.select(column).distinct().count()\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm_city_res.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: news_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path: data >> news_article\n",
    "\"\"\"Table: news_article schema\n",
    "pk: cord_uid -> news_cord_uid\n",
    "1. source_x -> news_source\n",
    "    schema: StringType()\n",
    "2. title -> news_title\n",
    "    schema: StringType()\n",
    "3. license -> news_licence\n",
    "    schema: StringType()\n",
    "4. abstract -> news_abstract\n",
    "    schema: StringType()\n",
    "5. publish_time -> news_publish_time (fk)\n",
    "    schema: TimestampType()\n",
    "6. authors -> news_authors\n",
    "    schema: StringType()\n",
    "7. url -> news_url\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "data_news = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "# news_schema = StructType([\n",
    "#     StructField(name=\"news_cord_uid\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_source\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_title\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_licence\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_abstract\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_publish_time\", dataType=DateType(), nullable=True),\n",
    "#     StructField(name=\"news_authors\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_url\", dataType=StringType(), nullable=True)\n",
    "# ])\n",
    "\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=data_news)\n",
    "\n",
    "df_news = df_news.withColumn(\"news_cord_uid\", col(\"cord_uid\").cast(\"String\")) \\\n",
    "                    .withColumn(\"news_source\", col(\"source_x\").cast(\"String\")) \\\n",
    "                        .withColumn(\"news_title\", col(\"title\").cast(\"String\")) \\\n",
    "                            .withColumn(\"news_licence\", col(\"license\").cast(\"String\")) \\\n",
    "                                .withColumn(\"news_abstract\", col(\"abstract\").cast(\"String\")) \\\n",
    "                                    .withColumn(\"news_publish_time\", to_date(df_news.publish_time, \"yyyy-MM-dd\")) \\\n",
    "                                        .withColumn(\"news_authors\", col(\"authors\").cast(\"String\")) \\\n",
    "                                            .withColumn(\"news_url\", col(\"url\").cast(\"String\")) \\\n",
    "                 .select(col(\"news_cord_uid\"),\n",
    "                         col(\"news_source\"),\n",
    "                         col(\"news_title\"),\n",
    "                         col(\"news_licence\"),\n",
    "                         col(\"news_abstract\"),\n",
    "                         col(\"news_publish_time\"),\n",
    "                         col(\"news_authors\"),\n",
    "                         col(\"news_url\")\n",
    "                         )\n",
    "\n",
    "df_news_tmp = df_news.createOrReplaceTempView(\"news_article_data\")\n",
    "\n",
    "df_news_tmp = spark.sql(\"SELECT DISTINCT publish_time FROM news_article_data\")\n",
    "\n",
    "df_news_tmp.persist()\n",
    "\n",
    "df_news_tmp.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_tmp.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Us Cities Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a us-cities data dimension table\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "pk: generated -> cidemo_id\n",
    "    schema: IntegerType()\n",
    "1. City -> cidemo_city\n",
    "    schema: StringType()\n",
    "2. State -> cidemo_state\n",
    "    schema: StringType()\n",
    "3. Median Age -> cidemo_median_age\n",
    "    schema: FloatType()\n",
    "4. Total Population -> cidemo_total_population\n",
    "    schema: IntegerType()\n",
    "5. State Code -> cidemo_state_code (fk)\n",
    "    schema: StringType()\n",
    "6. Count -> cidemo_count\n",
    "    schema: IntegerType()\n",
    "\"\"\"\n",
    "\n",
    "data_us_cities_demographics = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\"\n",
    "\n",
    "# TODO -> Must be defined a function that generated each table schema:\n",
    "us_cities_demographics_data_schema = StructType([\n",
    "    StructField(name=\"cidemo_city\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_median_age\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cidemo_total_population\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state_code\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_count\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using pyspark to read csv file\n",
    "df_us_cities_demographics = spark.read.options(header=True, delimiter=';').csv(data_us_cities_demographics)\n",
    "\"\"\"\n",
    "root\n",
    " |-- cidemo_city: string (nullable = true)\n",
    " |-- cidemo_state: string (nullable = true)\n",
    " |-- cidemo_median_age: float (nullable = true)\n",
    " |-- cidemo_total_population: integer (nullable = true)\n",
    " |-- cidemo_state_code: string (nullable = true)\n",
    " |-- cidemo_count: integer (nullable = true)\n",
    "\"\"\"\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_city\", col(\"City\").cast(\"String\")) \\\n",
    "                    .withColumn(\"cidemo_state\", col(\"State\").cast(\"String\")) \\\n",
    "                        .withColumn(\"cidemo_median_age\", col(\"Median Age\").cast(\"Float\")) \\\n",
    "                            .withColumn(\"cidemo_male_population\", col(\"Male Population\").cast(\"Integer\")) \\\n",
    "                                .withColumn(\"cidemo_female_population\", col(\"Female Population\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"cidemo_total_population\", col(\"Total Population\").cast(\"Integer\")) \\\n",
    "                                            .withColumn(\"cidemo_number_of_veterans\", col(\"Number of Veterans\").cast(\"Integer\")) \\\n",
    "                                                .withColumn(\"cidemo_foreign_born\", col(\"Foreign-born\").cast(\"Integer\")) \\\n",
    "                                                    .withColumn(\"cidemo_average_household_size\", col(\"Average Household Size\").cast(\"Float\")) \\\n",
    "                                                        .withColumn(\"cidemo_state_code\", col(\"State Code\").cast(\"String\")) \\\n",
    "                                                            .withColumn(\"cidemo_race\", col(\"Race\").cast(\"String\")) \\\n",
    "    .withColumn(\"cidemo_count\", col(\"Count\").cast(\"Integer\")) \\\n",
    "                    .select(col(\"cidemo_city\"),\n",
    "                            col(\"cidemo_state\"),\n",
    "                            col(\"cidemo_median_age\"),\n",
    "                            col(\"cidemo_total_population\"),\n",
    "                            col(\"cidemo_state_code\"),\n",
    "                            col(\"cidemo_count\"))\n",
    "\n",
    "# Auto-generated series of id\n",
    "df_us_cities_demographics = df_news.withColumn(\"cidemo_id\", monotonically_increasing_id())\n",
    "\n",
    "df_us_cities_demographics_temp = df_news.createOrReplaceTempView(\"us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp = spark.sql(\"SELECT * FROM us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp.persist()\n",
    "\n",
    "df_us_cities_demographics_temp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_cities_demographics_temp.show(n=5, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_cities_demographics_temp.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o124.load.\n: java.lang.NoClassDefFoundError: com/epam/parso/impl/SasFileReaderImpl\n\tat com.github.saurfang.sas.spark.SasRelation.inferSchema(SasRelation.scala:186)\n\tat com.github.saurfang.sas.spark.SasRelation.<init>(SasRelation.scala:73)\n\tat com.github.saurfang.sas.spark.SasRelation$.apply(SasRelation.scala:45)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:209)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:42)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:27)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000021?line=0'>1</a>\u001b[0m \u001b[39m# TODO: This code block is be placed in analysis jupyter notebook files\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000021?line=1'>2</a>\u001b[0m imm_data \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000021?line=4'>5</a>\u001b[0m df_imm_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mformat(\u001b[39m\"\u001b[39;49m\u001b[39mcom.github.saurfang.sas.spark\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mload(imm_data)\n",
      "File \u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py:158\u001b[0m, in \u001b[0;36mDataFrameReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=156'>157</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=157'>158</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mload(path))\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=158'>159</a>\u001b[0m \u001b[39melif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py?line=159'>160</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m!=\u001b[39m \u001b[39mlist\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1297'>1298</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1298'>1299</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1299'>1300</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1300'>1301</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1306'>1307</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o124.load.\n: java.lang.NoClassDefFoundError: com/epam/parso/impl/SasFileReaderImpl\n\tat com.github.saurfang.sas.spark.SasRelation.inferSchema(SasRelation.scala:186)\n\tat com.github.saurfang.sas.spark.SasRelation.<init>(SasRelation.scala:73)\n\tat com.github.saurfang.sas.spark.SasRelation$.apply(SasRelation.scala:45)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:209)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:42)\n\tat com.github.saurfang.sas.spark.DefaultSource.createRelation(DefaultSource.scala:27)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:350)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:274)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:245)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:245)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:188)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# TODO: This code block is be placed in analysis jupyter notebook files\n",
    "imm_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "\n",
    "\n",
    "df_imm_data = spark.read.format(\"com.github.saurfang.sas.spark\").load(imm_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_imm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "# show(n=5, truncate=5)\n",
    "df_immigration_personal = df_imm_data.withColumn(\"imm_per_cic_id\", col(\"cicid\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_person_birth_year\", col(\"biryear\").cast(\"Integer\"))\\\n",
    "           .withColumn(\"imm_person_gender\", col(\"gender\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_visatype\", col(\"visatype\").cast(\"String\")).select(col(\"imm_per_cic_id\"), \\\n",
    "                                                                              col(\"imm_person_birth_year\"), \\\n",
    "                                                                              col(\"imm_person_gender\"), \\\n",
    "                                                                              col(\"imm_visatype\"))\n",
    "\n",
    "df_immigration_personal_tmp = df_immigration_personal.createOrReplaceTempView(\"imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp = spark.sql(\"SELECT * FROM imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp.persist()\n",
    "\n",
    "df_immigration_personal_tmp.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr: 4 digit year of the arrival  -> imm_year\n",
    "2. i94mon: numeric month of the arrival -> imm_month\n",
    "3. i94citi&i94res: 3 digit code of origin city -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa: reason for immigration -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port: 3 character code of destination city --> Foreign key (used to map to USDemographics and City Temperature data) -> imm_port\n",
    "6. arrdate: arrival date of the departure -> imm_arrival_date:\n",
    "7. deptdate: departure date\n",
    "date_add\n",
    "7. i94mode: 1 digit travel code -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_datetime(days: DoubleType) -> datetime:\n",
    "    \"\"\"convert_to_datetime converts days to datetime format\n",
    "\n",
    "    Args:\n",
    "        days (DoubleType): from sas arrive or departure date\n",
    "\n",
    "    Returns:\n",
    "        datetime: added days to datetime format result.\n",
    "    \"\"\"\n",
    "    if days is not None:\n",
    "        date = datetime.strptime('1960-01-01', '%Y-%m-%d')\n",
    "\n",
    "        return date + timedelta(days=days)\n",
    "\n",
    "udf_convert_to_datetime = udf(lambda x: convert_to_datetime(x), DateType())\n",
    "\n",
    "immigration_main_information = df_imm_data.withColumn(\"imm_main_cic_id\", col(\"cicid\").cast(\"Integer\"))\\\n",
    "            .withColumn(\"imm_year\", col(\"i94yr\").cast(\"Integer\"))\\\n",
    "                .withColumn(\"imm_month\", col(\"i94mon\").cast(\"Integer\"))\\\n",
    "                    .withColumn(\"imm_cntyl\", col(\"i94cit\").cast(\"Integer\"))\\\n",
    "                        .withColumn(\"imm_visa\", col(\"i94visa\").cast(\"Integer\"))\\\n",
    "                            .withColumn(\"imm_port\", col(\"i94port\").cast(\"String\"))\\\n",
    "                                .withColumn(\"imm_arrival_date\", udf_convert_to_datetime(col(\"arrdate\")))\\\n",
    "                                    .withColumn(\"imm_departure_date\", udf_convert_to_datetime(col(\"depdate\")))\\\n",
    "                                        .withColumn(\"imm_model\", col(\"i94mode\").cast(\"Integer\"))\\\n",
    "                                            .withColumn(\"imm_address\", col(\"i94addr\").cast(\"String\"))\\\n",
    "                                                .withColumn(\"imm_airline\", col(\"airline\").cast(\"String\"))\\\n",
    "                                                    .withColumn(\"imm_flight_no\", col(\"fltno\").cast(\"String\"))\\\n",
    "        .select(col('imm_main_cic_id'), \\\n",
    "                    col('imm_year'),\\\n",
    "                        col('imm_month'),\\\n",
    "                            col('imm_cntyl'),\\\n",
    "                                col('imm_visa'),\\\n",
    "                                    col('imm_port'),\\\n",
    "                                        col('imm_arrival_date'),\\\n",
    "                                            col('imm_departure_date'),\\\n",
    "                                                col('imm_model'),\\\n",
    "                                                    col('imm_address'),\\\n",
    "                                                        col('imm_airline'),\\\n",
    "                                                            col('imm_flight_no'))\n",
    "\n",
    "df_immigration_main_information = immigration_main_information.createOrReplaceTempView(\n",
    "    \"immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information = spark.sql(\"SELECT * FROM immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information.persist()\n",
    "\n",
    "df_immigration_main_information.explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_data_read = SAS7BDAT(imm_data)\n",
    "\n",
    "# imm_data_read.header\n",
    "# for i in imm_data_read.columns:\n",
    "#     print(\"col_id \", i.col_id)\n",
    "#     print(\"  name\",  i.name.decode(encoding ='utf-8'))\n",
    "#     print(\"  label\", i.label.decode(encoding ='utf-8'))\n",
    "#     print(\"  format\", i.format)\n",
    "#     print(\"  type\", i.type)\n",
    "#     print(\"  length\", i.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with multiple data files\n",
    "# TODO: Make a def for doing this.\n",
    "# Method1: Using pandas to read file.\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat', iterator=True, chunksize=5000000)\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat')\n",
    "\n",
    "# imm_chunks_1 = list(pdf_immigration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.io.sas.sas7bdat import SAS7BDATReader\n",
    "\n",
    "# rdr = SAS7BDATReader(imm_data, convert_header_text=False)\n",
    "# df3 = rdr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Notification Table\n",
    "\"\"\"\n",
    "t2.imm_main_cic_id\n",
    "t2.imm_per_cic_id\n",
    "t2.news_cord_uid\n",
    "src.cidemo_id\n",
    "src.value_of_imm_destination_city\n",
    "t2.news_title\n",
    "t2.news_abstract\n",
    "t2.news_publish_time\n",
    "t2.news_authors\n",
    "\"\"\"\n",
    "\n",
    "#  ** t1: join imm two tables\n",
    "#  ** t2: join news table with t1\n",
    "#  ** t3: join us cities table with t2\n",
    "\n",
    "df_notification = spark.sql(\n",
    "    \"WITH t1 AS \\\n",
    "    (SELECT * \\\n",
    "       FROM immigration_main_information_data imid \\\n",
    "      LEFT JOIN imm_personal ip \\\n",
    "             ON imid.imm_main_cic_id = ip.imm_per_cic_id \\\n",
    "        WHERE imid.imm_year = 2016 \\\n",
    "     ), t2 AS \\\n",
    "        (SELECT * \\\n",
    "           FROM t1 \\\n",
    "         LEFT JOIN news_article_data nad \\\n",
    "                ON t1.imm_arrival_date = nad.news_publish_time \\\n",
    "     ) \\\n",
    "    SELECT t2.imm_main_cic_id \\\n",
    "           t2.imm_per_cic_id \\\n",
    "           t2.news_cord_uid \\\n",
    "           src.cidemo_id \\\n",
    "           src.value_of_imm_destination_city \\\n",
    "           t2.news_title \\\n",
    "           t2.news_abstract \\\n",
    "           t2.news_publish_time \\\n",
    "           t2.news_authors \\\n",
    "       FROM t2 \\\n",
    "     LEFT JOIN (SELECT * FROM us_cities_demographics_data ucdd INNER JOIN imm_destination_city_data idcd ON ucdd.cidemo_state_code = idcd.value_of_alias_imm_destination_city) src \\\n",
    "            ON t2.imm_port = src.code_of_imm_destination_city \\\n",
    "    \"\n",
    ")\n",
    "\n",
    "df_notification.show(n=5, truncate=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import configparser\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "\n",
    "\n",
    "# ******* Access AWS Server *******\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/.aws/credentials'))\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['default']['aws_access_key_id']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['default']['aws_secret_access_key']\n",
    "# **********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_CONN_ID = \"S3_default\"\n",
    "bucket_name = ''\n",
    "s3_prefix = '/data/usCitiesDemographics_data/usCitiesDemo.csv'\n",
    "DEST_BUCKET = 'mydatapool'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = S3Hook(AWS_S3_CONN_ID)\n",
    "# content = hook.list_keys(bucket_name=DEST_BUCKET)\n",
    "f = hook.get_key(key=s3_prefix, bucket_name=DEST_BUCKET)\n",
    "ff = f.get()['Body'].read().decode('utf-8')\n",
    "ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook.get_key(key=s3_prefix, bucket_name=DEST_BUCKET)\n",
    "file_c = hook.get()['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = AwsBaseHook(aws_conn_id='aws_conn', client_type='s3').get_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __secret_key__() -> str:\n",
    "    \"\"\"__secret_key__ _summary_\n",
    "\n",
    "        Purpose:\n",
    "            Building all access AWS server connection key and credentials.\n",
    "\n",
    "        Returns:\n",
    "            _type_: access key, secret key\n",
    "        \"\"\"\n",
    "\n",
    "    hook = AwsBaseHook(aws_conn_id='aws_conn',\n",
    "                       client_type='s3').get_credentials()\n",
    "\n",
    "    return hook.access_key, hook.secret_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,s = __secret_key__()\n",
    "print(a, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "hook = S3Hook(aws_conn_id='aws_conn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/job_flow/\"\n",
    "files = dict([os.path.join(root, file_), file_.split(\".\")[0]] for root, dirs, files in os.walk(filepath) for file_ in files)\n",
    "for k, v in files.items():\n",
    "    print(files[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_STEP_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "file_1 = dict([os.path.join(root, file_), file_.split(\".\")[0]] for root, dirs, files in os.walk(SPARK_STEP_FILE_PATH) for file_ in files if file_.endswith('.json'))\n",
    "file_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pprint import pprint\n",
    "pprint(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "import json\n",
    "AWS_CONN_ID = 'aws_conn'\n",
    "hook = S3Hook(aws_conn_id=AWS_CONN_ID)\n",
    "file_content = hook.get_key(\n",
    "    key='job_flow/job_flow_overrides.json', bucket_name='mydatapool')\n",
    "file_content = file_content.get()['Body'].read().decode('utf-8')\n",
    "json.loads(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "job_json = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/job_flow/job_flow_overrides.json\"\n",
    "f = open(job_json)\n",
    "data  = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_step = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/aws_emr_steps.json\"\n",
    "f = open(job_json)\n",
    "data = json.load(f)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_EMR_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "dict_etl_file_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    ETL_EMR_FILE_PATH) for file_ in files if file_.endswith('.py'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_etl_file_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "filepath = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data\"\n",
    "# ****** local data absolute path which is uploaded to S3 ******\n",
    "filepath_all = [os.path.join(root, file) for root,\n",
    "                dirs, files in os.walk(filepath) for file in files]\n",
    "\n",
    "files = [file_ for root, dirs, files in os.walk(filepath) for file_ in files]\n",
    "\n",
    "# s3 key where is saved upload of destination of aws s3 location\n",
    "s3_key_filename = [re.search(r'/data/*.*', each_filepath)[0]\n",
    "                   for each_filepath in filepath_all]\n",
    "\n",
    "each_file = [re.search(r'(^.+\\.)', files[i])[0] + str(i)\n",
    "             for i in range(len(files))]\n",
    "\n",
    "files_path = list(zip(each_file, s3_key_filename, filepath_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(files_path[0][0])  # file name + \".\" + index\n",
    "print(files_path[0][1])  # s3 key\n",
    "print(files_path[0][2])  # local absolute path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for i in files_path:\n",
    "    # print(i[0].split(\".\")[0])\n",
    "    # test = list(i[2], i[0].split('.')[0])\n",
    "    test = {i[2]: i[0].split('.')[0]}\n",
    "    print(type(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import XCom\n",
    "from airflow.utils.db import provide_session\n",
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "\n",
    "\n",
    "with DAG(dag_id=\"del_xcom\",\n",
    "         schedule_interval=None,\n",
    "         start_date=datetime.now(),) as dag:\n",
    "\n",
    "        @provide_session\n",
    "        def clean_xcom(session=None, **context):\n",
    "                dag = context[\"dag\"]\n",
    "                dag_id = dag._dag_id\n",
    "                session.query(XCom).filter(XCom.dag_id == dag_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dag'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 53'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000053?line=0'>1</a>\u001b[0m clean_xcom()\n",
      "File \u001b[0;32m/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py:70\u001b[0m, in \u001b[0;36mprovide_session.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py?line=67'>68</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py?line=68'>69</a>\u001b[0m     \u001b[39mwith\u001b[39;00m create_session() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> <a href='file:///usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/utils/session.py?line=69'>70</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, session\u001b[39m=\u001b[39;49msession, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 52'\u001b[0m in \u001b[0;36mclean_xcom\u001b[0;34m(session, **context)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=3'>4</a>\u001b[0m \u001b[39m@provide_session\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_xcom\u001b[39m(session\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcontext):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=5'>6</a>\u001b[0m         dag \u001b[39m=\u001b[39m context[\u001b[39m\"\u001b[39;49m\u001b[39mdag\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=6'>7</a>\u001b[0m         dag_id \u001b[39m=\u001b[39m dag\u001b[39m.\u001b[39m_dag_id\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000052?line=7'>8</a>\u001b[0m         session\u001b[39m.\u001b[39mquery(XCom)\u001b[39m.\u001b[39mfilter(XCom\u001b[39m.\u001b[39mdag_id \u001b[39m==\u001b[39m dag_id)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dag'"
     ]
    }
   ],
   "source": [
    "clean_xcom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data\"\n",
    "# ****** local data absolute path which is uploaded to S3 ******\n",
    "filepath_all = [os.path.join(root, file) for root,\n",
    "                dirs, files in os.walk(filepath) for file in files]\n",
    "\n",
    "# ****** Get the task name, s3_ky and file name from the file_path ******\n",
    "files = [file_ for root, dirs, files in os.walk(filepath) for file_ in files]\n",
    "\n",
    "# s3 key where is saved upload of destination of aws s3 location\n",
    "s3_key_filename = [re.search(r'/data/*.*', each_filepath)[0]\n",
    "                   for each_filepath in filepath_all]\n",
    "\n",
    "each_file = [re.search(r'(^.+\\.)', files[i])[0] + str(i)\n",
    "             for i in range(len(files))]\n",
    "\n",
    "files_path = list(zip(each_file, s3_key_filename, filepath_all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/usCitiesDemographics_data/usCitiesDemo.csv\n",
      "data/news_data/metadata.csv\n",
      "data/immigration_data/immigration_oct16_sub.sas7bdat\n",
      "data/immigration_data/immigration_jun16_sub.sas7bdat\n",
      "data/immigration_data/immigration_apr16_sub.sas7bdat\n",
      "data/immigration_data/immigration_aug16_sub.sas7bdat\n",
      "data/immigration_data/immigration_may16_sub.sas7bdat\n",
      "data/immigration_data/immigration_feb16_sub.sas7bdat\n",
      "data/immigration_data/immigration_jan16_sub.sas7bdat\n",
      "data/immigration_data/immigration_mar16_sub.sas7bdat\n",
      "data/immigration_data/immigration_jul16_sub.sas7bdat\n",
      "data/immigration_data/immigration_nov16_sub.sas7bdat\n",
      "data/immigration_data/immigration_sep16_sub.sas7bdat\n",
      "data/immigration_data/immigration_labels_descriptions.SAS\n",
      "data/immigration_data/immigration_dec16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "for each_filepath in filepath_all:\n",
    "    print(re.search(r'data/*.*', each_filepath)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_path = os.path.join(\n",
    "    \"mydatapool\", \"data/immigration_data/immigration_apr16_sub.sas7bdat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mydatapool/data/immigration_data/immigration_apr16_sub.sas7bdat'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ['s-2HJN029357WOV', 's-SS1ELLFXRPXF', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_STEPS = [\n",
    "    {\n",
    "        \"ActionOnFailure\": \"CONTINUE\",\n",
    "        \"HadoopJarStep\": {\n",
    "            \"Args\": [\n",
    "                \"s3-dist-cp\",\n",
    "                \"--src=s3://{{ var.value.Data_Bucket }}/upload_data/jars/spark-sas7bdat-3.0.0-s_2.12.jar\",\n",
    "                \"--dest=/usr/lib/spark/jars\"\n",
    "            ],\n",
    "            \"Jar\": \"command-runner.jar\"\n",
    "        },\n",
    "        \"Name\": \"Upload sas jars file from local to aws s3\"\n",
    "    },\n",
    "    {\n",
    "        \"ActionOnFailure\": \"CONTINUE\",\n",
    "        \"HadoopJarStep\": {\n",
    "            \"Args\": [\n",
    "                \"spark-submit\",\n",
    "                \"--master\",\n",
    "                \"yarn\",\n",
    "                \"--deploy-mode\",\n",
    "                \"cluster\",\n",
    "                \"--conf\",\n",
    "                \"spark.yarn.submit.waitAppCompletion=true\",\n",
    "                \"--name\",\n",
    "                \"data_spark_on_emr\",\n",
    "                \"s3://{{ var.value.Data_Bucket }}/upload_data/script/data_spark_on_emr.py\"\n",
    "            ],\n",
    "            \"Jar\": \"command-runner.jar\"\n",
    "        },\n",
    "        \"Name\": \"For Dealing with data and analytics using Spark on AWS EMR\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "5 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 61'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000060?line=0'>1</a>\u001b[0m [\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m]\u001b[39m.\u001b[39;49mindex(\u001b[39mlen\u001b[39;49m([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m])\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mValueError\u001b[0m: 5 is not in list"
     ]
    }
   ],
   "source": [
    "[1,2,3,3,1,2].index(len([1,2,3,3,1,2])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2916916465.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [19]\u001b[0;36m\u001b[0m\n\u001b[0;31m    {% if loop.index is divisibleby 3 %}\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "{% if loop.index is divisibleby 3 %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "# config.read_file(open('s3://mydatapool/config/dl.cfg'))\n",
    "config.read_file(\n",
    "    open('/Users/oneforall_nick/workspace/Udacity_capstone_project/dl.cfg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEST_S3_BUCKET = 's3://destetlbucket'\n",
    "imm_path = os.path.join(\n",
    "    DEST_S3_BUCKET, \"data/immigration_data/immigration_apr16_sub.sas7bdat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://destetlbucket/data/immigration_data/immigration_apr16_sub.sas7bdat'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project'\n",
    "dict_boostrap_emr_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    BOOTSTRAP_FILE_PATH) for file_ in files if file_.endswith('.sh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/bootstrap_emr.sh': 'bootstrap_emr'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_boostrap_emr_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/oneforall_nick/workspace/Udacity_capstone_project/bootstrap_emr.sh\n",
      "bootstrap_emr\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(BOOTSTRAP_FILE_PATH):\n",
    "    for file_ in files:\n",
    "        if file_.endswith('.sh'):\n",
    "            print(os.path.join(root, file_))\n",
    "            print(file_.split(\".\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg'\n",
    "dict_config_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    CONFIG_FILE_PATH) for file_ in files if file_.endswith('.cfg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg': 'dl'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_config_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOTSTRAP_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project'\n",
    "dict_boostrap_emr_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    BOOTSTRAP_FILE_PATH) for file_ in files if file_.endswith('.sh'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap_emr\n"
     ]
    }
   ],
   "source": [
    "for i in dict_boostrap_emr_info.items():\n",
    "    print(i[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg'\n",
    "dict_config_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    CONFIG_FILE_PATH) for file_ in files if file_.endswith('.cfg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg': 'dl'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_config_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ETL_EMR_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "dict_etl_file_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    ETL_EMR_FILE_PATH) for file_ in files if file_.endswith('.py'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/data_spark_on_emr.py': 'data_spark_on_emr'}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_etl_file_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *********\n",
    "ETL_EMR_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "dict_etl_file_info = dict([os.path.join(root, file_), file_.split(\".\")[0]]for root, dirs, files in os.walk(\n",
    "    ETL_EMR_FILE_PATH) for file_ in files if file_.endswith('.py'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/data_spark_on_emr.py': 'data_spark_on_emr'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_etl_file_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_spark_on_emr\n"
     ]
    }
   ],
   "source": [
    "for i in dict_etl_file_info.items():\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_spark_on_emr\n"
     ]
    }
   ],
   "source": [
    "for file_path, filename in dict_etl_file_info.items():\n",
    "    print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from signal import signal, SIGPIPE, SIG_DFL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Handlers.SIG_IGN: 1>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signal(SIGPIPE,SIG_DFL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "SAS_JARS_FILEPATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/jars/'\n",
    "dict_SAS_jars_info = dict([os.path.join(root, file_), file_.split(r\".jar\")[0]]for root, dirs, files in os.walk(\n",
    "    SAS_JARS_FILEPATH) for file_ in files if file_.endswith('.jar'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/jars/spark-sas7bdat-2.0.0-s_2.11.jar': 'spark-sas7bdat-2.0.0-s_2.11'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_SAS_jars_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 're'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 90'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000089?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m file_ \u001b[39min\u001b[39;00m files:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000089?line=2'>3</a>\u001b[0m     \u001b[39mif\u001b[39;00m file_\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.jar\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000089?line=3'>4</a>\u001b[0m         \u001b[39mprint\u001b[39m(file_\u001b[39m.\u001b[39;49mre(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m.]+$\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 're'"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(SAS_JARS_FILEPATH):\n",
    "    for file_ in files:\n",
    "        if file_.endswith('.jar'):\n",
    "            print(file_.re(r'[^\\.]+$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark-sas7bdat-2.0.0-s_2.11', '']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'spark-sas7bdat-2.0.0-s_2.11.jar'\n",
    "\n",
    "# re.compile(r'\\w.*\\.', re.IGNORECASE).search(string).group().split('jar')\n",
    "\n",
    "string.split('.jar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "# ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "# config.read_file(open('dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "DEST_BOOTSTRAP_BUCKET = '/destetlbucket/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.Session(\n",
    "    # service_name='s3',\n",
    "    # region_name='us-west-2',\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = session.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import Variable\n",
    "Data_Bucket = Variable.get('Data_Bucket')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.select_object_content(\n",
    "    Bucket=DEST_BOOTSTRAP_BUCKET,\n",
    "    Key='dimension_table/df_immigration_personal/imm_person_birth_year=1902/part-00006-4d920f82-bc2b-4f4a-bf17-cba974e4ba7a.c000.snappy.parquet',\n",
    "    Expression='SELECT COUNT(*) FROM S3Object',\n",
    "    ExpressionType='SQL',\n",
    "    InputSerialization={\n",
    "        'Parquet': {}\n",
    "    },\n",
    "    OutputSerialization={\n",
    "        'JSON': {\n",
    "            'RecordDelimiter': ','\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in resp['Payload']:\n",
    "    if 'Records' in e:\n",
    "        print(e['Records']['Payload'].e('utf-8'))\n",
    "    elif 'Stats' in e:\n",
    "        print(e['Stats'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = Path(DEST_BOOTSTRAP_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  s3path import S3Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_path = S3Path(DEST_BOOTSTRAP_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/destetlbucket/dimension_table\n",
      "/destetlbucket/fact_table\n"
     ]
    }
   ],
   "source": [
    "for path in bucket_path.iterdir():\n",
    "    if path.is_dir():\n",
    "        print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48f95af79faaae749105c8389524df19196568350894b01545e0b6f755f2644b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
