{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import boto3\n",
    "import awswrangler as wr\n",
    "from s3path import S3Path\n",
    "from pyspark.sql import SparkSession\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Source Data Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oneforall_nick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oneforall_nick/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/Users/oneforall_nick/spark-2.4.8-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c171e8b2-79ab-471a-b0fa-11aa50506d00;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;2.0.0-s_2.11 in spark-packages\n",
      "\tfound com.epam#parso;2.0.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.11;2.7 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.8 in central\n",
      ":: resolution report :: resolve 302ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.8 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.11;2.7 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.8 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;2.0.0-s_2.11 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   0   |   0   |   0   ||   5   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c171e8b2-79ab-471a-b0fa-11aa50506d00\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 5 already retrieved (0kB/6ms)\n",
      "22/06/24 11:20:46 WARN Utils: Your hostname, OneForAll-NickdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 172.20.10.6 instead (on interface en6)\n",
      "22/06/24 11:20:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "22/06/24 11:20:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Execute Spark Session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark_emr_udactity\") \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.20.10.6:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_emr_udactity</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7b81b2e990>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '51664'),\n",
       " ('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.app.name', 'spark_emr_udactity'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.driver.host', '172.20.10.6'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.id', 'local-1656040849238')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(\n",
    "    open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "# config.read_file(open('dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "# Access data from AWS S3\n",
    "# SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "SOURCE_S3_BUCKET = 's3://mydatapool'\n",
    "# Write data to AWS S3\n",
    "# DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n",
    "DEST_S3_BUCKET = 's3://destetlbucket'\n",
    "# *********************************************\n",
    "\n",
    "# ***** Local Testing configure ************\n",
    "# SOURCE_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/'\n",
    "# DEST_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/dest_data'\n",
    "\n",
    "# ***** Local Testing configure *****************\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")\n",
    "\n",
    "s3_access = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immigration Data: Extract apr16\n",
    "imm_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "\n",
    "imm_lable_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "# New Data\n",
    "news_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "# Us City Demographics Data\n",
    "us_city_dem_path = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immigration Data: Read and count\n",
    "df_imm_data = spark.read.format('com.github.saurfang.sas.spark').load(imm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_imm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'df_imm_data: apr16, count: 3096313'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"df_imm_data: apr16, count: {df_imm_data.count()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Immigration Data: Label\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")\n",
    "\n",
    "s3_access = session.resource('s3')\n",
    "\n",
    "\n",
    "s3_object = s3_access.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "\n",
    "text = s3_object['Body'].read()\n",
    "\n",
    "context = text.decode(encoding='utf-8')\n",
    "\n",
    "context = context.replace('\\t', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text data to dimension table\n",
    "def code_mapping(context, idx):\n",
    "        content_mapping = context[context.index(idx):]\n",
    "        content_line_split = content_mapping[:content_mapping.index(\n",
    "            ';')].split('\\n')\n",
    "        content_line_list = [line.replace(\"'\", \"\")\n",
    "                             for line in content_line_split]\n",
    "        content_two_dims = [i.strip().split('=')\n",
    "                            for i in content_line_list[1:]]\n",
    "        content_three_dims = [[i[0].strip(), i[1].strip().split(', ')[:][0], e]\n",
    "                              for i in content_two_dims if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "        return content_two_dims, content_three_dims\n",
    "\n",
    "# ***** imm_cit_res *****\n",
    "imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['582', 'MEXICO Air Sea', 'and Not Reported (I-94'],\n",
       " ['582', 'MEXICO Air Sea', 'no land arrivals)'],\n",
       " ['717', 'BONAIRE', 'ST EUSTATIUS'],\n",
       " ['717', 'BONAIRE', 'SABA'],\n",
       " ['245', 'CHINA', 'PRC'],\n",
       " ['473', 'MICRONESIA', 'FED. STATES OF'],\n",
       " ['471', 'INVALID: MARIANA ISLANDS', 'NORTHERN']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm_cit_res_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dimension Table: imm_cit_res_three, Count: 7'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Dimension Table: imm_cit_res_three, Count: {len(imm_cit_res_three)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Data\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=news_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_news)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'df_news: News, count: 45827'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"df_news: News, count: {df_news.count()}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Us City Demographics Data\n",
    "df_us_cities_demographics = spark.read.options(header=True, delimiter=';').csv(us_city_dem_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_us_cities_demographics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'df_news: Us Cities Demographics, count: 2891'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"df_news: Us Cities Demographics, count: {df_us_cities_demographics.count()}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check data is empty or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "dest_aws_s3_bucket = config[\"S3\"][\"DEST_S3_BUCKET\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_bucket_path = S3Path(f\"{dest_aws_s3_bucket}/dimension_table\")\n",
    "fact_bucket_path = S3Path(f\"{dest_aws_s3_bucket}/fact_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: df_immigration_personal, Count data objects: 114\n",
      "Data: imm_address, Count data objects: 9\n",
      "Data: imm_city_res_label, Count data objects: 9\n",
      "Data: imm_destination_city, Count data objects: 9\n",
      "Data: imm_travel_code, Count data objects: 6\n",
      "Data: imm_visa, Count data objects: 5\n",
      "Data: immigration_main_information, Count data objects: 1\n",
      "Data: news_article_data, Count data objects: 6305\n",
      "Data: us_cities_demographics_data, Count data objects: 301\n"
     ]
    }
   ],
   "source": [
    " # ******** Dimension Tables ********\n",
    "# Data Object Name: df_immigration_personal\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'df_immigration_personal']:\n",
    "    print(\n",
    "        f\"Data: df_immigration_personal, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "# Data Object Name: imm_address\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'imm_address']:\n",
    "    print(f\"Data: imm_address, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "\n",
    "# Data Object Name: imm_city_res_label\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'imm_city_res_label']:\n",
    "    print(f\"Data: imm_city_res_label, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "\n",
    "# Data Object Name: imm_destination_city\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'imm_destination_city']:\n",
    "    print(f\"Data: imm_destination_city, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "\n",
    "# Data Object Name: imm_travel_code\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'imm_travel_code']:\n",
    "    print(f\"Data: imm_travel_code, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "\n",
    "# Data Object Name: imm_visa\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'imm_visa']:\n",
    "    print(f\"Data: imm_visa, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "\n",
    "# Data Object Name: immigration_main_information\n",
    "if data_list := [str(path_1) for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir(\n",
    ") if str(path_1).split('/')[-2] == 'immigration_main_information' and path_1.is_dir()]:\n",
    "    print(\n",
    "        f\"Data: immigration_main_information, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "\n",
    "# Data Object Name: news_article_data\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'news_article_data']:\n",
    "    print(\n",
    "        f\"Data: news_article_data, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n",
    "\n",
    "\n",
    "# Data Object Name: us_cities_demographics_data\n",
    "if data_list := [str(path_1).split('/')[-1] for path in dim_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if str(path_1).split('/')[-2] in 'us_cities_demographics_data']:\n",
    "    print(\n",
    "        f\"Data: us_cities_demographics_data, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: notification, Count data objects: 2\n"
     ]
    }
   ],
   "source": [
    " # ******** Fact Tables ********\n",
    "# Data Object Name: notification\n",
    "if data_list := [str(path_1).split('/')[-1] for path in fact_bucket_path.iterdir() if path.is_dir() for path_1 in path.iterdir() if path_1.is_dir() if str(path_1).split('/')[-2] in 'notification']:\n",
    "    print(f\"Data: notification, Count data objects: {len(data_list)}\")\n",
    "else:\n",
    "    raise ValueError(\"This table does not contain data!!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Schema - Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 765 entries, 0 to 1\n",
      "Data columns (total 4 columns):\n",
      " #   Column                 Non-Null Count  Dtype   \n",
      "---  ------                 --------------  -----   \n",
      " 0   imm_per_cic_id         765 non-null    string  \n",
      " 1   imm_person_gender      651 non-null    string  \n",
      " 2   imm_visatype           765 non-null    string  \n",
      " 3   imm_person_birth_year  765 non-null    category\n",
      "dtypes: category(1), string(3)\n",
      "memory usage: 24.8 KB\n"
     ]
    }
   ],
   "source": [
    " # ***** df_immigration_personal column schema and data type *****\n",
    "partition_filter = lambda x: x[\"imm_person_birth_year\"] == \"2016\"\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/df_immigration_personal/\", dataset=True, partition_filter=partition_filter)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 55 entries, 0 to 6\n",
      "Data columns (total 2 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   code_of_imm_address   55 non-null     string\n",
      " 1   value_of_imm_address  55 non-null     string\n",
      "dtypes: string(2)\n",
      "memory usage: 1.3 KB\n"
     ]
    }
   ],
   "source": [
    " # ***** imm_address column schema and data type *****\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/imm_address/\", dataset=True)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7 entries, 0 to 0\n",
      "Data columns (total 3 columns):\n",
      " #   Column                            Non-Null Count  Dtype \n",
      "---  ------                            --------------  ----- \n",
      " 0   col_of_imm_cntyl                  7 non-null      Int32 \n",
      " 1   value_of_imm_cntyl                7 non-null      string\n",
      " 2   value_of_imm_cntyl_organizations  7 non-null      string\n",
      "dtypes: Int32(1), string(2)\n",
      "memory usage: 203.0 bytes\n"
     ]
    }
   ],
   "source": [
    " # ***** imm_city_res_label column schema and data type *****\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/imm_city_res_label/\", dataset=True)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 596 entries, 0 to 77\n",
      "Data columns (total 3 columns):\n",
      " #   Column                               Non-Null Count  Dtype \n",
      "---  ------                               --------------  ----- \n",
      " 0   code_of_imm_destination_city         596 non-null    string\n",
      " 1   value_of_imm_destination_city        596 non-null    string\n",
      " 2   value_of_alias_imm_destination_city  596 non-null    string\n",
      "dtypes: string(3)\n",
      "memory usage: 18.6 KB\n"
     ]
    }
   ],
   "source": [
    " # ***** imm_destination_city column schema and data type *****\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/imm_destination_city/\", dataset=True)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   code_of_imm_travel_code   0 non-null      Int32 \n",
      " 1   value_of_imm_travel_code  4 non-null      string\n",
      "dtypes: Int32(1), string(1)\n",
      "memory usage: 84.0 bytes\n"
     ]
    }
   ],
   "source": [
    " # ***** imm_travel_code column schema and data type *****\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/imm_travel_code/\", dataset=True)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3 entries, 0 to 0\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   code_of_imm_visa   3 non-null      Int32 \n",
      " 1   value_of_imm_visa  3 non-null      string\n",
      "dtypes: Int32(1), string(1)\n",
      "memory usage: 63.0 bytes\n"
     ]
    }
   ],
   "source": [
    " # ***** imm_visa column schema and data type *****\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/imm_visa/\", dataset=True)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3096313 entries, 0 to 10320\n",
      "Data columns (total 12 columns):\n",
      " #   Column              Dtype   \n",
      "---  ------              -----   \n",
      " 0   imm_main_cic_id     Int32   \n",
      " 1   imm_cntyl           Int32   \n",
      " 2   imm_visa            Int32   \n",
      " 3   imm_port            string  \n",
      " 4   imm_arrival_date    object  \n",
      " 5   imm_departure_date  object  \n",
      " 6   imm_model           Int32   \n",
      " 7   imm_address         string  \n",
      " 8   imm_airline         string  \n",
      " 9   imm_flight_no       string  \n",
      " 10  imm_year            category\n",
      " 11  imm_month           category\n",
      "dtypes: Int32(4), category(2), object(2), string(4)\n",
      "memory usage: 230.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# ***** immigration_main_information column schema and data type *****\n",
    "def partition_filter(x): return x[\"imm_year\"] == \"2016\"\n",
    "\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/immigration_main_information/\",\n",
    "                         dataset=True, partition_filter=partition_filter)\n",
    "dfs.info(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 41 entries, 0 to 0\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count  Dtype   \n",
      "---  ------             --------------  -----   \n",
      " 0   news_cord_uid      41 non-null     string  \n",
      " 1   news_source        41 non-null     string  \n",
      " 2   news_title         41 non-null     string  \n",
      " 3   news_licence       41 non-null     string  \n",
      " 4   news_abstract      36 non-null     string  \n",
      " 5   news_authors       41 non-null     string  \n",
      " 6   news_url           41 non-null     string  \n",
      " 7   news_publish_time  41 non-null     category\n",
      "dtypes: category(1), string(7)\n",
      "memory usage: 2.7 KB\n"
     ]
    }
   ],
   "source": [
    " # ***** news_article_data column schema and data type *****\n",
    "partition_filter = lambda x: \"2016-01-01\" <= x[\"news_publish_time\"] <= \"2016-01-02\"\n",
    "\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/news_article_data/\", dataset=True, partition_filter=partition_filter)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 2891 entries, 0 to 8\n",
      "Data columns (total 7 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   cidemo_city              2891 non-null   string \n",
      " 1   cidemo_state             2891 non-null   string \n",
      " 2   cidemo_median_age        2891 non-null   float32\n",
      " 3   cidemo_total_population  2891 non-null   Int32  \n",
      " 4   cidemo_state_code        2891 non-null   string \n",
      " 5   cidemo_count             2891 non-null   Int32  \n",
      " 6   cidemo_id                2891 non-null   Int64  \n",
      "dtypes: Int32(2), Int64(1), float32(1), string(3)\n",
      "memory usage: 155.3 KB\n"
     ]
    }
   ],
   "source": [
    " # ***** us_cities_demographics_data column schema and data type *****\n",
    "dfs = wr.s3.read_parquet(path=\"s3://destetlbucket/dimension_table/us_cities_demographics_data/\", dataset=True)\n",
    "dfs.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Schema - Fact Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9186 entries, 0 to 9185\n",
      "Data columns (total 33 columns):\n",
      " #   Column                               Non-Null Count  Dtype  \n",
      "---  ------                               --------------  -----  \n",
      " 0   imm_main_cic_id                      9186 non-null   Int32  \n",
      " 1   imm_year                             9186 non-null   Int32  \n",
      " 2   imm_month                            9186 non-null   Int32  \n",
      " 3   imm_cntyl                            9186 non-null   Int32  \n",
      " 4   imm_visa                             9186 non-null   Int32  \n",
      " 5   imm_port                             9186 non-null   string \n",
      " 6   imm_arrival_date                     9186 non-null   object \n",
      " 7   imm_departure_date                   8722 non-null   object \n",
      " 8   imm_model                            9186 non-null   Int32  \n",
      " 9   imm_address                          8662 non-null   string \n",
      " 10  imm_airline                          8891 non-null   string \n",
      " 11  imm_flight_no                        9046 non-null   string \n",
      " 12  imm_per_cic_id                       9186 non-null   string \n",
      " 13  imm_person_birth_year                9182 non-null   Int32  \n",
      " 14  imm_person_gender                    8389 non-null   string \n",
      " 15  imm_visatype                         9186 non-null   string \n",
      " 16  news_cord_uid                        9186 non-null   string \n",
      " 17  news_source                          9186 non-null   string \n",
      " 18  news_title                           9186 non-null   string \n",
      " 19  news_licence                         9186 non-null   string \n",
      " 20  news_abstract                        9186 non-null   string \n",
      " 21  news_authors                         9186 non-null   string \n",
      " 22  news_url                             9186 non-null   string \n",
      " 23  cidemo_city                          9186 non-null   string \n",
      " 24  cidemo_state                         9186 non-null   string \n",
      " 25  cidemo_median_age                    9186 non-null   float32\n",
      " 26  cidemo_total_population              9186 non-null   Int32  \n",
      " 27  cidemo_state_code                    9186 non-null   string \n",
      " 28  cidemo_count                         9186 non-null   Int32  \n",
      " 29  cidemo_id                            9186 non-null   Int64  \n",
      " 30  code_of_imm_destination_city         9186 non-null   string \n",
      " 31  value_of_imm_destination_city        9186 non-null   string \n",
      " 32  value_of_alias_imm_destination_city  9186 non-null   string \n",
      "dtypes: Int32(9), Int64(1), float32(1), object(2), string(20)\n",
      "memory usage: 2.1+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/23 13:46:36 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2289024 ms exceeds timeout 120000 ms\n",
      "22/06/23 13:46:37 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    " # ***** notification column schema and data type *****\n",
    "\n",
    "dfs = wr.s3.read_parquet(\n",
    "    path=\"s3://destetlbucket/fact_table/notification/news_publish_time=2016-04-02/part-00000-f532f766-0d4b-4fdd-b106-512af5269e4a.c000.snappy.parquet\")\n",
    "dfs.info(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f40c152a15a669d798eb1ad4e6f345bdd77350f6745bfc8751a72382d50440f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nick_udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
