{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "# from signal import signal, SIGPIPE, SIG_DFL\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, udf, to_date, count, lit, when\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               DateType,\n",
    "                               FloatType)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create local spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"spark_emr_udactity\") \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check spark session information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.104:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.8</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark_emr_udactity</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f84560c2c10>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.app.name', 'spark_emr_udactity'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', '192.168.1.104'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:2.0.0-s_2.11'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.app.id', 'local-1656206430658'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '52686'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-2.0.0-s_2.11.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.8.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.11-2.7.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.11.8.jar'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access AWS S3 to get my source data After I upload data from local to AWS S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ***** Access AWS Cloud configure ************\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/workspace/Udacity_capstone_project/cfg/dl.cfg'))\n",
    "# config.read_file(open('dl.cfg'))\n",
    "\n",
    "aws_access_key = config[\"ACCESS\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "aws_secret_access_key = config[\"ACCESS\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "aws_token = config[\"ACCESS\"][\"AWS_TOKEN\"]\n",
    "# Access data from AWS S3\n",
    "# SOURCE_S3_BUCKET = config['S3']['SOURCE_S3_BUCKET']\n",
    "SOURCE_S3_BUCKET = 's3://mydatapool'\n",
    "# Write data to AWS S3\n",
    "# DEST_S3_BUCKET = config['S3']['DEST_S3_BUCKET']\n",
    "DEST_S3_BUCKET = 's3://destetlbucket'\n",
    "# *********************************************\n",
    "\n",
    "# ***** Local Testing configure ************\n",
    "# SOURCE_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/'\n",
    "# DEST_S3_BUCKET = '/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/dest_data'\n",
    "\n",
    "# ***** Local Testing configure *****************\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_access_key,\n",
    "    aws_session_token=aws_token\n",
    ")\n",
    "\n",
    "s3_access = session.resource('s3')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Label Data\n",
    "*Data format: <br>*\n",
    "    TXT\n",
    "*This step will seperate multiple tables:*\n",
    "- imm_cit_res\n",
    "- imm_port\n",
    "- imm_mod\n",
    "- imm_addr\n",
    "- imm_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------------+\n",
      "|col_of_imm_cntyl|value_of_imm_cntyl|\n",
      "+----------------+------------------+\n",
      "|             471|          NORTHERN|\n",
      "|             473|    FED. STATES OF|\n",
      "|             245|               PRC|\n",
      "|             582| no land arrivals)|\n",
      "|             717|              SABA|\n",
      "+----------------+------------------+\n",
      "\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "|code_of_imm_destination_city|value_of_imm_destination_city|value_of_alias_imm_destination_city|\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "|                         AGA|                        AGANA|                                 GU|\n",
      "|                         ALC|                        ALCAN|                                 AK|\n",
      "|                         ANZ|                    ANZALDUAS|                                 TX|\n",
      "|                         BAL|                    BALTIMORE|                                 MD|\n",
      "|                         BCK|                     BUCKPORT|                                 ME|\n",
      "|                         BED|         HANSCOM FIELD - B...|                                 MA|\n",
      "|                         BGM|                       BANGOR|                                 ME|\n",
      "|                         BLA|                       BLAINE|                                 WA|\n",
      "|                         BQN|         BORINQUEN - AGUAD...|                                 PR|\n",
      "|                         BTM|                        BUTTE|                                 MT|\n",
      "|                         BTN|                  BATON ROUGE|                                 LA|\n",
      "|                         CAN|               CAPE CANAVERAL|                                 FL|\n",
      "|                         CAO|                        CAMPO|                                 CA|\n",
      "|                         CHM|                    CHAMPLAIN|                                 NY|\n",
      "|                         CLG|                      CALGARY|                             CANADA|\n",
      "|                         CLT|                    CHARLOTTE|                                 NC|\n",
      "|                         CNC|               CANNON CORNERS|                                 NY|\n",
      "|                         CRQ|                    CARAVELAS|                           BA #ARPT|\n",
      "|                         CUR|                       CURLEW|                           WA (BPS)|\n",
      "|                         DEN|                       DENVER|                                 CO|\n",
      "+----------------------------+-----------------------------+-----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------------------+------------------------+\n",
      "|code_of_imm_travel_code|value_of_imm_travel_code|\n",
      "+-----------------------+------------------------+\n",
      "|                    1.0|                     Air|\n",
      "|                    2.0|                     Sea|\n",
      "|                    9.0|            Not reported|\n",
      "|                    3.0|                    Land|\n",
      "+-----------------------+------------------------+\n",
      "\n",
      "+-------------------+--------------------+\n",
      "|code_of_imm_address|value_of_imm_address|\n",
      "+-------------------+--------------------+\n",
      "|                 AL|             ALABAMA|\n",
      "|                 CA|          CALIFORNIA|\n",
      "|                 CO|            COLORADO|\n",
      "|                 DE|            DELAWARE|\n",
      "|                 FL|             FLORIDA|\n",
      "|                 GA|             GEORGIA|\n",
      "|                 IA|                IOWA|\n",
      "|                 ID|               IDAHO|\n",
      "|                 IL|            ILLINOIS|\n",
      "|                 MN|           MINNESOTA|\n",
      "|                 NC|         N. CAROLINA|\n",
      "|                 NM|          NEW MEXICO|\n",
      "|                 NV|              NEVADA|\n",
      "|                 NY|            NEW YORK|\n",
      "|                 OK|            OKLAHOMA|\n",
      "|                 OR|              OREGON|\n",
      "|                 PA|        PENNSYLVANIA|\n",
      "|                 UT|                UTAH|\n",
      "|                 CT|         CONNECTICUT|\n",
      "|                 DC|   DIST. OF COLUMBIA|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+----------------+-----------------+\n",
      "|code_of_imm_visa|value_of_imm_visa|\n",
      "+----------------+-----------------+\n",
      "|               3|          Student|\n",
      "|               1|         Business|\n",
      "|               2|         Pleasure|\n",
      "+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ****** immigration_labels_descriptions ******\n",
    "\n",
    "# Get AWS S3 data Object: immigration_labels_descriptions.SAS\n",
    "s3_object = s3_access.Bucket('mydatapool').Object('data/immigration_data/immigration_labels_descriptions.SAS').get()\n",
    "text = s3_object['Body'].read()\n",
    "context = text.decode(encoding ='utf-8')\n",
    "# for obj in s3_object.objects.all():\n",
    "#     print(obj.key)\n",
    "\n",
    "context = context.replace('\\t', '')\n",
    "\n",
    "\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(\n",
    "        ';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\")\n",
    "                         for line in content_line_split]\n",
    "    content_two_dims = [i.strip().split('=') for i in content_line_list[1:]]\n",
    "    content_three_dims = [[i[0].strip(), i[1].strip().split(', ')[:][0], e]\n",
    "                          for i in content_two_dims if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_two_dims, content_three_dims\n",
    "\n",
    "imm_cit_res_two, imm_cit_res_three = code_mapping(context, \"i94cntyl\")\n",
    "df_imm_city_res_label = spark.sparkContext.parallelize(imm_cit_res_three).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\", \"value_of_imm_cntyl_organizations\"]) \\\n",
    "    .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl_organizations\").cast(\"String\")) \\\n",
    "    .select('col_of_imm_cntyl', 'value_of_imm_cntyl').distinct().dropDuplicates(['col_of_imm_cntyl'])\n",
    "\n",
    "df_imm_city_res_label.show()\n",
    "\n",
    "imm_port_two, imm_port_three = code_mapping(context, \"i94prtl\")\n",
    "df_imm_destination_city = spark.sparkContext.parallelize(imm_port_three).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"]) \\\n",
    "                                                .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "                                                .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "                                                .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_destination_city'])\n",
    "\n",
    "df_imm_destination_city.show()\n",
    "\n",
    "\n",
    "imm_mode_two, imm_mode_three = code_mapping(context, \"i94model\")\n",
    "df_imm_travel_code = spark.sparkContext.parallelize(imm_mode_two).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"]) \\\n",
    "                                           .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Float\")) \\\n",
    "                                           .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_travel_code'])\n",
    "df_imm_travel_code.show()\n",
    "\n",
    "imm_addr_two, imm_addr_three = code_mapping(context, \"i94addrl\")\n",
    "df_imm_address = spark.sparkContext.parallelize(imm_addr_two).toDF([\"code_of_imm_address\", \"value_of_imm_address\"]) \\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_address'])\n",
    "\n",
    "df_imm_address.show()\n",
    "\n",
    "imm_visa = {'1': 'Business',\n",
    "            '2': 'Pleasure',\n",
    "            '3': 'Student'}\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(imm_visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"]) \\\n",
    "                                    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\")).distinct().dropDuplicates(['code_of_imm_visa'])\n",
    "df_imm_visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim table: df_imm_city_res_label, Count: 5\n",
      "dim table: df_imm_destination_city, Count: 582\n",
      "dim table: df_imm_travel_code, Count: 4\n",
      "dim table: df_imm_address, Count: 55\n",
      "dim table: df_imm_visa, Count: 3\n"
     ]
    }
   ],
   "source": [
    "# Source Data Count:\n",
    "print(f\"dim table: df_imm_city_res_label, Count: {df_imm_city_res_label.count():,}\")\n",
    "\n",
    "# TODO:OK: three columns\n",
    "print(\n",
    "    f\"dim table: df_imm_destination_city, Count: {df_imm_destination_city.count():,}\")\n",
    "\n",
    "# TODO:OK: two columns\n",
    "print(\n",
    "    f\"dim table: df_imm_travel_code, Count: {df_imm_travel_code.count():,}\")\n",
    "\n",
    "# TODO:OK: two columns\n",
    "print(f\"dim table: df_imm_address, Count: {df_imm_address.count():,}\")\n",
    "\n",
    "# TODO:OK: two columns\n",
    "print(f\"dim table: df_imm_visa, Count: {df_imm_visa.count():,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: News\n",
    "- Data format: <br>\n",
    "    CSV\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [news_cord_uid#10480, news_source#10499, news_title#10519, news_licence#10540, news_abstract#10562, news_publish_time#10585, news_authors#10609, news_url#10634]\n",
      "   +- InMemoryRelation [news_cord_uid#10480, news_source#10499, news_title#10519, news_licence#10540, news_abstract#10562, news_publish_time#10585, news_authors#10609, news_url#10634], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- SortAggregate(key=[news_cord_uid#6406], functions=[first(news_source#6425, false), first(news_title#6445, false), first(news_licence#6466, false), first(news_abstract#6488, false), first(news_publish_time#6511, false), first(news_authors#6535, false), first(news_url#6560, false)])\n",
      "            +- *(3) Sort [news_cord_uid#6406 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(news_cord_uid#6406, 5)\n",
      "                  +- SortAggregate(key=[news_cord_uid#6406], functions=[partial_first(news_source#6425, false), partial_first(news_title#6445, false), partial_first(news_licence#6466, false), partial_first(news_abstract#6488, false), partial_first(news_publish_time#6511, false), partial_first(news_authors#6535, false), partial_first(news_url#6560, false)])\n",
      "                     +- *(2) Sort [news_cord_uid#6406 ASC NULLS FIRST], false, 0\n",
      "                        +- *(2) HashAggregate(keys=[news_source#6425, news_authors#6535, news_url#6560, news_publish_time#6511, news_cord_uid#6406, news_licence#6466, news_abstract#6488, news_title#6445], functions=[])\n",
      "                           +- Exchange hashpartitioning(news_source#6425, news_authors#6535, news_url#6560, news_publish_time#6511, news_cord_uid#6406, news_licence#6466, news_abstract#6488, news_title#6445, 5)\n",
      "                              +- *(1) HashAggregate(keys=[news_source#6425, news_authors#6535, news_url#6560, news_publish_time#6511, news_cord_uid#6406, news_licence#6466, news_abstract#6488, news_title#6445], functions=[])\n",
      "                                 +- *(1) Project [cord_uid#6372 AS news_cord_uid#6406, source_x#6374 AS news_source#6425, title#6375 AS news_title#6445, license#6379 AS news_licence#6466, abstract#6380 AS news_abstract#6488, cast(cast(unix_timestamp(publish_time#6381, yyyy-MM-dd, Some(Asia/Taipei)) as timestamp) as date) AS news_publish_time#6511, authors#6382 AS news_authors#6535, url#6388 AS news_url#6560]\n",
      "                                    +- *(1) FileScan csv [cord_uid#6372,source_x#6374,title#6375,license#6379,abstract#6380,publish_time#6381,authors#6382,url#6388] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<cord_uid:string,source_x:string,title:string,license:string,abstract:string,publish_time:s...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/27 13:45:07 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    }
   ],
   "source": [
    "# file path: data >> news_article\n",
    "\"\"\"Table: news_article schema\n",
    "pk: cord_uid -> news_cord_uid\n",
    "1. source_x -> news_source\n",
    "    schema: StringType()\n",
    "2. title -> news_title\n",
    "    schema: StringType()\n",
    "3. license -> news_licence\n",
    "    schema: StringType()\n",
    "4. abstract -> news_abstract\n",
    "    schema: StringType()\n",
    "5. publish_time -> news_publish_time (fk)\n",
    "    schema: TimestampType()\n",
    "6. authors -> news_authors\n",
    "    schema: StringType()\n",
    "7. url -> news_url\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "data_news = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=data_news)\n",
    "\n",
    "df_news = df_news.withColumn(\"news_cord_uid\", col(\"cord_uid\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_source\", col(\"source_x\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_title\", col(\"title\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_licence\", col(\"license\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_abstract\", col(\"abstract\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_publish_time\", to_date(col(\"publish_time\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"news_authors\", col(\"authors\").cast(\"String\")) \\\n",
    "    .withColumn(\"news_url\", col(\"url\").cast(\"String\")) \\\n",
    "    .select(col(\"news_cord_uid\"),\n",
    "            col(\"news_source\"),\n",
    "            col(\"news_title\"),\n",
    "            col(\"news_licence\"),\n",
    "            col(\"news_abstract\"),\n",
    "            col(\"news_publish_time\"),\n",
    "            col(\"news_authors\"),\n",
    "            col(\"news_url\")).distinct().dropDuplicates(['news_cord_uid'])\n",
    "\n",
    "\n",
    "df_news_tmp = df_news.createOrReplaceTempView(\"news_article_data\")\n",
    "\n",
    "df_news_tmp = spark.sql(\"SELECT * FROM news_article_data\")\n",
    "\n",
    "df_news_tmp.persist()\n",
    "\n",
    "df_news_tmp.explain()\n",
    "\n",
    "# df_news_tmp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 539:=======>                                                 (1 + 7) / 8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "|news_cord_uid|news_source|news_title|news_licence|news_abstract|news_publish_time|news_authors|  news_url|\n",
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "|    A.; Ke...|       null|      null|  https:/...|         null|             null|        null|      null|\n",
      "|     000tfenb|        PMC|Prevale...|       cc-by|   BACKGRO...|       2017-11-22|  Liu, Pe...|https:/...|\n",
      "|     00bjha6b|        PMC|Cystiti...|         unk|   Female ...|       2009-11-27|  Munday,...|https:/...|\n",
      "+-------------+-----------+----------+------------+-------------+-----------------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_news.show(n=3, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 851:>                                                        (0 + 5) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+--------------------+------------+--------------------+-----------------+--------------------+--------------------+-------------------+\n",
      "|news_cord_uid|news_source|          news_title|news_licence|       news_abstract|news_publish_time|        news_authors|            news_url| label_covid_for_ml|\n",
      "+-------------+-----------+--------------------+------------+--------------------+-----------------+--------------------+--------------------+-------------------+\n",
      "|     bpfarowa|   Elsevier|Chapter 2 Anatomi...|   els-covid|Abstract Mucosal ...|       2020-12-31|Silva-Sanchez, Aa...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     ytk5s6m8|   Elsevier|Chapter 7 Coronav...|   els-covid|Abstract The coro...|       2020-12-31|Kasmi, Yassine; K...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     cydvmm4r|   Elsevier|Chapter 20 Design...|   els-covid|Abstract Once the...|       2020-12-31|Slezak, Tom; Hart...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     1mcgocvb|   Elsevier|20 Infection Prev...|   els-covid|Abstract Tropical...|       2020-12-31|Shakoor, Sadia; W...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     dde4nlhh|   Elsevier|Diseases, Emergin...|   els-covid|Abstract Emerging...|       2020-12-31|Antabe, Roger; Zi...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     hig24p9x|   Elsevier|Chapter 19 Nanoma...|   els-covid|Abstract As the n...|       2020-12-31|        Ojha, Ankita|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     j05b0llz|   Elsevier|Chapter 28 Select...|   els-covid|Abstract Several ...|       2020-12-31|Morse, Stephen A....|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     alm3p31f|   Elsevier|Chapter 3 Immunob...|   els-covid|Abstract Immuniza...|       2020-12-31|Chu, Helen Y.; Ma...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     kzdp9edn|   Elsevier|Chapter 8 Quasisp...|   els-covid|Abstract Medical ...|       2020-12-31|    Domingo, Esteban|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     8sklix0x|   Elsevier|Chapter 12 Next g...|   els-covid|Abstract Biologic...|       2020-12-31|Sharma, Anshula; ...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     l7dehit8|   Elsevier|Chapter 7 Nanopar...|   els-covid|Abstract Infectio...|       2020-12-31|Diaz-Arévalo, Dia...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     37l48ch4|   Elsevier|Chapter 13 Viral ...|   els-covid|Abstract This cha...|       2020-12-31|Macy, James D.; C...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     lytujkq4|   Elsevier| Interdisciplinarity|   els-covid|Abstract Interdis...|       2020-12-31|    Raento, Pauliina|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     e6ooqmga|   Elsevier|Chapter 9 Trends ...|   els-covid|Abstract Viral po...|       2020-12-31|    Domingo, Esteban|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     r5va3y1n|   Elsevier|Chapter 48 Mucosa...|   els-covid|Abstract Because ...|       2020-12-31|Wilson, Heather L...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     eifz7dsc|   Elsevier|Chapter 17 Antimi...|   els-covid|Abstract Marine e...|       2020-12-31|Kurhekar, Jaya Vikas|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     ru8xqqj4|   Elsevier|Chapter 20 The po...|   els-covid|Abstract Exosomes...|       2020-12-31|       Kang, Ju-Seop|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     7rmzaau4|   Elsevier|Chapter 19 Curren...|   els-covid|Abstract Mucosal ...|       2020-12-31|    Rhee, Joon Haeng|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     sn7rswab|   Elsevier|Chapter 8 The Mid...|   els-covid|Abstract Middle E...|       2020-12-31|Khan, Gulfaraz; S...|https://doi.org/1...|Covid_News_Abstract|\n",
      "|     5165078t|   Elsevier|Globalization of ...|   els-covid|Abstract Fueled b...|       2020-12-31|   Oppong, Joseph R.|https://doi.org/1...|Covid_News_Abstract|\n",
      "+-------------+-----------+--------------------+------------+--------------------+-----------------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df_news.filter('news_title rlike \"covid\"').select(col('news_abstract')).dropna().show(truncate=False)\n",
    "\n",
    "# df_news.filter((col('news_title').rlike(\"covid\"))).show(truncate=False)\n",
    "\n",
    "# df_news.withColumn(\"label_covid_for_ml\", when(((df_news.news_title.rlike(\"covid\")) | (df_news.news_abstract.isNull()), \"Covid_News_Abstract_Null\"))).show()\n",
    "\n",
    "# TODO: How to use tables: Point 1\n",
    "# 1. Filter news articles title and abstract\n",
    "# 2. Label news title and abstract related articles that were matched covid word for user to split news_abstract words and language detect.\n",
    "df_news.withColumn(\"label_covid_for_ml\", when( (df_news.news_title.rlike(\"covid\")) | (df_news.news_abstract.isNull()), \"Covid_News_Abstract_Null\")\\\n",
    "                   .when((df_news.news_title.rlike(\"covid\")) | (df_news.news_abstract.isNotNull()), \"Covid_News_Abstract\")\n",
    "                                        .otherwise(\"Other News\")).filter(col('news_publish_time').isNotNull()).sort(col('news_publish_time').desc()).show()\n",
    "\n",
    "# TODO: How to use tables: Point 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- news_cord_uid: string (nullable = true)\n",
      " |-- news_source: string (nullable = true)\n",
      " |-- news_title: string (nullable = true)\n",
      " |-- news_licence: string (nullable = true)\n",
      " |-- news_abstract: string (nullable = true)\n",
      " |-- news_publish_time: date (nullable = true)\n",
      " |-- news_authors: string (nullable = true)\n",
      " |-- news_url: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_news.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45805"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"df_news source count: {df_news.count():,}\" # drop duplicates column: uid not yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|news_cord_uid|count(news_cord_uid)|\n",
      "+-------------+--------------------+\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select news_cord_uid, count(news_cord_uid) from news_article_data group by news_cord_uid having count(news_cord_uid) > 1\").show(\n",
    "    truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|news_cord_uid|check_duplicated_IDs|\n",
      "+-------------+--------------------+\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Duplicate data record\n",
    "uuid = 'news_cord_uid'\n",
    "df_news.groupBy(col(uuid)).agg(count(uuid).alias('check_duplicated_IDs')).filter(col('check_duplicated_IDs') > 1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table: df_news has no duplicated IDs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if df_news.groupBy(col(uuid)).agg(count(uuid).alias('check_duplicated_IDs')).filter(col('check_duplicated_IDs') > 1).count() == 0:\n",
    "    print(\"The table: df_news has no duplicated IDs\")\n",
    "else:\n",
    "    raise Exception(\n",
    "        f\"The table has duplicated IDs please check it: {df_news[uuid]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Us Cities Demographics data\n",
    "- Data format: <br>\n",
    "    CSV\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [cidemo_city#10982, cidemo_state#10997, cidemo_median_age#11013, cidemo_total_population#11067, cidemo_state_code#11153, cidemo_count#11202, cidemo_id#11234L]\n",
      "   +- InMemoryRelation [cidemo_city#10982, cidemo_state#10997, cidemo_median_age#11013, cidemo_total_population#11067, cidemo_state_code#11153, cidemo_count#11202, cidemo_id#11234L], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- *(2) HashAggregate(keys=[cidemo_state_code#11153, cidemo_total_population#11067, cidemo_state#10997, cidemo_city#10982, cidemo_count#11202, cidemo_median_age#11013], functions=[])\n",
      "            +- *(2) HashAggregate(keys=[cidemo_state_code#11153, cidemo_total_population#11067, cidemo_state#10997, cidemo_city#10982, cidemo_count#11202, cidemo_median_age#11013], functions=[])\n",
      "               +- *(2) HashAggregate(keys=[cidemo_state_code#11153, cidemo_total_population#11067, cidemo_state#10997, cidemo_city#10982, cidemo_count#11202, cidemo_median_age#11013], functions=[])\n",
      "                  +- Exchange hashpartitioning(cidemo_state_code#11153, cidemo_total_population#11067, cidemo_state#10997, cidemo_city#10982, cidemo_count#11202, cidemo_median_age#11013, 5)\n",
      "                     +- *(1) HashAggregate(keys=[cidemo_state_code#11153, cidemo_total_population#11067, cidemo_state#10997, cidemo_city#10982, cidemo_count#11202, cidemo_median_age#11013], functions=[])\n",
      "                        +- *(1) Project [City#10944 AS cidemo_city#10982, State#10945 AS cidemo_state#10997, cast(Median Age#10946 as float) AS cidemo_median_age#11013, cast(Total Population#10949 as int) AS cidemo_total_population#11067, State Code#10953 AS cidemo_state_code#11153, cast(Count#10955 as int) AS cidemo_count#11202]\n",
      "                           +- *(1) FileScan csv [City#10944,State#10945,Median Age#10946,Total Population#10949,State Code#10953,Count#10955] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCi..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<City:string,State:string,Median Age:string,Total Population:string,State Code:string,Count...\n"
     ]
    }
   ],
   "source": [
    "# Create a us-cities data dimension table\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "pk: generated -> cidemo_id\n",
    "    schema: IntegerType()\n",
    "1. City -> cidemo_city\n",
    "    schema: StringType()\n",
    "2. State -> cidemo_state\n",
    "    schema: StringType()\n",
    "3. Median Age -> cidemo_median_age\n",
    "    schema: FloatType()\n",
    "4. Total Population -> cidemo_total_population\n",
    "    schema: IntegerType()\n",
    "5. State Code -> cidemo_state_code (fk)\n",
    "    schema: StringType()\n",
    "6. Count -> cidemo_count\n",
    "    schema: IntegerType()\n",
    "\"\"\"\n",
    "\n",
    "data_us_cities_demographics = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\"\n",
    "\n",
    "# -> Must be defined a function that generated each table schema:\n",
    "us_cities_demographics_data_schema = StructType([\n",
    "    StructField(name=\"cidemo_city\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_median_age\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cidemo_total_population\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state_code\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_count\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using pyspark to read csv file\n",
    "df_us_cities_demographics = spark.read.options(header=True, delimiter=';').csv(data_us_cities_demographics)\n",
    "\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_id\", monotonically_increasing_id()) \\\n",
    "                .withColumn(\"cidemo_city\", col(\"City\").cast(\"String\")) \\\n",
    "                    .withColumn(\"cidemo_state\", col(\"State\").cast(\"String\")) \\\n",
    "                        .withColumn(\"cidemo_median_age\", col(\"Median Age\").cast(\"Float\")) \\\n",
    "                            .withColumn(\"cidemo_male_population\", col(\"Male Population\").cast(\"Integer\")) \\\n",
    "                                .withColumn(\"cidemo_female_population\", col(\"Female Population\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"cidemo_total_population\", col(\"Total Population\").cast(\"Integer\")) \\\n",
    "                                            .withColumn(\"cidemo_number_of_veterans\", col(\"Number of Veterans\").cast(\"Integer\")) \\\n",
    "                                                .withColumn(\"cidemo_foreign_born\", col(\"Foreign-born\").cast(\"Integer\")) \\\n",
    "                                                    .withColumn(\"cidemo_average_household_size\", col(\"Average Household Size\").cast(\"Float\")) \\\n",
    "                                                        .withColumn(\"cidemo_state_code\", col(\"State Code\").cast(\"String\")) \\\n",
    "                                                            .withColumn(\"cidemo_race\", col(\"Race\").cast(\"String\")) \\\n",
    "    .withColumn(\"cidemo_count\", col(\"Count\").cast(\"Integer\")) \\\n",
    "                    .select(col(\"cidemo_city\"),\n",
    "                            col(\"cidemo_state\"),\n",
    "                            col(\"cidemo_median_age\"),\n",
    "                            col(\"cidemo_total_population\"),\n",
    "                            col(\"cidemo_state_code\"),\n",
    "                            col(\"cidemo_count\")).distinct().dropDuplicates()\n",
    "\n",
    "# Auto-generated series of id\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_id\", monotonically_increasing_id())\n",
    "\n",
    "df_us_cities_demographics_temp = df_us_cities_demographics.createOrReplaceTempView(\"us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp = spark.sql(\"SELECT * FROM us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp.persist()\n",
    "\n",
    "df_us_cities_demographics_temp.explain()\n",
    "\n",
    "# df_us_cities_demographics_temp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "|cidemo_city|  cidemo_state|cidemo_median_age|cidemo_total_population|cidemo_state_code|cidemo_count|cidemo_id|\n",
      "+-----------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "| High Point|North Carolina|             35.5|                 109828|               NC|       11060|        0|\n",
      "|     Folsom|    California|             40.9|                  76368|               CA|         998|        1|\n",
      "| Fort Myers|       Florida|             37.3|                  74015|               FL|       50169|        2|\n",
      "|Santa Clara|    California|             35.2|                 126216|               CA|       55847|        3|\n",
      "|Bolingbrook|      Illinois|             33.7|                  72096|               IL|         323|        4|\n",
      "+-----------+--------------+-----------------+-----------------------+-----------------+------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_cities_demographics_temp.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cidemo_city: string (nullable = true)\n",
      " |-- cidemo_state: string (nullable = true)\n",
      " |-- cidemo_median_age: float (nullable = true)\n",
      " |-- cidemo_total_population: integer (nullable = true)\n",
      " |-- cidemo_state_code: string (nullable = true)\n",
      " |-- cidemo_count: integer (nullable = true)\n",
      " |-- cidemo_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_cities_demographics_temp.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2,891'"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{df_us_cities_demographics_temp.count():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|cidemo_id|\n",
      "+---------+\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select cidemo_id from us_cities_demographics_data group by cidemo_id having count(cidemo_id) > 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 594:================================>                       (8 + 6) / 14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94mode|\n",
      "+-------+\n",
      "|   null|\n",
      "|    9.0|\n",
      "|    1.0|\n",
      "|    2.0|\n",
      "|    3.0|\n",
      "+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ****** imm_data ******\n",
    "imm_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "df_imm_data = spark.read.format('com.github.saurfang.sas.spark').load(imm_data)\n",
    "\n",
    "df_imm_data.select(col('i94mode')).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3,096,313'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{df_imm_data.count():,}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_imm_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration personal data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "# Dimension Table: Immigration personal data\n",
    "df_immigration_personal = df_imm_data.withColumn(\"imm_per_cic_id\", col(\"cicid\").cast(\"String\")) \\\n",
    "    .withColumn(\"imm_person_birth_year\", col(\"biryear\").cast(\"Integer\")) \\\n",
    "    .withColumn(\"imm_person_gender\", col(\"gender\").cast(\"String\")) \\\n",
    "    .withColumn(\"imm_visatype\", col(\"visatype\").cast(\"String\")) \\\n",
    "    .select(col(\"imm_per_cic_id\"),\n",
    "            col(\"imm_person_birth_year\"),\n",
    "            col(\"imm_person_gender\"),\n",
    "            col(\"imm_visatype\")).distinct().dropDuplicates(['imm_per_cic_id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3,096,313'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{df_immigration_personal.count():,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension: Immigration main data\n",
    "- Data format: <br>\n",
    "    SAS\n",
    "- explain: <br>\n",
    "    display data persist in local memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "InMemoryTableScan [imm_main_cic_id#11808, imm_year#11838, imm_month#11869, imm_cntyl#11901, imm_visa#11934, imm_port#11968, imm_arrival_date#12004, imm_departure_date#12041, imm_model#12078, imm_address#12116, imm_airline#12155, imm_flight_no#12195]\n",
      "   +- InMemoryRelation [imm_main_cic_id#11808, imm_year#11838, imm_month#11869, imm_cntyl#11901, imm_visa#11934, imm_port#11968, imm_arrival_date#12004, imm_departure_date#12041, imm_model#12078, imm_address#12116, imm_airline#12155, imm_flight_no#12195], StorageLevel(disk, memory, 1 replicas)\n",
      "         +- SortAggregate(key=[imm_main_cic_id#11808], functions=[first(imm_year#11838, false), first(imm_month#11869, false), first(imm_cntyl#11901, false), first(imm_visa#11934, false), first(imm_port#11968, false), first(imm_arrival_date#12004, false), first(imm_departure_date#12041, false), first(imm_model#12078, false), first(imm_address#12116, false), first(imm_airline#12155, false), first(imm_flight_no#12195, false)])\n",
      "            +- *(4) Sort [imm_main_cic_id#11808 ASC NULLS FIRST], false, 0\n",
      "               +- Exchange hashpartitioning(imm_main_cic_id#11808, 5)\n",
      "                  +- SortAggregate(key=[imm_main_cic_id#11808], functions=[partial_first(imm_year#11838, false), partial_first(imm_month#11869, false), partial_first(imm_cntyl#11901, false), partial_first(imm_visa#11934, false), partial_first(imm_port#11968, false), partial_first(imm_arrival_date#12004, false), partial_first(imm_departure_date#12041, false), partial_first(imm_model#12078, false), partial_first(imm_address#12116, false), partial_first(imm_airline#12155, false), partial_first(imm_flight_no#12195, false)])\n",
      "                     +- *(3) Sort [imm_main_cic_id#11808 ASC NULLS FIRST], false, 0\n",
      "                        +- *(3) HashAggregate(keys=[imm_departure_date#12041, imm_flight_no#12195, imm_model#12078, imm_arrival_date#12004, imm_airline#12155, imm_main_cic_id#11808, imm_address#12116, imm_visa#11934, imm_year#11838, imm_port#11968, imm_cntyl#11901, imm_month#11869], functions=[])\n",
      "                           +- Exchange hashpartitioning(imm_departure_date#12041, imm_flight_no#12195, imm_model#12078, imm_arrival_date#12004, imm_airline#12155, imm_main_cic_id#11808, imm_address#12116, imm_visa#11934, imm_year#11838, imm_port#11968, imm_cntyl#11901, imm_month#11869, 5)\n",
      "                              +- *(2) HashAggregate(keys=[imm_departure_date#12041, imm_flight_no#12195, imm_model#12078, imm_arrival_date#12004, imm_airline#12155, imm_main_cic_id#11808, imm_address#12116, imm_visa#11934, imm_year#11838, imm_port#11968, imm_cntyl#11901, imm_month#11869], functions=[])\n",
      "                                 +- *(2) Project [cast(cicid#11525 as int) AS imm_main_cic_id#11808, cast(i94yr#11526 as int) AS imm_year#11838, cast(i94mon#11527 as int) AS imm_month#11869, cast(i94cit#11528 as int) AS imm_cntyl#11901, cast(i94visa#11536 as int) AS imm_visa#11934, i94port#11530 AS imm_port#11968, pythonUDF0#12345 AS imm_arrival_date#12004, pythonUDF1#12346 AS imm_departure_date#12041, cast(i94mode#11532 as int) AS imm_model#12078, i94addr#11533 AS imm_address#12116, airline#11549 AS imm_airline#12155, fltno#11551 AS imm_flight_no#12195]\n",
      "                                    +- BatchEvalPython [<lambda>(arrdate#11531), <lambda>(depdate#11534)], [airline#11549, arrdate#11531, cicid#11525, depdate#11534, fltno#11551, i94addr#11533, i94cit#11528, i94mode#11532, i94mon#11527, i94port#11530, i94visa#11536, i94yr#11526, pythonUDF0#12345, pythonUDF1#12346]\n",
      "                                       +- *(1) Project [airline#11549, arrdate#11531, cicid#11525, depdate#11534, fltno#11551, i94addr#11533, i94cit#11528, i94mode#11532, i94mon#11527, i94port#11530, i94visa#11536, i94yr#11526]\n",
      "                                          +- *(1) Scan SasRelation(/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat,null,Configuration: core-default.xml, core-site.xml, mapred-default.xml, mapred-site.xml, yarn-default.xml, yarn-site.xml, hdfs-default.xml, hdfs-site.xml,0) [cicid#11525,i94yr#11526,i94mon#11527,i94cit#11528,i94res#11529,i94port#11530,arrdate#11531,i94mode#11532,i94addr#11533,depdate#11534,i94bir#11535,i94visa#11536,count#11537,dtadfile#11538,visapost#11539,occup#11540,entdepa#11541,entdepd#11542,entdepu#11543,matflag#11544,biryear#11545,dtaddto#11546,gender#11547,insnum#11548,... 4 more fields] PushedFilters: [], ReadSchema: struct<cicid:double,i94yr:double,i94mon:double,i94cit:double,i94res:double,i94port:string,arrdate...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr: 4 digit year of the arrival  -> imm_year\n",
    "2. i94mon: numeric month of the arrival -> imm_month\n",
    "3. i94citi&i94res: 3 digit code of origin city -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa: reason for immigration -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port: 3 character code of destination city --> Foreign key (used to map to USDemographics and City Temperature data) -> imm_port\n",
    "6. arrdate: arrival date of the departure -> imm_arrival_date:\n",
    "7. deptdate: departure date\n",
    "date_add\n",
    "7. i94mode: 1 digit travel code -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_datetime(days: DoubleType) -> datetime:\n",
    "    \"\"\"convert_to_datetime converts days to datetime format\n",
    "\n",
    "    Args:\n",
    "        days (DoubleType): from sas arrive or departure date\n",
    "\n",
    "    Returns:\n",
    "        datetime: added days to datetime format result.\n",
    "    \"\"\"\n",
    "    if days is not None:\n",
    "        date = datetime.strptime('1960-01-01', '%Y-%m-%d')\n",
    "\n",
    "        return date + timedelta(days=days)\n",
    "\n",
    "udf_convert_to_datetime = udf(lambda x: convert_to_datetime(x), DateType())\n",
    "\n",
    "immigration_main_information = df_imm_data.withColumn(\"imm_main_cic_id\", col(\"cicid\").cast(\"Integer\"))\\\n",
    "            .withColumn(\"imm_year\", col(\"i94yr\").cast(\"Integer\"))\\\n",
    "                .withColumn(\"imm_month\", col(\"i94mon\").cast(\"Integer\"))\\\n",
    "                    .withColumn(\"imm_cntyl\", col(\"i94cit\").cast(\"Integer\"))\\\n",
    "                        .withColumn(\"imm_visa\", col(\"i94visa\").cast(\"Integer\"))\\\n",
    "                            .withColumn(\"imm_port\", col(\"i94port\").cast(\"String\"))\\\n",
    "                                .withColumn(\"imm_arrival_date\", udf_convert_to_datetime(col(\"arrdate\")))\\\n",
    "                                    .withColumn(\"imm_departure_date\", udf_convert_to_datetime(col(\"depdate\")))\\\n",
    "                                        .withColumn(\"imm_model\", col(\"i94mode\").cast(\"Integer\"))\\\n",
    "                                            .withColumn(\"imm_address\", col(\"i94addr\").cast(\"String\"))\\\n",
    "                                                .withColumn(\"imm_airline\", col(\"airline\").cast(\"String\"))\\\n",
    "                                                    .withColumn(\"imm_flight_no\", col(\"fltno\").cast(\"String\"))\\\n",
    "        .select(col('imm_main_cic_id'), \\\n",
    "                    col('imm_year'),\\\n",
    "                        col('imm_month'),\\\n",
    "                            col('imm_cntyl'),\\\n",
    "                                col('imm_visa'),\\\n",
    "                                    col('imm_port'),\\\n",
    "                                        col('imm_arrival_date'),\\\n",
    "                                            col('imm_departure_date'),\\\n",
    "                                                col('imm_model'),\\\n",
    "                                                    col('imm_address'),\\\n",
    "                                                        col('imm_airline'),\\\n",
    "                col('imm_flight_no')).distinct().dropDuplicates(['imm_main_cic_id'])\n",
    "\n",
    "df_immigration_main_information = immigration_main_information.createOrReplaceTempView(\n",
    "    \"immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information = spark.sql(\"SELECT * FROM immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information.persist()\n",
    "\n",
    "df_immigration_main_information.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- imm_main_cic_id: integer (nullable = true)\n",
      " |-- imm_year: integer (nullable = true)\n",
      " |-- imm_month: integer (nullable = true)\n",
      " |-- imm_cntyl: integer (nullable = true)\n",
      " |-- imm_visa: integer (nullable = true)\n",
      " |-- imm_port: string (nullable = true)\n",
      " |-- imm_arrival_date: date (nullable = true)\n",
      " |-- imm_departure_date: date (nullable = true)\n",
      " |-- imm_model: integer (nullable = true)\n",
      " |-- imm_address: string (nullable = true)\n",
      " |-- imm_airline: string (nullable = true)\n",
      " |-- imm_flight_no: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_immigration_main_information.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3,096,313'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{df_immigration_main_information.count():,}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact: Nofification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=======================>                                  (2 + 3) / 5]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "|imm_main_cic_id|imm_year|imm_month|imm_cntyl|imm_visa|imm_port|imm_arrival_date|imm_departure_date|imm_model|imm_address|imm_airline|imm_flight_no|imm_per_cic_id|imm_person_birth_year|imm_person_gender|imm_visatype|news_cord_uid|news_source|news_title|news_licence|news_abstract|news_publish_time|news_authors|news_url|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          vhs|        PMC|       The|         cc-|          RNA|              201|         Jär|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          1am|        PMC|       Pre|         cc-|          We |              201|         Guo|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          4ov|        PMC|       Doe|         cc-|          The|              201|         Poo|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          rkd|        PMC|       Mea|         cc-|          Bil|              201|         Ken|     htt|\n",
      "|            982|     201|        4|      101|       2|     BOS|             201|               201|        1|         MI|         TK|          000|           982|                  194|                M|          B2|          jns|        PMC|       DNA|         cc-|          To |              201|         Liu|     htt|\n",
      "+---------------+--------+---------+---------+--------+--------+----------------+------------------+---------+-----------+-----------+-------------+--------------+---------------------+-----------------+------------+-------------+-----------+----------+------------+-------------+-----------------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/23 23:02:40 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 2361589 ms exceeds timeout 120000 ms\n",
      "22/06/23 23:02:40 ERROR TaskSchedulerImpl: Lost executor driver on localhost: Executor heartbeat timed out after 2361589 ms\n",
      "22/06/23 23:02:40 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_10_piece0 !\n",
      "22/06/23 23:02:40 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_11_piece0 !\n",
      "22/06/23 23:02:40 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Notification Table\n",
    "\"\"\"\n",
    "t2.imm_main_cic_id\n",
    "t2.imm_per_cic_id\n",
    "t2.news_cord_uid\n",
    "src.cidemo_id\n",
    "src.value_of_imm_destination_city\n",
    "t2.news_title\n",
    "t2.news_abstract\n",
    "t2.news_publish_time\n",
    "t2.news_authors\n",
    "\"\"\"\n",
    "\n",
    "#  ** t1: join imm two tables\n",
    "#  ** t2: join news table with t1\n",
    "#  ** t3: join us cities table with t2\n",
    "\n",
    "df_notification = spark.sql(\n",
    "        \"WITH t1 AS \\\n",
    "            (SELECT * \\\n",
    "               FROM immigration_main_information_data imid \\\n",
    "             INNER JOIN imm_personal ip \\\n",
    "                    ON imid.imm_main_cic_id = ip.imm_per_cic_id \\\n",
    "                 WHERE imid.imm_year = 2016 \\\n",
    "            ), t2 AS \\\n",
    "                (SELECT * \\\n",
    "                   FROM t1 \\\n",
    "                 INNER JOIN news_article_data nad \\\n",
    "                        ON t1.imm_arrival_date = nad.news_publish_time \\\n",
    "            ) \\\n",
    "            SELECT  * \\\n",
    "              FROM t2 \\\n",
    "            LIMIT 5 \\\n",
    "        \"\n",
    "    )\n",
    "\n",
    "df_notification.show(n=5, truncate=3)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f40c152a15a669d798eb1ad4e6f345bdd77350f6745bfc8751a72382d50440f0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nick_udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
