{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id, udf, to_date\n",
    "from pyspark.sql.types import (StructType,\n",
    "                               StructField,\n",
    "                               StringType,\n",
    "                               IntegerType,\n",
    "                               DoubleType,\n",
    "                               DateType,\n",
    "                               FloatType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/29 23:42:47 WARN Utils: Your hostname, OneForAll-NickdeMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.101 instead (on interface en0)\n",
      "22/05/29 23:42:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/oneforall_nick/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/oneforall_nick/.ivy2/jars\n",
      "saurfang#spark-sas7bdat added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-38cff8b4-2dc5-4483-984a-4dd0a8c2f8af;1.0\n",
      "\tconfs: [default]\n",
      "\tfound saurfang#spark-sas7bdat;3.0.0-s_2.12 in spark-packages\n",
      "\tfound com.epam#parso;2.0.11 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api-scala_2.12;12.0 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.10 in central\n",
      "\tfound org.apache.logging.log4j#log4j-api;2.13.2 in central\n",
      ":: resolution report :: resolve 306ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.epam#parso;2.0.11 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api;2.13.2 from central in [default]\n",
      "\torg.apache.logging.log4j#log4j-api-scala_2.12;12.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.10 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\tsaurfang#spark-sas7bdat;3.0.0-s_2.12 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-38cff8b4-2dc5-4483-984a-4dd0a8c2f8af\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/7ms)\n",
      "22/05/29 23:42:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.\\\n",
    "    appName(\"data_spark_on_emr\").\\\n",
    "    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:3.0.0-s_2.12\").\\\n",
    "    getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>data_spark_on_emr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb59e5e9bb0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop spark session if I don't need it.\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.initial.jar.urls',\n",
       "  'spark://10.0.0.101:49607/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,spark://10.0.0.101:49607/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar,spark://10.0.0.101:49607/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,spark://10.0.0.101:49607/jars/com.epam_parso-2.0.11.jar,spark://10.0.0.101:49607/jars/org.slf4j_slf4j-api-1.7.5.jar,spark://10.0.0.101:49607/jars/org.scala-lang_scala-reflect-2.12.10.jar'),\n",
       " ('spark.sql.parquet.binaryAsString', 'true'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.driver.memory', '4g'),\n",
       " ('spark.executor.memory', '4g'),\n",
       " ('spark.driver.host', '10.0.0.101'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/spark-warehouse'),\n",
       " ('spark.driver.port', '49607'),\n",
       " ('spark.app.initial.file.urls',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '\"-Dio.netty.tryReflectionSetAccessible=true\"'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.files',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.name', 'data_spark_on_emr'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.repl.local.jars',\n",
       "  'file:///Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,file:///Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,file:///Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,file:///Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,file:///Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.app.id', 'local-1653838970012'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.submit.pyFiles',\n",
       "  '/Users/oneforall_nick/.ivy2/jars/saurfang_spark-sas7bdat-3.0.0-s_2.12.jar,/Users/oneforall_nick/.ivy2/jars/com.epam_parso-2.0.11.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-scala_2.12-12.0.jar,/Users/oneforall_nick/.ivy2/jars/org.slf4j_slf4j-api-1.7.5.jar,/Users/oneforall_nick/.ivy2/jars/org.scala-lang_scala-reflect-2.12.10.jar,/Users/oneforall_nick/.ivy2/jars/org.apache.logging.log4j_log4j-api-2.13.2.jar'),\n",
       " ('spark.jars.packages', 'saurfang:spark-sas7bdat:3.0.0-s_2.12'),\n",
       " ('spark.app.startTime', '1653838968818')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark session setting configuration\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "spark.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ ALL DATA AND SAVE TO simulation data file, and then analysis those data in spark.\n",
    "# ****** Read data from local file system(be a s3) to spark DataFrame ******\n",
    "# All data format will be saved as a parquet file.\n",
    "\n",
    "# file path: data >> immigration_data\n",
    "# There are three tables: immigration_table, immigration_personal_table and immigration_label_table\n",
    "# ***immigration_table, immigration_personal_table and immigration_label_table schema***\n",
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr  -> imm_year\n",
    "2. i94mon -> imm_month\n",
    "3. i94citi&i94res -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port -> imm_port\n",
    "6. arrdate -> imm_arrival_date:\n",
    "7. i94mode -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: immigration_label\n",
    "\n",
    "- df_imm_city_res_label\n",
    "- df_imm_destination_city\n",
    "- df_imm_travel_code\n",
    "- df_imm_address\n",
    "- df_imm_visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_labels_descriptions.SAS\"\n",
    "\n",
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_label_data_read = SAS7BDAT(label_data)\n",
    "\n",
    "# t = list(imm_label_data_read)\n",
    "# TODO: make sure that is a correct data\n",
    "\n",
    "# def read_data_to_df(call_func):\n",
    "#     # def wrapper(label_data):\n",
    "#     def wrapper(*args, **kwargs):\n",
    "#         # return call_func(*args)\n",
    "#         print(\"Reading data...\")\n",
    "#         with open(label_data) as f:\n",
    "#             context = f.read().replace('\\t', '')\n",
    "#         call_func(context, *args, **kwargs)\n",
    "#         print(\"Data reading finished.\")\n",
    "#     return wrapper\n",
    "\n",
    "\n",
    "with open(label_data) as f:\n",
    "    context = f.read().replace('\\t', '')\n",
    "    # content_mapping = context[context.index('i94prtl'):]\n",
    "    # content_line_split = content_mapping[:content_mapping.index(\n",
    "    #     ';')].split('\\n')\n",
    "    # content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    # content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    # # content_dict = [[i[0].strip(), i[1].strip(), [e for e in i[1].strip().split(', ')[1:]]] for i in content_dict if len(i) == 2]\n",
    "    # content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "\n",
    "\n",
    "# @read_data_to_df\n",
    "def code_mapping(context, idx):\n",
    "    content_mapping = context[context.index(idx):]\n",
    "    content_line_split = content_mapping[:content_mapping.index(';')].split('\\n')\n",
    "    content_line_list = [line.replace(\"'\", \"\") for line in content_line_split]\n",
    "    content_dict = [i.split('=') for i in content_line_list[1:]]\n",
    "    content_dict = [[i[0].strip(), i[1].strip().split(', ')[:][0], e] for i in content_dict if len(i) == 2 for e in i[1].strip().split(', ')[1:]]\n",
    "    return content_dict\n",
    "\n",
    "i94cit_res = code_mapping(context, \"i94cntyl\")\n",
    "i94port = code_mapping(context, \"i94prtl\")\n",
    "i94mode = code_mapping(context, \"i94model\")\n",
    "i94addr = code_mapping(context, \"i94addrl\")\n",
    "i94visa = {'1': 'Business',\n",
    "           '2': 'Pleasure',\n",
    "           '3': 'Student'}\n",
    "\n",
    "# for i in i94port:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"code1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Writing into aws s3 and format parquet file.\n",
    "df_imm_city_res_label = spark.sparkContext.parallelize(i94cit_res.items()).toDF([\"col_of_imm_cntyl\", \"value_of_imm_cntyl\"])\\\n",
    "    .withColumn(\"col_of_imm_cntyl\", col(\"col_of_imm_cntyl\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_cntyl\", col(\"value_of_imm_cntyl\").cast(\"String\"))\n",
    "\n",
    "df_imm_destination_city = spark.sparkContext.parallelize(i94port).toDF([\"code_of_imm_destination_city\", \"value_of_imm_destination_city\", \"value_of_alias_imm_destination_city\"])\\\n",
    "    .withColumn(\"code_of_imm_destination_city\", col(\"code_of_imm_destination_city\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_imm_destination_city\", col(\"value_of_imm_destination_city\").cast(\"String\")) \\\n",
    "    .withColumn(\"value_of_alias_imm_destination_city\", col(\"value_of_alias_imm_destination_city\").cast(\"String\"))\n",
    "\n",
    "df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city_data\")\n",
    "df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city_data\")\n",
    "\n",
    "# df_imm_destination_city_tmp = df_imm_destination_city.createOrReplaceTempView(\"imm_destination_city\")\n",
    "# df_imm_destination_city_tmp = spark.sql(\"SELECT * FROM imm_destination_city WHERE code_of_imm_destination_city IS NOT NULL\")\n",
    "\n",
    "df_imm_travel_code = spark.sparkContext.parallelize(i94mode.items()).toDF([\"code_of_imm_travel_code\", \"value_of_imm_travel_code\"])\\\n",
    "    .withColumn(\"code_of_imm_travel_code\", col(\"code_of_imm_travel_code\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_travel_code\", col(\"value_of_imm_travel_code\").cast(\"String\"))\n",
    "\n",
    "df_imm_address = spark.sparkContext.parallelize(i94addr.items()).toDF([\"code_of_imm_address\", \"value_of_imm_address\"])\\\n",
    "    .withColumn(\"code_of_imm_address\", col(\"code_of_imm_address\").cast(\"String\"))\\\n",
    "    .withColumn(\"value_of_imm_address\", col(\"value_of_imm_address\").cast(\"String\"))\n",
    "\n",
    "df_imm_visa = spark.sparkContext.parallelize(i94visa.items()).toDF([\"code_of_imm_visa\", \"value_of_imm_visa\"])\\\n",
    "    .withColumn(\"code_of_imm_visa\", col(\"code_of_imm_visa\").cast(\"Integer\"))\\\n",
    "    .withColumn(\"value_of_imm_visa\", col(\"value_of_imm_visa\").cast(\"String\"))\n",
    "\n",
    "\n",
    "\n",
    "# df_imm_city_res_label.createOrReplaceTempView(\"imm_city_res_label\")\n",
    "\n",
    "# df_imm_city_res_label = spark.sql(\"SELECT * FROM imm_city_res_label\")\n",
    "# df_imm_city_res.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm_destination_city_tmp.show(n=5, truncate=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Check data source\n",
    "for column in df_imm_city_res_label.columns:\n",
    "    n = df_imm_city_res_label.select(column).distinct().count()\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm_city_res.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: news_article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path: data >> news_article\n",
    "\"\"\"Table: news_article schema\n",
    "pk: cord_uid -> news_cord_uid\n",
    "1. source_x -> news_source\n",
    "    schema: StringType()\n",
    "2. title -> news_title\n",
    "    schema: StringType()\n",
    "3. license -> news_licence\n",
    "    schema: StringType()\n",
    "4. abstract -> news_abstract\n",
    "    schema: StringType()\n",
    "5. publish_time -> news_publish_time (fk)\n",
    "    schema: TimestampType()\n",
    "6. authors -> news_authors\n",
    "    schema: StringType()\n",
    "7. url -> news_url\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "data_news = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/news_data/metadata.csv\"\n",
    "\n",
    "# news_schema = StructType([\n",
    "#     StructField(name=\"news_cord_uid\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_source\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_title\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_licence\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_abstract\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_publish_time\", dataType=DateType(), nullable=True),\n",
    "#     StructField(name=\"news_authors\", dataType=StringType(), nullable=True),\n",
    "#     StructField(name=\"news_url\", dataType=StringType(), nullable=True)\n",
    "# ])\n",
    "\n",
    "df_news = spark.read.options(header=True, delimiter=',').csv(path=data_news)\n",
    "\n",
    "df_news = df_news.withColumn(\"news_cord_uid\", col(\"cord_uid\").cast(\"String\")) \\\n",
    "                    .withColumn(\"news_source\", col(\"source_x\").cast(\"String\")) \\\n",
    "                        .withColumn(\"news_title\", col(\"title\").cast(\"String\")) \\\n",
    "                            .withColumn(\"news_licence\", col(\"license\").cast(\"String\")) \\\n",
    "                                .withColumn(\"news_abstract\", col(\"abstract\").cast(\"String\")) \\\n",
    "                                    .withColumn(\"news_publish_time\", to_date(df_news.publish_time, \"yyyy-MM-dd\")) \\\n",
    "                                        .withColumn(\"news_authors\", col(\"authors\").cast(\"String\")) \\\n",
    "                                            .withColumn(\"news_url\", col(\"url\").cast(\"String\")) \\\n",
    "                 .select(col(\"news_cord_uid\"),\n",
    "                         col(\"news_source\"),\n",
    "                         col(\"news_title\"),\n",
    "                         col(\"news_licence\"),\n",
    "                         col(\"news_abstract\"),\n",
    "                         col(\"news_publish_time\"),\n",
    "                         col(\"news_authors\"),\n",
    "                         col(\"news_url\")\n",
    "                         )\n",
    "\n",
    "df_news_tmp = df_news.createOrReplaceTempView(\"news_article_data\")\n",
    "\n",
    "df_news_tmp = spark.sql(\"SELECT DISTINCT publish_time FROM news_article_data\")\n",
    "\n",
    "df_news_tmp.persist()\n",
    "\n",
    "df_news_tmp.explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news_tmp.unpersist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Us Cities Demographics data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a us-cities data dimension table\n",
    "\"\"\"Table: us_cities_demographics schema\n",
    "pk: generated -> cidemo_id\n",
    "    schema: IntegerType()\n",
    "1. City -> cidemo_city\n",
    "    schema: StringType()\n",
    "2. State -> cidemo_state\n",
    "    schema: StringType()\n",
    "3. Median Age -> cidemo_median_age\n",
    "    schema: FloatType()\n",
    "4. Total Population -> cidemo_total_population\n",
    "    schema: IntegerType()\n",
    "5. State Code -> cidemo_state_code (fk)\n",
    "    schema: StringType()\n",
    "6. Count -> cidemo_count\n",
    "    schema: IntegerType()\n",
    "\"\"\"\n",
    "\n",
    "data_us_cities_demographics = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/usCitiesDemographics_data/usCitiesDemo.csv\"\n",
    "\n",
    "# TODO -> Must be defined a function that generated each table schema:\n",
    "us_cities_demographics_data_schema = StructType([\n",
    "    StructField(name=\"cidemo_city\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_median_age\", dataType=FloatType(), nullable=True),\n",
    "    StructField(name=\"cidemo_total_population\", dataType=IntegerType(), nullable=True),\n",
    "    StructField(name=\"cidemo_state_code\", dataType=StringType(), nullable=True),\n",
    "    StructField(name=\"cidemo_count\", dataType=IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Using pyspark to read csv file\n",
    "df_us_cities_demographics = spark.read.options(header=True, delimiter=';').csv(data_us_cities_demographics)\n",
    "\"\"\"\n",
    "root\n",
    " |-- cidemo_city: string (nullable = true)\n",
    " |-- cidemo_state: string (nullable = true)\n",
    " |-- cidemo_median_age: float (nullable = true)\n",
    " |-- cidemo_total_population: integer (nullable = true)\n",
    " |-- cidemo_state_code: string (nullable = true)\n",
    " |-- cidemo_count: integer (nullable = true)\n",
    "\"\"\"\n",
    "df_us_cities_demographics = df_us_cities_demographics.withColumn(\"cidemo_city\", col(\"City\").cast(\"String\")) \\\n",
    "                    .withColumn(\"cidemo_state\", col(\"State\").cast(\"String\")) \\\n",
    "                        .withColumn(\"cidemo_median_age\", col(\"Median Age\").cast(\"Float\")) \\\n",
    "                            .withColumn(\"cidemo_male_population\", col(\"Male Population\").cast(\"Integer\")) \\\n",
    "                                .withColumn(\"cidemo_female_population\", col(\"Female Population\").cast(\"Integer\")) \\\n",
    "                                    .withColumn(\"cidemo_total_population\", col(\"Total Population\").cast(\"Integer\")) \\\n",
    "                                            .withColumn(\"cidemo_number_of_veterans\", col(\"Number of Veterans\").cast(\"Integer\")) \\\n",
    "                                                .withColumn(\"cidemo_foreign_born\", col(\"Foreign-born\").cast(\"Integer\")) \\\n",
    "                                                    .withColumn(\"cidemo_average_household_size\", col(\"Average Household Size\").cast(\"Float\")) \\\n",
    "                                                        .withColumn(\"cidemo_state_code\", col(\"State Code\").cast(\"String\")) \\\n",
    "                                                            .withColumn(\"cidemo_race\", col(\"Race\").cast(\"String\")) \\\n",
    "    .withColumn(\"cidemo_count\", col(\"Count\").cast(\"Integer\")) \\\n",
    "                    .select(col(\"cidemo_city\"),\n",
    "                            col(\"cidemo_state\"),\n",
    "                            col(\"cidemo_median_age\"),\n",
    "                            col(\"cidemo_total_population\"),\n",
    "                            col(\"cidemo_state_code\"),\n",
    "                            col(\"cidemo_count\"))\n",
    "\n",
    "# Auto-generated series of id\n",
    "df_us_cities_demographics = df_news.withColumn(\"cidemo_id\", monotonically_increasing_id())\n",
    "\n",
    "df_us_cities_demographics_temp = df_news.createOrReplaceTempView(\"us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp = spark.sql(\"SELECT * FROM us_cities_demographics_data\")\n",
    "\n",
    "df_us_cities_demographics_temp.persist()\n",
    "\n",
    "df_us_cities_demographics_temp.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_cities_demographics_temp.show(n=5, truncate=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_cities_demographics_temp.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This code block is be placed in analysis jupyter notebook files\n",
    "imm_data = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/airflow/data/immigration_data/immigration_apr16_sub.sas7bdat\"\n",
    "\n",
    "\n",
    "df_imm_data = spark.read.format(\"com.github.saurfang.sas.spark\").load(imm_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_imm_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration personal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Table: immigration_personal schema -> According to this person data that I will make a core data table to display notifications information.\n",
    "pk: cicid -> imm_per_cic_id\n",
    "    schema: StringType()\n",
    "1. biryear -> imm_person_birth_year\n",
    "    schema: IntegerType()\n",
    "2. gender -> imm_person_gender\n",
    "    schema: StringType()\n",
    "3. visatype -> imm_person_visa_type\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "# show(n=5, truncate=5)\n",
    "df_immigration_personal = df_imm_data.withColumn(\"imm_per_cic_id\", col(\"cicid\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_person_birth_year\", col(\"biryear\").cast(\"Integer\"))\\\n",
    "           .withColumn(\"imm_person_gender\", col(\"gender\").cast(\"String\"))\\\n",
    "           .withColumn(\"imm_visatype\", col(\"visatype\").cast(\"String\")).select(col(\"imm_per_cic_id\"), \\\n",
    "                                                                              col(\"imm_person_birth_year\"), \\\n",
    "                                                                              col(\"imm_person_gender\"), \\\n",
    "                                                                              col(\"imm_visatype\"))\n",
    "\n",
    "df_immigration_personal_tmp = df_immigration_personal.createOrReplaceTempView(\"imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp = spark.sql(\"SELECT * FROM imm_personal\")\n",
    "\n",
    "df_immigration_personal_tmp.persist()\n",
    "\n",
    "df_immigration_personal_tmp.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension Table: Immigration main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Table: immigration_main_information schema\n",
    "pk: cicid -> imm_main_cic_id\n",
    "1. i94yr: 4 digit year of the arrival  -> imm_year\n",
    "2. i94mon: numeric month of the arrival -> imm_month\n",
    "3. i94citi&i94res: 3 digit code of origin city -> imm_citi_res -> imm_cntyl\n",
    "4. i94visa: reason for immigration -> imm_visa\n",
    "    three categories:\n",
    "        1 = Business\n",
    "        2 = Pleasure\n",
    "        3 = Student\n",
    "5. i94port: 3 character code of destination city --> Foreign key (used to map to USDemographics and City Temperature data) -> imm_port\n",
    "6. arrdate: arrival date of the departure -> imm_arrival_date:\n",
    "7. deptdate: departure date\n",
    "date_add\n",
    "7. i94mode: 1 digit travel code -> imm_model:\n",
    "    four categories:\n",
    "        1 = 'Air'\n",
    "\t    2 = 'Sea'\n",
    "\t    3 = 'Land'\n",
    "\t    9 = 'Not reported'\n",
    "8. i94addr -> imm_address\n",
    "    ex: 'AL'='ALABAMA'\n",
    "9. airline -> imm_airline\n",
    "10 fltno -> imm_flight_no\n",
    "    schema: StringType()\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_datetime(days: DoubleType) -> datetime:\n",
    "    \"\"\"convert_to_datetime converts days to datetime format\n",
    "\n",
    "    Args:\n",
    "        days (DoubleType): from sas arrive or departure date\n",
    "\n",
    "    Returns:\n",
    "        datetime: added days to datetime format result.\n",
    "    \"\"\"\n",
    "    if days is not None:\n",
    "        date = datetime.strptime('1960-01-01', '%Y-%m-%d')\n",
    "\n",
    "        return date + timedelta(days=days)\n",
    "\n",
    "udf_convert_to_datetime = udf(lambda x: convert_to_datetime(x), DateType())\n",
    "\n",
    "immigration_main_information = df_imm_data.withColumn(\"imm_main_cic_id\", col(\"cicid\").cast(\"Integer\"))\\\n",
    "            .withColumn(\"imm_year\", col(\"i94yr\").cast(\"Integer\"))\\\n",
    "                .withColumn(\"imm_month\", col(\"i94mon\").cast(\"Integer\"))\\\n",
    "                    .withColumn(\"imm_cntyl\", col(\"i94cit\").cast(\"Integer\"))\\\n",
    "                        .withColumn(\"imm_visa\", col(\"i94visa\").cast(\"Integer\"))\\\n",
    "                            .withColumn(\"imm_port\", col(\"i94port\").cast(\"String\"))\\\n",
    "                                .withColumn(\"imm_arrival_date\", udf_convert_to_datetime(col(\"arrdate\")))\\\n",
    "                                    .withColumn(\"imm_departure_date\", udf_convert_to_datetime(col(\"depdate\")))\\\n",
    "                                        .withColumn(\"imm_model\", col(\"i94mode\").cast(\"Integer\"))\\\n",
    "                                            .withColumn(\"imm_address\", col(\"i94addr\").cast(\"String\"))\\\n",
    "                                                .withColumn(\"imm_airline\", col(\"airline\").cast(\"String\"))\\\n",
    "                                                    .withColumn(\"imm_flight_no\", col(\"fltno\").cast(\"String\"))\\\n",
    "        .select(col('imm_main_cic_id'), \\\n",
    "                    col('imm_year'),\\\n",
    "                        col('imm_month'),\\\n",
    "                            col('imm_cntyl'),\\\n",
    "                                col('imm_visa'),\\\n",
    "                                    col('imm_port'),\\\n",
    "                                        col('imm_arrival_date'),\\\n",
    "                                            col('imm_departure_date'),\\\n",
    "                                                col('imm_model'),\\\n",
    "                                                    col('imm_address'),\\\n",
    "                                                        col('imm_airline'),\\\n",
    "                                                            col('imm_flight_no'))\n",
    "\n",
    "df_immigration_main_information = immigration_main_information.createOrReplaceTempView(\n",
    "    \"immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information = spark.sql(\"SELECT * FROM immigration_main_information_data\")\n",
    "\n",
    "df_immigration_main_information.persist()\n",
    "\n",
    "df_immigration_main_information.explain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data column attributes\n",
    "# from sas7bdat import SAS7BDAT\n",
    "\n",
    "# Using the library for knowing more data information\n",
    "# imm_data_read = SAS7BDAT(imm_data)\n",
    "\n",
    "# imm_data_read.header\n",
    "# for i in imm_data_read.columns:\n",
    "#     print(\"col_id \", i.col_id)\n",
    "#     print(\"  name\",  i.name.decode(encoding ='utf-8'))\n",
    "#     print(\"  label\", i.label.decode(encoding ='utf-8'))\n",
    "#     print(\"  format\", i.format)\n",
    "#     print(\"  type\", i.type)\n",
    "#     print(\"  length\", i.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dealing with multiple data files\n",
    "# TODO: Make a def for doing this.\n",
    "# Method1: Using pandas to read file.\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat', iterator=True, chunksize=5000000)\n",
    "# pdf_immigration = pd.read_sas(imm_data, format='sas7bdat')\n",
    "\n",
    "# imm_chunks_1 = list(pdf_immigration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pandas.io.sas.sas7bdat import SAS7BDATReader\n",
    "\n",
    "# rdr = SAS7BDATReader(imm_data, convert_header_text=False)\n",
    "# df3 = rdr.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Notification Table\n",
    "\"\"\"\n",
    "t2.imm_main_cic_id\n",
    "t2.imm_per_cic_id\n",
    "t2.news_cord_uid\n",
    "src.cidemo_id\n",
    "src.value_of_imm_destination_city\n",
    "t2.news_title\n",
    "t2.news_abstract\n",
    "t2.news_publish_time\n",
    "t2.news_authors\n",
    "\"\"\"\n",
    "\n",
    "#  ** t1: join imm two tables\n",
    "#  ** t2: join news table with t1\n",
    "#  ** t3: join us cities table with t2\n",
    "\n",
    "df_notification = spark.sql(\n",
    "    \"WITH t1 AS \\\n",
    "    (SELECT * \\\n",
    "       FROM immigration_main_information_data imid \\\n",
    "      LEFT JOIN imm_personal ip \\\n",
    "             ON imid.imm_main_cic_id = ip.imm_per_cic_id \\\n",
    "        WHERE imid.imm_year = 2016 \\\n",
    "     ), t2 AS \\\n",
    "        (SELECT * \\\n",
    "           FROM t1 \\\n",
    "         LEFT JOIN news_article_data nad \\\n",
    "                ON t1.imm_arrival_date = nad.news_publish_time \\\n",
    "     ) \\\n",
    "    SELECT t2.imm_main_cic_id \\\n",
    "           t2.imm_per_cic_id \\\n",
    "           t2.news_cord_uid \\\n",
    "           src.cidemo_id \\\n",
    "           src.value_of_imm_destination_city \\\n",
    "           t2.news_title \\\n",
    "           t2.news_abstract \\\n",
    "           t2.news_publish_time \\\n",
    "           t2.news_authors \\\n",
    "       FROM t2 \\\n",
    "     LEFT JOIN (SELECT * FROM us_cities_demographics_data ucdd INNER JOIN imm_destination_city_data idcd ON ucdd.cidemo_state_code = idcd.value_of_alias_imm_destination_city) src \\\n",
    "            ON t2.imm_port = src.code_of_imm_destination_city \\\n",
    "    \"\n",
    ")\n",
    "\n",
    "df_notification.show(n=5, truncate=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/udacity_capstone_project/lib/python3.9/site-packages/airflow/configuration.py:276: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if StrictVersion(sqlite3.sqlite_version) < StrictVersion(min_sqlite_version):\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import configparser\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "\n",
    "\n",
    "# ******* Access AWS Server *******\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('/Users/oneforall_nick/.aws/credentials'))\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = config['default']['aws_access_key_id']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['default']['aws_secret_access_key']\n",
    "# **********************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "AWS_S3_CONN_ID = \"S3_default\"\n",
    "bucket_name = ''\n",
    "s3_prefix = '/data/usCitiesDemographics_data/usCitiesDemo.csv'\n",
    "DEST_BUCKET = 'mydatapool'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = S3Hook(AWS_S3_CONN_ID)\n",
    "# content = hook.list_keys(bucket_name=DEST_BUCKET)\n",
    "f = hook.get_key(key=s3_prefix, bucket_name=DEST_BUCKET)\n",
    "ff = f.get()['Body'].read().decode('utf-8')\n",
    "ff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2022-05-30 20:35:45,624\u001b[0m] {\u001b[34mbase_aws.py:\u001b[0m447} WARNING\u001b[0m - Unable to use Airflow Connection for credentials.\u001b[0m\n",
      "[\u001b[34m2022-05-30 20:35:45,652\u001b[0m] {\u001b[34mcredentials.py:\u001b[0m1120} INFO\u001b[0m - Found credentials in environment variables.\u001b[0m\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'S3Hook' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000037?line=0'>1</a>\u001b[0m hook\u001b[39m.\u001b[39mget_key(key\u001b[39m=\u001b[39ms3_prefix, bucket_name\u001b[39m=\u001b[39mDEST_BUCKET)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/practice_and_learn_aws_emr_process.ipynb#ch0000037?line=1'>2</a>\u001b[0m file_c \u001b[39m=\u001b[39m hook\u001b[39m.\u001b[39;49mget()[\u001b[39m'\u001b[39m\u001b[39mBody\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mread()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'S3Hook' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "hook.get_key(key=s3_prefix, bucket_name=DEST_BUCKET)\n",
    "file_c = hook.get()['Body'].read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.base_aws import AwsBaseHook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = AwsBaseHook(aws_conn_id='aws_conn', client_type='s3').get_credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __secret_key__() -> str:\n",
    "    \"\"\"__secret_key__ _summary_\n",
    "\n",
    "        Purpose:\n",
    "            Building all access AWS server connection key and credentials.\n",
    "\n",
    "        Returns:\n",
    "            _type_: access key, secret key\n",
    "        \"\"\"\n",
    "\n",
    "    hook = AwsBaseHook(aws_conn_id='aws_conn',\n",
    "                       client_type='s3').get_credentials()\n",
    "\n",
    "    return hook.access_key, hook.secret_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2022-05-31 11:42:52,581\u001b[0m] {\u001b[34mbase_aws.py:\u001b[0m206} INFO\u001b[0m - Credentials retrieved from login\u001b[0m\n",
      "[\u001b[34m2022-05-31 11:42:52,581\u001b[0m] {\u001b[34mbase_aws.py:\u001b[0m100} INFO\u001b[0m - Retrieving region_name from Connection.extra_config['region_name']\u001b[0m\n",
      "AKIAUW7BQYXKDAIHT7HW eFtYlQ7iIlOI26bAl8XYsWB2c9VzgbYF7UhqpD1z\n"
     ]
    }
   ],
   "source": [
    "a,s = __secret_key__()\n",
    "print(a, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "hook = S3Hook(aws_conn_id='aws_conn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job_flow_overrides\n"
     ]
    }
   ],
   "source": [
    "filepath = \"/Users/oneforall_nick/workspace/Udacity_capstone_project/job_flow/\"\n",
    "files = dict([os.path.join(root, file_), file_.split(\".\")[0]] for root, dirs, files in os.walk(filepath) for file_ in files)\n",
    "for k, v in files.items():\n",
    "    print(files[k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/aws_emr_steps.json': 'aws_emr_steps'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPARK_STEP_FILE_PATH = '/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps/'\n",
    "file_1 = dict([os.path.join(root, file_), file_.split(\".\")[0]] for root, dirs, files in os.walk(SPARK_STEP_FILE_PATH) for file_ in files if file_.endswith('.json'))\n",
    "file_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'/Users/oneforall_nick/workspace/Udacity_capstone_project/aws_emr_steps'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pprint import pprint\n",
    "pprint(sys.path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2022-06-01 00:23:04,019\u001b[0m] {\u001b[34mbase_aws.py:\u001b[0m206} INFO\u001b[0m - Credentials retrieved from login\u001b[0m\n",
      "[\u001b[34m2022-06-01 00:23:04,021\u001b[0m] {\u001b[34mbase_aws.py:\u001b[0m100} INFO\u001b[0m - Retrieving region_name from Connection.extra_config['region_name']\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Name': 'demo-cluster-airflow',\n",
       " 'ReleaseLabel': '{{ var.value.release_label }}',\n",
       " 'LogUri': 's3n://{{ var.value.logs_bucket }}',\n",
       " 'Applications': [{'Name': 'Spark'}],\n",
       " 'Instances': {'InstanceFleets': [{'Name': 'MASTER',\n",
       "    'InstanceFleetType': 'MASTER',\n",
       "    'TargetSpotCapacity': 1,\n",
       "    'InstanceTypeConfigs': [{'InstanceType': 'm5.xlarge'}]},\n",
       "   {'Name': 'CORE',\n",
       "    'InstanceFleetType': 'CORE',\n",
       "    'TargetSpotCapacity': 2,\n",
       "    'InstanceTypeConfigs': [{'InstanceType': 'r5.2xlarge'}]}],\n",
       "  'Ec2SubnetId': '{{ var.value.ec2_subnet_id }}',\n",
       "  'KeepJobFlowAliveWhenNoSteps': False,\n",
       "  'TerminationProtected': False,\n",
       "  'Ec2KeyName': '{{ var.value.emr_ec2_key_pair }}'},\n",
       " 'BootstrapActions': [{'Name': 'string',\n",
       "   'ScriptBootstrapAction': {'Path': 's3://{{ var.value.bootstrap_bucket }}/bootstrap_actions.sh'}}],\n",
       " 'Configurations': [{'Classification': 'spark-hive-site',\n",
       "   'Properties': {'hive.metastore.client.factory.class': 'com.amazonaws.glue.catalog.metastore.AWSGlueDataCatalogHiveClientFactory'}}],\n",
       " 'VisibleToAllUsers': True,\n",
       " 'JobFlowRole': '{{ var.value.job_flow_role }}',\n",
       " 'ServiceRole': '{{ var.value.service_role }}',\n",
       " 'EbsRootVolumeSize': 32,\n",
       " 'StepConcurrencyLevel': 5,\n",
       " 'Tags': [{'Key': 'Environment', 'Value': 'Development'},\n",
       "  {'Key': 'Name', 'Value': 'Airflow EMR Demo Project'},\n",
       "  {'Key': 'Owner', 'Value': 'Data Analytics Team'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "import json\n",
    "AWS_CONN_ID = 'aws_conn'\n",
    "hook = S3Hook(aws_conn_id=AWS_CONN_ID)\n",
    "file_content = hook.get_key(\n",
    "    key='job_flow/job_flow_overrides.json', bucket_name='mydatapool')\n",
    "file_content = file_content.get()['Body'].read().decode('utf-8')\n",
    "json.loads(file_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48f95af79faaae749105c8389524df19196568350894b01545e0b6f755f2644b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('udacity_capstone_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
